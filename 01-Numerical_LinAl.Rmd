# Numerical Linear Algebra 

## Columnspace, Nullspace and all
Consider a matrix $A\in\mathbb{R}^{m\times n}$ defined as:
$$A = \begin{bmatrix} -&r_1^T&-\\ -&r_2^T&-\\ & \vdots& \\ - & r_m^T& - \end{bmatrix} = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}$$
where $r_i \in \mathbb{R}^{n\times 1}$ for $1\le i \le m$ are the rows and $a_i \in \mathbb{R}^{m\times 1}$ for $1\le i\le n$ are the columns of $A$.

Columnspace of a matrix $A$ is the span(linear combination) of columns of $A$. Also called as Range of $A$.
\begin{equation}
\text{Range}(A) =\text{Columnspace}(A) =  \{ Ax : x\in \mathbb{R}^{n\times 1} \}
\end{equation}

$$Ax = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}x_1\\x_2\\ \vdots\\ x_n \end{bmatrix} = \sum_{i=1}^n a_i x_i$$


Rowspace of a matrix $A$ is the span(linear combination) of rows of $A$. 
\begin{equation}
\text{Rowspace}(A) = \{ A^Ty : y\in \mathbb{R}^{m\times 1} \} 
\end{equation}

$$A^Ty = \begin{bmatrix} | & | &  & | \\ r_1 & r_2 &\cdots & r_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots\\ y_n \end{bmatrix} = \sum_{i=1}^n r_i y_i$$
Nullspace of a matrix $A$ is defined as follows:
\begin{equation}
\text{Nullspace}(A) = \{ z\in \mathbb{R}^{n\times 1}: Az=0\}
\end{equation}

NOTE:-

1. A linear system $Ax=b$ has a solution ONLY IF $b\in\text{Range}(A)$.

2. Dimension of Range$(A)$ is the number of linearly independent columns of $A$ or the column rank of $A$. Similarly, the dimension of Rowspace$(A)$ is the row rank or the number of independent rows of $A$. 

3. For a matrix $A$, row rank = column rank = rank$\le \min(m,n)$.

4. Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector $z\in \text{Nullspace}(A)$ and $w \in \text{Rowspace}(A)$  , $z$ is orthogonal to $w$.

Proof:- Let $w \in \text{Rowspace}(A)$, then $\exists y\in\mathbb{R}^{n\times 1}$ such that $w = A^Ty$. 

Also as $z\in \text{Nullspace}(A)$, we have $Az=0$.

Therefore,
$$\langle w,z\rangle = w^Tz = y^T Az = 0$$
Thus, nullspace of a matrix is orthogonal to row space of a matrix.

5. **Rank-Nullity Theorem:** Dimension of Nullspace$(A)$+Rank$(A)$ = $n$ = No. of columns of $A$

 

## Matrix Norms
Consider a matrix $A\in\mathbb{R}^{m\times n}$. Just like how we have defined a vector norm, we could have defined an "element wise matrix norm" as follows:
\begin{equation}
\lVert A \rVert_p^* = \left(\sum_{i=1}^n |A_{ij}|^p\right)^{\frac{1}{p}} 
\end{equation}

But this definition of norm does not satisfy the **submultiplicative property**. We are interested in this property as this dictates the convergence of iterative schemes.

A matrix norm is said to be submultiplicative if for any matrices $A\in \mathbb{R}^{m\times k}$ and $B \in \mathbb{R}^{k\times n}$, we have 
\begin{equation}
\lvert AB \rVert \le \lVert A \rVert  \lVert B \rVert
\end{equation}

Consider the case $$A = \begin{bmatrix} 2&2\\2&2 \end{bmatrix}$$ and $p\to \infty$, Therefore we have $$ \lVert A \rVert_{\infty}^* = \max_{1\le i \le m,1 \le j \le n} |A_{ij}| = 2$$.

$$A^2 = \begin{bmatrix} 8 & 8\\ 8& 8 \end{bmatrix}$$
Therefore,
$$\lVert A^2 \rVert_{\infty}^* = 8$$
We can clearly see that:
$$\lVert A^2 \rVert_{\infty}^* = 8 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4$$

<!-- Consider the case $$A = \begin{bmatrix} 1&1\\1&1 \end{bmatrix}$$ and $p=1$. Therefore, we have $$\lVert A \rVert_{1}^* = \sum_i\sum_j |A_{ij}| = 4$$ -->
<!-- As $A$ is an identity matrix, we have $$A^2 = A$$ Therefore $$\lVert A^2 \rVert_{1}^* = 4$$ -->
<!-- We can clearly see that: -->
<!-- $$\lVert A^2 \rVert_{\infty}^* = 4 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4$$ -->
which violates the submultiplicative property.


Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows. 
\begin{equation}
\lVert A \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p} = \sup_{\lVert y \rVert_p = 1} \lVert Ay \rVert_p
\end{equation}

p-norms are submultiplicative.

PROOF:- From the definition of p-norm,
<!-- $$\lVert AB \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}$$ -->
\begin{align*}
\lVert AB \rVert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}\\
&= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \\
&\le \left[\sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \right] \cdot \left[ \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \right]  \\
&= \lVert A \rVert_p \cdot \lVert B \rVert_p\\
\therefore \lVert AB \rVert_p \le \lVert A \rVert_p \cdot \lVert B \rVert_p
\end{align*}

Using this property, we can say that 
\begin{equation}
 \lVert A^n \rVert_p \le \lVert A \rVert^n_p
\end{equation}
 
 $\lVert A\rVert_1$ = Maximum of column sum of absolute values. 

 $\lVert A\rVert_{\infty}$  = Maximum of row sum of absolute values. 

<!-- $$\therefore$$ -->





## Condition number of Matrix vector products

Consider a matrix $A\in\mathbb{R}^{m\times n}$ and a vector $x\in\mathbb{R}^{n\times 1}$. Assume that there is no error in representing $A$. We are interested in finding the condition number of the Matrix-Vector product $f(x;A)= Ax$. 

From the definition of condition number, we can write the condition number $\kappa_r$ of the matrix vector product as:

$$\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_q\le r} \dfrac{\frac{\Vert A(x+\delta x)-Ax \Vert_p}{\Vert Ax \Vert_p}}{\frac{\Vert x+\delta x-x\Vert_q}{\Vert x\Vert_q}}$$
For simplicity let us choose $p=q$. Therefore,
$$\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} \frac{\Vert x \Vert_p}{\Vert Ax\Vert_p}$$
From the definition of matrix p-norm, we can say that: $$\lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} = \Vert A\Vert_p$$
Therefore the condition number of the matrix vector product is:
\begin{equation}
\kappa_r =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}
\end{equation}

From Sub-multiplicative property, as $\Vert Ax\Vert_p \ge\Vert A \Vert_p \Vert x \Vert_p$, we can show that $\kappa_r\ge 1$. 

**Case-1:-** $A\in \mathbb{R}^{m\times n}$ is a fat matrix($m<n$) 

From Rank-Nullity Theorem, we know that 

$$\text{Dimension of Nullspace$(A)$+Rank$(A)$ = $n$}$$

We know that Rank$(A) \le \min(m,n) \implies$ Rank $(A) \le m$ as $A$ is a fat matrix. 

$$\implies \text{Dimension of Nullspace}(A) \ge n-m$$
$\implies \exists$ a non-zero vector $z \in$ Nullspace$(A)$ i.e., $\exists z\in \mathbb{R}^{n\times 1}$ such that $Az=0$.

From the definition of condition number as in equation(), we have:
$\kappa_r(A,x) =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}$

If $x=z$ then as $Az=0$, we have $\Vert Az\Vert_p = 0$. Therefore $\kappa_r \to \infty$. 

Thus, multiplication by a fat matrix is **highly ill-conditioned.**

**Case-2:-** $A$ is an invertible square matrix

From the definition of condition number as in equation(), we have:
\begin{align}
\kappa_r(A,x) &=  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}\\
 &= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}

From submultiplicative property of matrix norms, we can write $\Vert A^{-1}(Ax) \Vert_p \le \Vert A^{-1} \Vert_p \Vert Ax \Vert_p$. Therefore, 

\begin{align}
\kappa_r(A,x)&= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\implies \kappa_r(A,x)&\le \frac{\Vert A \Vert_p \Vert A^{-1} \Vert_p \Vert Ax \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}

\begin{equation}
\kappa_r(A,x) \le \Vert A \Vert_p \Vert A^{-1} \Vert_p
\end{equation}
Therefore condition number is bounded above by $\Vert A \Vert_p \Vert A^{-1} \Vert_p$ which is independent of vector $x$. 

Define Condition number of matrix $A$ as $$\kappa_p(A) = \Vert A \Vert_p \Vert A^{-1} \Vert_p$$.

From submultiplicative property we can show that $\kappa_p(A)\ge 1$. 

Let us find condition number for some special matrices. Let us consider $A=Q$ to be an orthogonal/ unitary matrix. 

As $Q$ is an orthogonal matrix $Q^TQ = I\implies Q^T = Q^{-1}$. 


Therefore $$\Vert Qx \Vert_2^2 = (Qx)^TQx = x^TQ^TQx = x^Tx = \Vert x\Vert_2^2$$ 

Thus, $$\Vert Qx \Vert_2= \Vert x\Vert_2 = \Vert Q^Tx \Vert_2$$ 

From definition of 2-norm of a matrix, we have,
\begin{align}
\Vert Q\Vert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Qx \rVert_p}{\lVert x \rVert_p}\\
\implies \Vert Q\Vert_2 &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert x \rVert_2}{\lVert x \rVert_2}\\
\implies \Vert Q\Vert_2 &= 1
\end{align}

Therefore,
\begin{equation}
\Vert Q^T\Vert_2 = 1
\end{equation}

The condition number of $Q$ is therefore,
\begin{equation}
\kappa_2(Q) = \Vert Q \Vert_p \Vert Q^{-1} \Vert_p = \Vert Q \Vert_2 \Vert Q^T \Vert_2 = 1 
\end{equation}

## Solving Linear Systems

Consider the linear system $Ax=b$ where $A\in\mathbb{R}^{m\times n}$, $\mathbf{x}\in\mathbb{R}^{n\times 1}$ and $\mathbf{b}\in\mathbb{R}^{m\times 1}$ . Inputs are $A, \mathbf{b}$ and output is a vector $\mathbf{x}$. 

In this course, we assume that $A$ is a square(i.e., $m=n$) and invertible matrix (so that we have a unique $x$). Our goal is to find that unique $x$ which satisfies the system of equations. 

Let's go step by step. Let us consider special matrices first and then discuss about a general matrix $A$.

### Special Matrices

1. If $A$ is a unitary matrix, then $AA^T = I=A^TA$. 
$$Ax=b \implies A^T Ax = A^Tb \implies x = A^T b$$
To get $x$, we need to perform $n^2$ multiplications and $n(n-1)$ additions.

2. If $A=U$ is an upper triangular matrix. 

Let $Ux=b$ in matrix form be:
\begin{equation}
\begin{bmatrix}
U_{11} & U_{12} & U_{13} & \cdots & U_{1n}\\
0      & U_{22} & U_{23} & \cdots & U_{2n}\\
0      & 0      & U_{33} & \cdots & U_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0      & 0      & 0      & \cdots & U_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}

The last row of $U$ corresponds to  the equation 
\begin{equation}
U_{nn} x_n = b_n
\end{equation}

Now as $U$ is invertible, all the diagonal elements are non-zero i.e., $U_{ii}\neq 0 \ \forall i \in\{1,2,\cdots , n\}$. 

Therefore 
\begin{equation}
U_{nn} x_n = b_n \implies x_n = \frac{b_n}{U_{nn}}
\end{equation}

Similarly, we have:
\begin{equation}
U_{(n-1),(n-1)}x_{n-1}+U_{(n-1),n}x_n = b_{n-1} \implies x_{n-1} = \frac{b_{n-1}-U_{(n-1),n}x_n}{U_{(n-1),(n-1)}}
\end{equation}

Using induction, we can prove that for $i \in \{1,2,\cdots,n-1\}$, we have:
\begin{equation}
x_i = \frac{b_i -\sum_{j=i+1}^n U_{i,j}x_j}{U_{ii}}
\end{equation}

To calculate $x_i$(for $i \in \{1,2,\cdots,n\}$), we need to perform 1 division, $(n-i)$ multiplications and $(n-i)$ additions/subtractions. 

Therefore to calculate the output vector $x$, we need to perform 

* $\sum_{i=1}^n 1 = n$ divisions

* $\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}$ multiplications

* $\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}$ additions/subtractions

* In total $n+ \frac{n^2-n}{2}+\frac{n^2-n}{2} = n^2$ operations are required to get $x$. In other words "Computational Complexity is $n^2$"

> **NOTE:- ** 
>
> 1. Loosely speaking computational complexity of an algorithm means the number of operations performed in the algorithm. 
>
> 2. We say that $f(n)\in \mathcal{O}(g(n))$



### General Case 

## Solving Overdetermined Systems

## Solving Underdetermined Systems

## Iterative Methods for solving Linear Systems

Previously, we have seen "direct methods" to solve linear systems like Gauss-Jordan elimination, Partial and complete pivoting. They can be solved exactly with $\infty$ precision machines. But the computational cost is high. 

For lesser computational cost, we use iterative methods. But the issue is that we can not solve exactly using these methods always. 


<!-- All chapters start with a first-level heading followed by your chapter title, like the line above. There should be only one first-level heading (`#`) per .Rmd file. -->

<!-- ## A section -->

<!-- All chapter sections start with a second-level (`##`) or higher heading followed by your section title, like the sections above and below here. You can have as many as you want within a chapter. -->

<!-- ### An unnumbered section {-} -->

<!-- Chapters and sections are numbered by default. To un-number a heading, add a `{.unnumbered}` or the shorter `{-}` at the end of the heading, like in this section. -->
