# Numerical Linear Algebra 

## Columnspace, Nullspace and all
Consider a matrix $A\in\mathbb{R}^{m\times n}$ defined as:
$$A = \begin{bmatrix} -&r_1^T&-\\ -&r_2^T&-\\ & \vdots& \\ - & r_m^T& - \end{bmatrix} = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}$$
where $r_i \in \mathbb{R}^{n\times 1}$ for $1\le i \le m$ are the rows and $a_i \in \mathbb{R}^{m\times 1}$ for $1\le i\le n$ are the columns of $A$.

Columnspace of a matrix $A$ is the span(linear combination) of columns of $A$. Also called as Range of $A$.
\begin{equation}
\text{Range}(A) =\text{Columnspace}(A) =  \{ Ax : x\in \mathbb{R}^{n\times 1} \}
\end{equation}

$$Ax = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}x_1\\x_2\\ \vdots\\ x_n \end{bmatrix} = \sum_{i=1}^n a_i x_i$$


Rowspace of a matrix $A$ is the span(linear combination) of rows of $A$. 
\begin{equation}
\text{Rowspace}(A) = \{ A^Ty : y\in \mathbb{R}^{m\times 1} \} 
\end{equation}

$$A^Ty = \begin{bmatrix} | & | &  & | \\ r_1 & r_2 &\cdots & r_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots\\ y_n \end{bmatrix} = \sum_{i=1}^n r_i y_i$$
Nullspace of a matrix $A$ is defined as follows:
\begin{equation}
\text{Nullspace}(A) = \{ z\in \mathbb{R}^{n\times 1}: Az=0\}
\end{equation}

NOTE:-

1. A linear system $Ax=b$ has a solution ONLY IF $b\in\text{Range}(A)$.

2. Dimension of Range$(A)$ is the number of linearly independent columns of $A$ or the column rank of $A$. Similarly, the dimension of Rowspace$(A)$ is the row rank or the number of independent rows of $A$. 

3. For a matrix $A$, row rank = column rank = rank$\le \min(m,n)$.

4. Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector $z\in \text{Nullspace}(A)$ and $w \in \text{Rowspace}(A)$  , $z$ is orthogonal to $w$.

Proof:- Let $w \in \text{Rowspace}(A)$, then $\exists y\in\mathbb{R}^{n\times 1}$ such that $w = A^Ty$. 

Also as $z\in \text{Nullspace}(A)$, we have $Az=0$.

Therefore,
$$\langle w,z\rangle = w^Tz = y^T Az = 0$$
Thus, nullspace of a matrix is orthogonal to row space of a matrix.

5. **Rank-Nullity Theorem:** Dimension of Nullspace$(A)$+Rank$(A)$ = $n$ = No. of columns of $A$

 

## Matrix Norms
Consider a matrix $A\in\mathbb{R}^{m\times n}$. Just like how we have defined a vector norm, we could have defined an "element wise matrix norm" as follows:
\begin{equation}
\lVert A \rVert_p^* = \left(\sum_{i=1}^n |A_{ij}|^p\right)^{\frac{1}{p}} 
\end{equation}

But this definition of norm does not satisfy the **submultiplicative property**. We are interested in this property as this dictates the convergence of iterative schemes.

A matrix norm is said to be submultiplicative if for any matrices $A\in \mathbb{R}^{m\times k}$ and $B \in \mathbb{R}^{k\times n}$, we have 
\begin{equation}
\lvert AB \rVert \le \lVert A \rVert  \lVert B \rVert
\end{equation}

Consider the case $$A = \begin{bmatrix} 2&2\\2&2 \end{bmatrix}$$ and $p\to \infty$, Therefore we have $$ \lVert A \rVert_{\infty}^* = \max_{1\le i \le m,1 \le j \le n} |A_{ij}| = 2$$.

$$A^2 = \begin{bmatrix} 8 & 8\\ 8& 8 \end{bmatrix}$$
Therefore,
$$\lVert A^2 \rVert_{\infty}^* = 8$$
We can clearly see that:
$$\lVert A^2 \rVert_{\infty}^* = 8 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4$$

<!-- Consider the case $$A = \begin{bmatrix} 1&1\\1&1 \end{bmatrix}$$ and $p=1$. Therefore, we have $$\lVert A \rVert_{1}^* = \sum_i\sum_j |A_{ij}| = 4$$ -->
<!-- As $A$ is an identity matrix, we have $$A^2 = A$$ Therefore $$\lVert A^2 \rVert_{1}^* = 4$$ -->
<!-- We can clearly see that: -->
<!-- $$\lVert A^2 \rVert_{\infty}^* = 4 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4$$ -->
which violates the submultiplicative property.


Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows. 
\begin{equation}
\lVert A \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p} = \sup_{\lVert y \rVert_p = 1} \lVert Ay \rVert_p
\end{equation}

p-norms are submultiplicative.

PROOF:- From the definition of p-norm,
<!-- $$\lVert AB \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}$$ -->
\begin{align*}
\lVert AB \rVert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}\\
&= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \\
&\le \left[\sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \right] \cdot \left[ \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \right]  \\
&= \lVert A \rVert_p \cdot \lVert B \rVert_p\\
\therefore \lVert AB \rVert_p \le \lVert A \rVert_p \cdot \lVert B \rVert_p
\end{align*}

Using this property, we can say that 
\begin{equation}
 \lVert A^n \rVert_p \le \lVert A \rVert^n_p
\end{equation}
 
 $\lVert A\rVert_1$ = Maximum of column sum of absolute values. 

 $\lVert A\rVert_{\infty}$  = Maximum of row sum of absolute values. 

<!-- $$\therefore$$ -->





## Condition number of Matrix vector products

Consider a matrix $A\in\mathbb{R}^{m\times n}$ and a vector $x\in\mathbb{R}^{n\times 1}$. Assume that there is no error in representing $A$. We are interested in finding the condition number of the Matrix-Vector product $f(x;A)= Ax$. 

From the definition of condition number, we can write the condition number $\kappa_r$ of the matrix vector product as:

$$\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_q\le r} \dfrac{\frac{\Vert A(x+\delta x)-Ax \Vert_p}{\Vert Ax \Vert_p}}{\frac{\Vert x+\delta x-x\Vert_q}{\Vert x\Vert_q}}$$
For simplicity let us choose $p=q$. Therefore,
$$\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} \frac{\Vert x \Vert_p}{\Vert Ax\Vert_p}$$
From the definition of matrix p-norm, we can say that: $$\lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} = \Vert A\Vert_p$$
Therefore the condition number of the matrix vector product is:
\begin{equation}
\kappa_r =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}
\end{equation}

From Sub-multiplicative property, as $\Vert Ax\Vert_p \ge\Vert A \Vert_p \Vert x \Vert_p$, we can show that $\kappa_r\ge 1$. 

**Case-1:-** $A\in \mathbb{R}^{m\times n}$ is a fat matrix($m<n$) 

From Rank-Nullity Theorem, we know that 

$$\text{Dimension of Nullspace$(A)$+Rank$(A)$ = $n$}$$

We know that Rank$(A) \le \min(m,n) \implies$ Rank $(A) \le m$ as $A$ is a fat matrix. 

$$\implies \text{Dimension of Nullspace}(A) \ge n-m$$
$\implies \exists$ a non-zero vector $z \in$ Nullspace$(A)$ i.e., $\exists z\in \mathbb{R}^{n\times 1}$ such that $Az=0$.

From the definition of condition number as in equation(), we have:
$\kappa_r(A,x) =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}$

If $x=z$ then as $Az=0$, we have $\Vert Az\Vert_p = 0$. Therefore $\kappa_r \to \infty$. 

Thus, multiplication by a fat matrix is **highly ill-conditioned.**

**Case-2:-** $A$ is an invertible square matrix

From the definition of condition number as in equation(), we have:
\begin{align}
\kappa_r(A,x) &=  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}\\
 &= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}

From submultiplicative property of matrix norms, we can write $\Vert A^{-1}(Ax) \Vert_p \le \Vert A^{-1} \Vert_p \Vert Ax \Vert_p$. Therefore, 

\begin{align}
\kappa_r(A,x)&= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\implies \kappa_r(A,x)&\le \frac{\Vert A \Vert_p \Vert A^{-1} \Vert_p \Vert Ax \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}

\begin{equation}
\kappa_r(A,x) \le \Vert A \Vert_p \Vert A^{-1} \Vert_p
\end{equation}
Therefore condition number is bounded above by $\Vert A \Vert_p \Vert A^{-1} \Vert_p$ which is independent of vector $x$. 

Define Condition number of matrix $A$ as $$\kappa_p(A) = \Vert A \Vert_p \Vert A^{-1} \Vert_p$$.

From submultiplicative property we can show that $\kappa_p(A)\ge 1$. 

Let us find condition number for some special matrices. Let us consider $A=Q$ to be an orthogonal/ unitary matrix. 

As $Q$ is an orthogonal matrix $Q^TQ = I\implies Q^T = Q^{-1}$. 


Therefore $$\Vert Qx \Vert_2^2 = (Qx)^TQx = x^TQ^TQx = x^Tx = \Vert x\Vert_2^2$$ 

Thus, $$\Vert Qx \Vert_2= \Vert x\Vert_2 = \Vert Q^Tx \Vert_2$$ 

From definition of 2-norm of a matrix, we have,
\begin{align}
\Vert Q\Vert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Qx \rVert_p}{\lVert x \rVert_p}\\
\implies \Vert Q\Vert_2 &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert x \rVert_2}{\lVert x \rVert_2}\\
\implies \Vert Q\Vert_2 &= 1
\end{align}

Therefore,
\begin{equation}
\Vert Q^T\Vert_2 = 1
\end{equation}

The condition number of $Q$ is therefore,
\begin{equation}
\kappa_2(Q) = \Vert Q \Vert_p \Vert Q^{-1} \Vert_p = \Vert Q \Vert_2 \Vert Q^T \Vert_2 = 1 
\end{equation}

## Solving Linear Systems

Consider the linear system $Ax=b$ where $A\in\mathbb{R}^{m\times n}$, $\mathbf{x}\in\mathbb{R}^{n\times 1}$ and $\mathbf{b}\in\mathbb{R}^{m\times 1}$ . Inputs are $A, \mathbf{b}$ and output is a vector $\mathbf{x}$. 

In this course, we assume that $A$ is a square(i.e., $m=n$) and invertible matrix (so that we have a unique $x$). Our goal is to find that unique $x$ which satisfies the system of equations. 

Let's go step by step. Let us consider special matrices first and then discuss about a general matrix $A$.

### Special Matrices

1. If $A$ is a unitary matrix, then $AA^T = I=A^TA$. 
$$Ax=b \implies A^T Ax = A^Tb \implies x = A^T b$$
To get $x$, we need to perform $n^2$ multiplications and $n(n-1)$ additions.

2. If $A=U$ is an upper triangular matrix. 

Let $Ux=b$ in matrix form be:
\begin{equation}
\begin{bmatrix}
U_{11} & U_{12} & U_{13} & \cdots & U_{1n}\\
0      & U_{22} & U_{23} & \cdots & U_{2n}\\
0      & 0      & U_{33} & \cdots & U_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0      & 0      & 0      & \cdots & U_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}

The last row of $U$ corresponds to  the equation 
\begin{equation}
U_{nn} x_n = b_n
\end{equation}

Now as $U$ is invertible, all the diagonal elements are non-zero i.e., $U_{ii}\neq 0 \ \forall i \in\{1,2,\cdots , n\}$. 

Therefore 
\begin{equation}
U_{nn} x_n = b_n \implies x_n = \frac{b_n}{U_{nn}}
\end{equation}

Similarly, we have:
\begin{equation}
U_{(n-1),(n-1)}x_{n-1}+U_{(n-1),n}x_n = b_{n-1} \implies x_{n-1} = \frac{b_{n-1}-U_{(n-1),n}x_n}{U_{(n-1),(n-1)}}
\end{equation}

Using induction, we can prove that for $i \in \{1,2,\cdots,n-1\}$, we have:
\begin{equation}
x_i = \frac{b_i -\sum_{j=i+1}^n U_{i,j}x_j}{U_{ii}}
\end{equation}

To calculate $x_i$(for $i \in \{1,2,\cdots,n\}$), we need to perform 1 division, $(n-i)$ multiplications and $(n-i)$ additions/subtractions. 

Therefore to calculate the output vector $x$, we need to perform 

* $\sum_{i=1}^n 1 = n$ divisions

* $\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}$ multiplications

* $\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}$ additions/subtractions

* In total $n+ \frac{n^2-n}{2}+\frac{n^2-n}{2} = n^2$ operations are required to get $x$. In other words "Computational Complexity is $n^2$"

> **NOTE:- ** 
>
> 1. Loosely speaking computational complexity of an algorithm means the number of operations performed in the algorithm. 
>
> 2. We say that $f(n)\in \mathcal{O}(g(n))$



### General Case 

We know that to solve $Ax=b$, we convert the Augmented matrix $[A|b]$ to its Row Reduced Echelon Foem(RREF) by doing elementary row operations on $A$ to get an Identity matrix.

Instead of fully converting to an Identity matrix, let us convert to an upper triangular matrix, say $U$.i.e., we do row operations to get $$Ux=c$$ from $Ax=b$ where $U$ is an upper triangular matrix and $c$ is the vector obtained by performing corresponding operations on $b$.

Let $Ax=b$ in matrix form be:
\begin{equation}
\begin{bmatrix}
A_{11} & A_{12} & A_{13} & \cdots & A_{1n}\\
A_{21} & A_{22} & A_{23} & \cdots & A_{2n}\\
A_{31} & A_{32} & A_{33} & \cdots & A_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
A_{n1} & A_{n2} & A_{n3} & \cdots & A_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}

To convert $A$ to an upper triangular matrix $U$ (i.e., to have $U_{ij} = 0$ for $i>j$) using row operations, we need to make all elements below the diagonal to be zero. To achieve this, we firstly make all elements below $A_{11}$ in the 1st column to be zero using the below operations on $j^{th}$ row ($j>1$) :(We assume that $A_{11}\neq 0$) 

\begin{align}
R_{j,1} &=0\\
R_{j,2:n} &\gets R_{j,2:n} - \frac{A_{j1}}{A_{11}}R_{1,2:n} \\
b_j &\gets b_j - \frac{A_{j1}}{A_{11}} b_1
\end{align}

We proceed like this to make all elements below the diagonal to be zero. Consider $k^{th}$ step in this process. Let the matrix in this step be:

\begin{equation}
\begin{bmatrix}
A_{11} & A_{12} & A_{13} & \cdots & A_{1,k} & \cdots & A_{1n}\\
0      & A_{22} & A_{23} & \cdots & A_{2,k} & \cdots & A_{2n}\\
0      & 0      & A_{33} & \cdots & A_{3,k} & \cdots & A_{3n}\\
\vdots & \vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0      & 0      & 0      & \cdots & A_{k,k} & \cdots & A_{k,n}\\
0      & 0      & 0      & \cdots & A_{k+1,k} & \cdots & A_{k+1,n}\\
\vdots & \vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0      & 0      & 0      & \cdots & A_{n,k} & \cdots & A_{n,n}\\
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}
The row operations, to be done on $j^{th}$ row, so that all elements below $A_{kk}$ become zero (in $k^{th}$ column) is given as follows: For $j>k$ and $A_{k,k}\neq 0$

\begin{align}
R_{j,k} &=0\\
R_{j,k+1:n} &\gets R_{j,k+1:n} - \frac{A_{j,k}}{A_{k,k}}R_{k,k+1:n} \\
b_j &\gets b_j - \frac{A_{j,k}}{A_{k,k}} b_k
\end{align}

Let us now calculate the number of operations required to be done for converting $A$ to RREF. In 1st step, for the row operation $$R_{j,2:n} \gets R_{j,2:n} - \frac{A_{j1}}{A_{11}}R_{1,2:n}$$ for a particular row-$j$, we need to do 1 division, $n-1$ multiplications and $n-1$ additions. There are $n-1$ such rows. Therefore, for the row operation on all these $n-1$ rows, $n-1$ divisions, $(n-1)^2$ multiplications and $(n-1)^2$ additions have to be done. 

For $k^{th}$ step($1\le k\le n-1$), row operation $$R_{j,k+1:n} \gets R_{j,k+1:n} - \frac{A_{j,k}}{A_{k,k}}R_{k,k+1:n}$$ on row-$j$ for $k+1\le j\le n$, requires 1 division, $(n-k)$ multiplications and $(n-k)$ additions. Therefore, we need
to perform $n-k$ divisions, $(n-k)^2$ multiplications and $(n-k)^2$ additions. 

Therefore, the number of operations required to convert $A$ to $U$ are:

1. $\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}$ Divisions

2. $\sum_{k=0}^{n-1} (n-k)^2 = \sum_{k=1}^n k^2 = \frac{n(n-1)(2n-1)}{6}$ Multiplications

3. $\sum_{k=0}^{n-1} (n-k)^2 = \sum_{k=1}^n k^2 = \frac{n(n-1)(2n-1)}{6}$ Additions

Total number of operations to convert from $A$ to $U$ = $\frac{n(n-1)}{2}+2\frac{n(n-1)(2n-1)}{6}$

Now, let us calculate the number of operations required to get $c$ from $b$. Consider the operation on the $k^{th}$ step($1\le k\le n-1$): $$b_j \gets b_j - \frac{A_{j,k}}{A_{k,k}} b_k$$. This operation requires $(n-k)$ multiplications and $(n-k)$ additions. Note that we have already calculated $\frac{A_{j,k}}{A_{k,k}}$ while converting $A$ to $U$. Hence, no separate division operation is involved. 

Therefore, number of operations on RHS are:

1. $\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}$ Multiplications.

2. $\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}$ Additions

After converting $Ax=b$ to $Ux=c$, we have seen that solving $Ux=c$ requires $n$ divisions, $\frac{n(n-1)}{2}$ additions and $\frac{n(n-1)}{2}$ multiplications. 

Therefore, for solving $Ax=b$:

1. Number of divisions required = $\frac{n(n+1)}{2}+n$

2. Number of multiplications required = $\frac{n(n-1)(2n-1)}{6}+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}$

3. Number of additions required = $\frac{n(n-1)(2n-1)}{6}+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}$

We can see that multiplications & addition operations are the most expensive(time consuming)

Therefore, the total number of operations = $2\left[ \frac{n(n-1)(2n-1)}{6}+n(n-1)\right]+\frac{n(n+1)}{2}+n$

In other words Total number of operations $\in \mathcal{O}(n^3)$

Now the question arises. Where will this fail?

1. We have assumed that $A_{11} \neq 0$ in the first step. This may not always hold true. Also, there is a possibility that $A_{kk}=0$ in the intermediate steps also. If this occurs then we can't proceed further. 

2. Precision issues might occur. Consider solving the system of equations. 

\begin{align}
2^{-60}x_1+x_2&=1\\
x_1+x_2&=2
\end{align}
The equations can be written in matrix form as:
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
1& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\2
\end{bmatrix}
\end{equation}

The solution is $x_1 \approx 1$ and $x_2 \approx 1$. 

Let us try to solve this with the help of method described in this section. 
First, we do the row operations $R_1 \gets R_2 - \frac{1}{2^{-60}}R_1 \implies R_1 \gets R_2 - 2^{60} R_2$ to get:
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
0& 1-2^{60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\2-2^{60}
\end{bmatrix}
\end{equation}

$1-2^{60}$  and $2-2^{60}$ is represented on the 64 bit machine as $2^{-60}$. Therefore, the machine represents the equations as:
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
0& -2^{60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\-2^{60}
\end{bmatrix}
\end{equation}

This implies that $x_2=1$ and $2^{-60}x_1+x_2=1\implies x_1=0$. But this is **significantly** different from the exact solution. 

If we have had an infinite precision machine, then we would have got accurate results. But since, we have only finite precision machines, we have this issue. 

Let us calculate condition number of this matrix. The condition number as calculated by MATLAB using ```cond()``` function gives condition number as 2.6180 which is a relatively small condition number. This means that small changes in input don't give very large changes in output. Hence it is a well conditioned problem. Therefore, condition number is not an issue. 

Therefore, Algorithm has to be blamed here 

We say that an algorithm is stable if it gives almost correct answer to a well conditioned problem. As this gives a completely different answer, we say that this algorithm is unstable.(even if it doesn't encounter zero in the diagonal)

How to overcome these issues? Let us think this way. For the first issue, if a zero diagonal entry is encountered, let us try swapping row with a row which has a non-zero element in that column. Now the question arises which row? We tend to swap with the row which has largest absolute value. Instead of swapping only when zero is encountered, we do it in every step. 

Let us consider the example problem, we were discussing earlier.
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
1& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\2
\end{bmatrix}
\end{equation}

In first column, the maximum value is 1. Thus, we swap 1st and 2nd rows. 
\begin{equation}
\begin{bmatrix}
1& 1\\
2^{-60}& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
2\\1
\end{bmatrix}
\end{equation}
We do a row operation $R_2\gets R_2-2^{-60}R_1$ to get:
\begin{equation}
\begin{bmatrix}
1& 1\\
0& 1-2^{-60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
2\\1-2^{-59}
\end{bmatrix}
\end{equation}

$1-2^{-60}$  and $1-2^{-59}$ is represented on the 64 bit machine as $1$. Therefore, the machine represents the equations as:
\begin{equation}
\begin{bmatrix}
1& 1\\
0& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
2\\1
\end{bmatrix}
\end{equation}

The solution to these system of equations give $x_2=1$ and $x_1=1$.

This method in which we swap rows is called **partial pivoting**. Partial pivoting is seen to be stable in practice. 

Eg: Solve the system using partial pivoting
\begin{align}
x_1+2x_2+3x_3 &=6\\
x_1+2x_2+4x_3 &=7\\
x_1-2x_2+x_3  &= 0
\end{align}

A much more better way is including a column swap as well. The process is decribed in the below example. This process is called **Complete Pivoting** which is stable but has more computation cost(due to multiple swaps/ comparisions)

Eg: Solve the system using complete pivoting.
\begin{align}
x_1+2x_2+3x_3 &= 6\\
x_1+2x_2+4x_3 &= 7\\
x_1-2x_2+10x_3&= -11
\end{align}

**ADD SOLUTION**

Order of computational cost: Complete Pivoting > Partial Pivoting > No Pivoting

Order of Stability: Complete Pivoting > Partial Pivoting > No Pivoting

## Solving Overdetermined Systems

## Solving Underdetermined Systems

## Iterative Methods for solving Linear Systems

Previously, we have seen "direct methods" to solve linear systems like Gauss-Jordan elimination, Partial and complete pivoting. They can be solved exactly with $\infty$ precision machines. But the computational cost is high. 

For lesser computational cost, we use iterative methods. But the issue is that we can not solve exactly using these methods always. 


<!-- All chapters start with a first-level heading followed by your chapter title, like the line above. There should be only one first-level heading (`#`) per .Rmd file. -->

<!-- ## A section -->

<!-- All chapter sections start with a second-level (`##`) or higher heading followed by your section title, like the sections above and below here. You can have as many as you want within a chapter. -->

<!-- ### An unnumbered section {-} -->

<!-- Chapters and sections are numbered by default. To un-number a heading, add a `{.unnumbered}` or the shorter `{-}` at the end of the heading, like in this section. -->
