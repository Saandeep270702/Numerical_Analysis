<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 2 Numerical Linear Algebra | Numerical Analysis</title>
  <meta name="description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="generator" content="bookdown 0.34 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 2 Numerical Linear Algebra | Numerical Analysis" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 2 Numerical Linear Algebra | Numerical Analysis" />
  
  <meta name="twitter:description" content="<p>This is a minimal example of using the bookdown package to write a book.
The HTML output format for this example is bookdown::gitbook,
set in the _output.yml file.</p>" />
  

<meta name="author" content="Sai Saandeep.S, Dr. Sivaram" />


<meta name="date" content="2023-07-16" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="index.html"/>
<link rel="next" href="interpolation.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>



<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">A Minimal Book Example</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Introduction</a>
<ul>
<li class="chapter" data-level="1.1" data-path="index.html"><a href="index.html#why-numerical-analysis"><i class="fa fa-check"></i><b>1.1</b> Why Numerical Analysis?</a></li>
<li class="chapter" data-level="1.2" data-path="index.html"><a href="index.html#representing-numbers-on-a-machine"><i class="fa fa-check"></i><b>1.2</b> Representing Numbers on a Machine</a></li>
<li class="chapter" data-level="1.3" data-path="index.html"><a href="index.html#condition-number-of-a-problem"><i class="fa fa-check"></i><b>1.3</b> Condition Number of a Problem</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="index.html"><a href="index.html#vector-normsrecap"><i class="fa fa-check"></i><b>1.3.1</b> Vector Norms(Recap)</a></li>
<li class="chapter" data-level="1.3.2" data-path="index.html"><a href="index.html#examples-on-finding-condition-number"><i class="fa fa-check"></i><b>1.3.2</b> Examples on finding Condition number</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html"><i class="fa fa-check"></i><b>2</b> Numerical Linear Algebra</a>
<ul>
<li class="chapter" data-level="2.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#columnspace-nullspace-and-all"><i class="fa fa-check"></i><b>2.1</b> Columnspace, Nullspace and all</a></li>
<li class="chapter" data-level="2.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#matrix-norms"><i class="fa fa-check"></i><b>2.2</b> Matrix Norms</a></li>
<li class="chapter" data-level="2.3" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#condition-number-of-matrix-vector-products"><i class="fa fa-check"></i><b>2.3</b> Condition number of Matrix vector products</a></li>
<li class="chapter" data-level="2.4" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#solving-linear-systems"><i class="fa fa-check"></i><b>2.4</b> Solving Linear Systems</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#special-matrices"><i class="fa fa-check"></i><b>2.4.1</b> Special Matrices</a></li>
<li class="chapter" data-level="2.4.2" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#general-case"><i class="fa fa-check"></i><b>2.4.2</b> General Case</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#solving-overdetermined-systems"><i class="fa fa-check"></i><b>2.5</b> Solving Overdetermined Systems</a></li>
<li class="chapter" data-level="2.6" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#solving-underdetermined-systems"><i class="fa fa-check"></i><b>2.6</b> Solving Underdetermined Systems</a></li>
<li class="chapter" data-level="2.7" data-path="numerical-linear-algebra.html"><a href="numerical-linear-algebra.html#iterative-methods-for-solving-linear-systems"><i class="fa fa-check"></i><b>2.7</b> Iterative Methods for solving Linear Systems</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="interpolation.html"><a href="interpolation.html"><i class="fa fa-check"></i><b>3</b> Interpolation</a>
<ul>
<li class="chapter" data-level="3.1" data-path="interpolation.html"><a href="interpolation.html#motivation---interpolation-vs.-approximation"><i class="fa fa-check"></i><b>3.1</b> Motivation - Interpolation vs. Approximation</a></li>
<li class="chapter" data-level="3.2" data-path="interpolation.html"><a href="interpolation.html#lagrange-interpolation"><i class="fa fa-check"></i><b>3.2</b> Lagrange Interpolation</a>
<ul>
<li class="chapter" data-level="3.2.1" data-path="interpolation.html"><a href="interpolation.html#motivation"><i class="fa fa-check"></i><b>3.2.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2.2" data-path="interpolation.html"><a href="interpolation.html#lagrange-interpolant"><i class="fa fa-check"></i><b>3.2.2</b> Lagrange Interpolant</a></li>
</ul></li>
<li class="chapter" data-level="3.3" data-path="interpolation.html"><a href="interpolation.html#choice-of-nodes"><i class="fa fa-check"></i><b>3.3</b> Choice of Nodes</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="interpolation.html"><a href="interpolation.html#motivation-1"><i class="fa fa-check"></i><b>3.3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.3.2" data-path="interpolation.html"><a href="interpolation.html#fundamental-theorem-of-polynomial-interpolation"><i class="fa fa-check"></i><b>3.3.2</b> Fundamental Theorem of Polynomial Interpolation</a></li>
<li class="chapter" data-level="3.3.3" data-path="interpolation.html"><a href="interpolation.html#different-possible-types-of-nodes"><i class="fa fa-check"></i><b>3.3.3</b> Different Possible types of nodes</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="interpolation.html"><a href="interpolation.html#wierstrass-approximation-theorem"><i class="fa fa-check"></i><b>3.4</b> Wierstrass Approximation theorem</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="quadratures.html"><a href="quadratures.html"><i class="fa fa-check"></i><b>4</b> Quadratures</a>
<ul>
<li class="chapter" data-level="4.1" data-path="quadratures.html"><a href="quadratures.html#different-schemes"><i class="fa fa-check"></i><b>4.1</b> Different Schemes</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="quadratures.html"><a href="quadratures.html#basic-schemes"><i class="fa fa-check"></i><b>4.1.1</b> Basic Schemes</a></li>
<li class="chapter" data-level="4.1.2" data-path="quadratures.html"><a href="quadratures.html#accuracy-of-these-schemes"><i class="fa fa-check"></i><b>4.1.2</b> Accuracy of these schemes</a></li>
<li class="chapter" data-level="4.1.3" data-path="quadratures.html"><a href="quadratures.html#simpsons-rule"><i class="fa fa-check"></i><b>4.1.3</b> Simpson’s Rule</a></li>
<li class="chapter" data-level="4.1.4" data-path="quadratures.html"><a href="quadratures.html#trapezoidal-rule-with-end-point-corrections"><i class="fa fa-check"></i><b>4.1.4</b> Trapezoidal Rule with End point Corrections</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="quadratures.html"><a href="quadratures.html#euler-maclaurian-formula"><i class="fa fa-check"></i><b>4.2</b> Euler-Maclaurian Formula</a>
<ul>
<li class="chapter" data-level="4.2.1" data-path="quadratures.html"><a href="quadratures.html#introduction-1"><i class="fa fa-check"></i><b>4.2.1</b> Introduction</a></li>
<li class="chapter" data-level="4.2.2" data-path="quadratures.html"><a href="quadratures.html#formula-and-derivation"><i class="fa fa-check"></i><b>4.2.2</b> Formula and Derivation</a></li>
<li class="chapter" data-level="4.2.3" data-path="quadratures.html"><a href="quadratures.html#higher-order-quadratures"><i class="fa fa-check"></i><b>4.2.3</b> Higher Order Quadratures</a></li>
</ul></li>
<li class="chapter" data-level="4.3" data-path="quadratures.html"><a href="quadratures.html#gaussian-quadratures"><i class="fa fa-check"></i><b>4.3</b> Gaussian Quadratures</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="root-finding-algorithms.html"><a href="root-finding-algorithms.html"><i class="fa fa-check"></i><b>5</b> Root Finding Algorithms</a>
<ul>
<li class="chapter" data-level="5.1" data-path="root-finding-algorithms.html"><a href="root-finding-algorithms.html#motivation-2"><i class="fa fa-check"></i><b>5.1</b> Motivation</a></li>
<li class="chapter" data-level="5.2" data-path="root-finding-algorithms.html"><a href="root-finding-algorithms.html#bisection-method"><i class="fa fa-check"></i><b>5.2</b> Bisection Method</a></li>
<li class="chapter" data-level="5.3" data-path="root-finding-algorithms.html"><a href="root-finding-algorithms.html#newton-method"><i class="fa fa-check"></i><b>5.3</b> Newton Method</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="root-finding-algorithms.html"><a href="root-finding-algorithms.html#rate-of-convergence"><i class="fa fa-check"></i><b>5.3.1</b> Rate of Convergence</a></li>
<li class="chapter" data-level="5.3.2" data-path="root-finding-algorithms.html"><a href="root-finding-algorithms.html#possibility-of-non-convergence"><i class="fa fa-check"></i><b>5.3.2</b> Possibility of Non-Convergence?</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="numerical-differentiation.html"><a href="numerical-differentiation.html"><i class="fa fa-check"></i><b>6</b> Numerical Differentiation</a>
<ul>
<li class="chapter" data-level="6.1" data-path="numerical-differentiation.html"><a href="numerical-differentiation.html#motivation-3"><i class="fa fa-check"></i><b>6.1</b> Motivation</a></li>
<li class="chapter" data-level="6.2" data-path="numerical-differentiation.html"><a href="numerical-differentiation.html#finite-differences"><i class="fa fa-check"></i><b>6.2</b> Finite Differences</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html"><i class="fa fa-check"></i><b>7</b> Solving Ordinary Differential Equations Numerically</a>
<ul>
<li class="chapter" data-level="7.1" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html#introduction-2"><i class="fa fa-check"></i><b>7.1</b> Introduction</a></li>
<li class="chapter" data-level="7.2" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html#different-basic-methods"><i class="fa fa-check"></i><b>7.2</b> Different Basic Methods</a></li>
<li class="chapter" data-level="7.3" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html#linear-stability-analysis"><i class="fa fa-check"></i><b>7.3</b> Linear Stability Analysis</a></li>
<li class="chapter" data-level="7.4" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html#accuracy"><i class="fa fa-check"></i><b>7.4</b> Accuracy</a></li>
<li class="chapter" data-level="7.5" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html#simple-pendulum---solving-system-of-odes"><i class="fa fa-check"></i><b>7.5</b> Simple Pendulum - Solving System of ODEs</a></li>
<li class="chapter" data-level="7.6" data-path="solving-ordinary-differential-equations-numerically.html"><a href="solving-ordinary-differential-equations-numerically.html#simple-pendulum---finite-difference-for-2nd-order-derivative"><i class="fa fa-check"></i><b>7.6</b> Simple Pendulum - Finite difference for 2nd order derivative</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Numerical Analysis</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="numerical-linear-algebra" class="section level1 hasAnchor" number="2">
<h1><span class="header-section-number">Chapter 2</span> Numerical Linear Algebra<a href="numerical-linear-algebra.html#numerical-linear-algebra" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<div id="columnspace-nullspace-and-all" class="section level2 hasAnchor" number="2.1">
<h2><span class="header-section-number">2.1</span> Columnspace, Nullspace and all<a href="numerical-linear-algebra.html#columnspace-nullspace-and-all" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a matrix <span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span> defined as:
<span class="math display">\[A = \begin{bmatrix} -&amp;r_1^T&amp;-\\ -&amp;r_2^T&amp;-\\ &amp; \vdots&amp; \\ - &amp; r_m^T&amp; - \end{bmatrix} = \begin{bmatrix} | &amp; | &amp;  &amp; | \\ a_1 &amp; a_2 &amp;\cdots &amp; a_n \\ | &amp; | &amp; &amp; |\end{bmatrix}\]</span>
where <span class="math inline">\(r_i \in \mathbb{R}^{n\times 1}\)</span> for <span class="math inline">\(1\le i \le m\)</span> are the rows and <span class="math inline">\(a_i \in \mathbb{R}^{m\times 1}\)</span> for <span class="math inline">\(1\le i\le n\)</span> are the columns of <span class="math inline">\(A\)</span>.</p>
<p>Columnspace of a matrix <span class="math inline">\(A\)</span> is the span(linear combination) of columns of <span class="math inline">\(A\)</span>. Also called as Range of <span class="math inline">\(A\)</span>.
<span class="math display">\[\begin{equation}
\text{Range}(A) =\text{Columnspace}(A) =  \{ Ax : x\in \mathbb{R}^{n\times 1} \}
\end{equation}\]</span></p>
<p><span class="math display">\[Ax = \begin{bmatrix} | &amp; | &amp;  &amp; | \\ a_1 &amp; a_2 &amp;\cdots &amp; a_n \\ | &amp; | &amp; &amp; |\end{bmatrix}\begin{bmatrix}x_1\\x_2\\ \vdots\\ x_n \end{bmatrix} = \sum_{i=1}^n a_i x_i\]</span></p>
<p>Rowspace of a matrix <span class="math inline">\(A\)</span> is the span(linear combination) of rows of <span class="math inline">\(A\)</span>.
<span class="math display">\[\begin{equation}
\text{Rowspace}(A) = \{ A^Ty : y\in \mathbb{R}^{m\times 1} \}
\end{equation}\]</span></p>
<p><span class="math display">\[A^Ty = \begin{bmatrix} | &amp; | &amp;  &amp; | \\ r_1 &amp; r_2 &amp;\cdots &amp; r_n \\ | &amp; | &amp; &amp; |\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots\\ y_n \end{bmatrix} = \sum_{i=1}^n r_i y_i\]</span>
Nullspace of a matrix <span class="math inline">\(A\)</span> is defined as follows:
<span class="math display">\[\begin{equation}
\text{Nullspace}(A) = \{ z\in \mathbb{R}^{n\times 1}: Az=0\}
\end{equation}\]</span></p>
<p>NOTE:-</p>
<ol style="list-style-type: decimal">
<li><p>A linear system <span class="math inline">\(Ax=b\)</span> has a solution ONLY IF <span class="math inline">\(b\in\text{Range}(A)\)</span>.</p></li>
<li><p>Dimension of Range<span class="math inline">\((A)\)</span> is the number of linearly independent columns of <span class="math inline">\(A\)</span> or the column rank of <span class="math inline">\(A\)</span>. Similarly, the dimension of Rowspace<span class="math inline">\((A)\)</span> is the row rank or the number of independent rows of <span class="math inline">\(A\)</span>.</p></li>
<li><p>For a matrix <span class="math inline">\(A\)</span>, row rank = column rank = rank<span class="math inline">\(\le \min(m,n)\)</span>.</p></li>
<li><p>Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector <span class="math inline">\(z\in \text{Nullspace}(A)\)</span> and <span class="math inline">\(w \in \text{Rowspace}(A)\)</span> , <span class="math inline">\(z\)</span> is orthogonal to <span class="math inline">\(w\)</span>.</p></li>
</ol>
<p>Proof:- Let <span class="math inline">\(w \in \text{Rowspace}(A)\)</span>, then <span class="math inline">\(\exists y\in\mathbb{R}^{n\times 1}\)</span> such that <span class="math inline">\(w = A^Ty\)</span>.</p>
<p>Also as <span class="math inline">\(z\in \text{Nullspace}(A)\)</span>, we have <span class="math inline">\(Az=0\)</span>.</p>
<p>Therefore,
<span class="math display">\[\langle w,z\rangle = w^Tz = y^T Az = 0\]</span>
Thus, nullspace of a matrix is orthogonal to row space of a matrix.</p>
<ol start="5" style="list-style-type: decimal">
<li><strong>Rank-Nullity Theorem:</strong> Dimension of Nullspace<span class="math inline">\((A)\)</span>+Rank<span class="math inline">\((A)\)</span> = <span class="math inline">\(n\)</span> = No. of columns of <span class="math inline">\(A\)</span></li>
</ol>
</div>
<div id="matrix-norms" class="section level2 hasAnchor" number="2.2">
<h2><span class="header-section-number">2.2</span> Matrix Norms<a href="numerical-linear-algebra.html#matrix-norms" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a matrix <span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span>. Just like how we have defined a vector norm, we could have defined an “element wise matrix norm” as follows:
<span class="math display">\[\begin{equation}
\lVert A \rVert_p^* = \left(\sum_{i=1}^n |A_{ij}|^p\right)^{\frac{1}{p}}
\end{equation}\]</span></p>
<p>But this definition of norm does not satisfy the <strong>submultiplicative property</strong>. We are interested in this property as this dictates the convergence of iterative schemes.</p>
<p>A matrix norm is said to be submultiplicative if for any matrices <span class="math inline">\(A\in \mathbb{R}^{m\times k}\)</span> and <span class="math inline">\(B \in \mathbb{R}^{k\times n}\)</span>, we have
<span class="math display">\[\begin{equation}
\lvert AB \rVert \le \lVert A \rVert  \lVert B \rVert
\end{equation}\]</span></p>
<p>Consider the case <span class="math display">\[A = \begin{bmatrix} 2&amp;2\\2&amp;2 \end{bmatrix}\]</span> and <span class="math inline">\(p\to \infty\)</span>, Therefore we have <span class="math display">\[ \lVert A \rVert_{\infty}^* = \max_{1\le i \le m,1 \le j \le n} |A_{ij}| = 2\]</span>.</p>
<p><span class="math display">\[A^2 = \begin{bmatrix} 8 &amp; 8\\ 8&amp; 8 \end{bmatrix}\]</span>
Therefore,
<span class="math display">\[\lVert A^2 \rVert_{\infty}^* = 8\]</span>
We can clearly see that:
<span class="math display">\[\lVert A^2 \rVert_{\infty}^* = 8 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4\]</span></p>
<!-- Consider the case $$A = \begin{bmatrix} 1&1\\1&1 \end{bmatrix}$$ and $p=1$. Therefore, we have $$\lVert A \rVert_{1}^* = \sum_i\sum_j |A_{ij}| = 4$$ -->
<!-- As $A$ is an identity matrix, we have $$A^2 = A$$ Therefore $$\lVert A^2 \rVert_{1}^* = 4$$ -->
<!-- We can clearly see that: -->
<!-- $$\lVert A^2 \rVert_{\infty}^* = 4 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4$$ -->
<p>which violates the submultiplicative property.</p>
<p>Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows.
<span class="math display">\[\begin{equation}
\lVert A \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p} = \sup_{\lVert y \rVert_p = 1} \lVert Ay \rVert_p
\end{equation}\]</span></p>
<p>p-norms are submultiplicative.</p>
<p>PROOF:- From the definition of p-norm,
<!-- $$\lVert AB \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}$$ -->
<span class="math display">\[\begin{align*}
\lVert AB \rVert_p &amp;= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}\\
&amp;= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \\
&amp;\le \left[\sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \right] \cdot \left[ \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \right]  \\
&amp;= \lVert A \rVert_p \cdot \lVert B \rVert_p\\
\therefore \lVert AB \rVert_p \le \lVert A \rVert_p \cdot \lVert B \rVert_p
\end{align*}\]</span></p>
<p>Using this property, we can say that
<span class="math display">\[\begin{equation}
\lVert A^n \rVert_p \le \lVert A \rVert^n_p
\end{equation}\]</span></p>
<p><span class="math inline">\(\lVert A\rVert_1\)</span> = Maximum of column sum of absolute values.</p>
<p><span class="math inline">\(\lVert A\rVert_{\infty}\)</span> = Maximum of row sum of absolute values.</p>
<!-- $$\therefore$$ -->
</div>
<div id="condition-number-of-matrix-vector-products" class="section level2 hasAnchor" number="2.3">
<h2><span class="header-section-number">2.3</span> Condition number of Matrix vector products<a href="numerical-linear-algebra.html#condition-number-of-matrix-vector-products" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider a matrix <span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span> and a vector <span class="math inline">\(x\in\mathbb{R}^{n\times 1}\)</span>. Assume that there is no error in representing <span class="math inline">\(A\)</span>. We are interested in finding the condition number of the Matrix-Vector product <span class="math inline">\(f(x;A)= Ax\)</span>.</p>
<p>From the definition of condition number, we can write the condition number <span class="math inline">\(\kappa_r\)</span> of the matrix vector product as:</p>
<p><span class="math display">\[\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_q\le r} \dfrac{\frac{\Vert A(x+\delta x)-Ax \Vert_p}{\Vert Ax \Vert_p}}{\frac{\Vert x+\delta x-x\Vert_q}{\Vert x\Vert_q}}\]</span>
For simplicity let us choose <span class="math inline">\(p=q\)</span>. Therefore,
<span class="math display">\[\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} \frac{\Vert x \Vert_p}{\Vert Ax\Vert_p}\]</span>
From the definition of matrix p-norm, we can say that: <span class="math display">\[\lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} = \Vert A\Vert_p\]</span>
Therefore the condition number of the matrix vector product is:
<span class="math display">\[\begin{equation}
\kappa_r =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}
\end{equation}\]</span></p>
<p>From Sub-multiplicative property, as <span class="math inline">\(\Vert Ax\Vert_p \ge\Vert A \Vert_p \Vert x \Vert_p\)</span>, we can show that <span class="math inline">\(\kappa_r\ge 1\)</span>.</p>
<p><strong>Case-1:-</strong> <span class="math inline">\(A\in \mathbb{R}^{m\times n}\)</span> is a fat matrix(<span class="math inline">\(m&lt;n\)</span>)</p>
<p>From Rank-Nullity Theorem, we know that</p>
<p><span class="math display">\[\text{Dimension of Nullspace$(A)$+Rank$(A)$ = $n$}\]</span></p>
<p>We know that Rank<span class="math inline">\((A) \le \min(m,n) \implies\)</span> Rank <span class="math inline">\((A) \le m\)</span> as <span class="math inline">\(A\)</span> is a fat matrix.</p>
<p><span class="math display">\[\implies \text{Dimension of Nullspace}(A) \ge n-m\]</span>
<span class="math inline">\(\implies \exists\)</span> a non-zero vector <span class="math inline">\(z \in\)</span> Nullspace<span class="math inline">\((A)\)</span> i.e., <span class="math inline">\(\exists z\in \mathbb{R}^{n\times 1}\)</span> such that <span class="math inline">\(Az=0\)</span>.</p>
<p>From the definition of condition number as in equation(), we have:
<span class="math inline">\(\kappa_r(A,x) = \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}\)</span></p>
<p>If <span class="math inline">\(x=z\)</span> then as <span class="math inline">\(Az=0\)</span>, we have <span class="math inline">\(\Vert Az\Vert_p = 0\)</span>. Therefore <span class="math inline">\(\kappa_r \to \infty\)</span>.</p>
<p>Thus, multiplication by a fat matrix is <strong>highly ill-conditioned.</strong></p>
<p><strong>Case-2:-</strong> <span class="math inline">\(A\)</span> is an invertible square matrix</p>
<p>From the definition of condition number as in equation(), we have:
<span class="math display">\[\begin{align}
\kappa_r(A,x) &amp;=  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}\\
&amp;= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}\]</span></p>
<p>From submultiplicative property of matrix norms, we can write <span class="math inline">\(\Vert A^{-1}(Ax) \Vert_p \le \Vert A^{-1} \Vert_p \Vert Ax \Vert_p\)</span>. Therefore,</p>
<p><span class="math display">\[\begin{align}
\kappa_r(A,x)&amp;= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\implies \kappa_r(A,x)&amp;\le \frac{\Vert A \Vert_p \Vert A^{-1} \Vert_p \Vert Ax \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}\]</span></p>
<p><span class="math display">\[\begin{equation}
\kappa_r(A,x) \le \Vert A \Vert_p \Vert A^{-1} \Vert_p
\end{equation}\]</span>
Therefore condition number is bounded above by <span class="math inline">\(\Vert A \Vert_p \Vert A^{-1} \Vert_p\)</span> which is independent of vector <span class="math inline">\(x\)</span>.</p>
<p>Define Condition number of matrix <span class="math inline">\(A\)</span> as <span class="math display">\[\kappa_p(A) = \Vert A \Vert_p \Vert A^{-1} \Vert_p\]</span>.</p>
<p>From submultiplicative property we can show that <span class="math inline">\(\kappa_p(A)\ge 1\)</span>.</p>
<p>Let us find condition number for some special matrices. Let us consider <span class="math inline">\(A=Q\)</span> to be an orthogonal/ unitary matrix.</p>
<p>As <span class="math inline">\(Q\)</span> is an orthogonal matrix <span class="math inline">\(Q^TQ = I\implies Q^T = Q^{-1}\)</span>.</p>
<p>Therefore <span class="math display">\[\Vert Qx \Vert_2^2 = (Qx)^TQx = x^TQ^TQx = x^Tx = \Vert x\Vert_2^2\]</span></p>
<p>Thus, <span class="math display">\[\Vert Qx \Vert_2= \Vert x\Vert_2 = \Vert Q^Tx \Vert_2\]</span></p>
<p>From definition of 2-norm of a matrix, we have,
<span class="math display">\[\begin{align}
\Vert Q\Vert_p &amp;= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Qx \rVert_p}{\lVert x \rVert_p}\\
\implies \Vert Q\Vert_2 &amp;= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert x \rVert_2}{\lVert x \rVert_2}\\
\implies \Vert Q\Vert_2 &amp;= 1
\end{align}\]</span></p>
<p>Therefore,
<span class="math display">\[\begin{equation}
\Vert Q^T\Vert_2 = 1
\end{equation}\]</span></p>
<p>The condition number of <span class="math inline">\(Q\)</span> is therefore,
<span class="math display">\[\begin{equation}
\kappa_2(Q) = \Vert Q \Vert_p \Vert Q^{-1} \Vert_p = \Vert Q \Vert_2 \Vert Q^T \Vert_2 = 1
\end{equation}\]</span></p>
</div>
<div id="solving-linear-systems" class="section level2 hasAnchor" number="2.4">
<h2><span class="header-section-number">2.4</span> Solving Linear Systems<a href="numerical-linear-algebra.html#solving-linear-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Consider the linear system <span class="math inline">\(Ax=b\)</span> where <span class="math inline">\(A\in\mathbb{R}^{m\times n}\)</span>, <span class="math inline">\(\mathbf{x}\in\mathbb{R}^{n\times 1}\)</span> and <span class="math inline">\(\mathbf{b}\in\mathbb{R}^{m\times 1}\)</span> . Inputs are <span class="math inline">\(A, \mathbf{b}\)</span> and output is a vector <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>In this course, we assume that <span class="math inline">\(A\)</span> is a square(i.e., <span class="math inline">\(m=n\)</span>) and invertible matrix (so that we have a unique <span class="math inline">\(x\)</span>). Our goal is to find that unique <span class="math inline">\(x\)</span> which satisfies the system of equations.</p>
<p>Let’s go step by step. Let us consider special matrices first and then discuss about a general matrix <span class="math inline">\(A\)</span>.</p>
<div id="special-matrices" class="section level3 hasAnchor" number="2.4.1">
<h3><span class="header-section-number">2.4.1</span> Special Matrices<a href="numerical-linear-algebra.html#special-matrices" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<ol style="list-style-type: decimal">
<li><p>If <span class="math inline">\(A\)</span> is a unitary matrix, then <span class="math inline">\(AA^T = I=A^TA\)</span>.
<span class="math display">\[Ax=b \implies A^T Ax = A^Tb \implies x = A^T b\]</span>
To get <span class="math inline">\(x\)</span>, we need to perform <span class="math inline">\(n^2\)</span> multiplications and <span class="math inline">\(n(n-1)\)</span> additions.</p></li>
<li><p>If <span class="math inline">\(A=U\)</span> is an upper triangular matrix.</p></li>
</ol>
<p>Let <span class="math inline">\(Ux=b\)</span> in matrix form be:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
U_{11} &amp; U_{12} &amp; U_{13} &amp; \cdots &amp; U_{1n}\\
0      &amp; U_{22} &amp; U_{23} &amp; \cdots &amp; U_{2n}\\
0      &amp; 0      &amp; U_{33} &amp; \cdots &amp; U_{3n}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
0      &amp; 0      &amp; 0      &amp; \cdots &amp; U_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}\]</span></p>
<p>The last row of <span class="math inline">\(U\)</span> corresponds to the equation
<span class="math display">\[\begin{equation}
U_{nn} x_n = b_n
\end{equation}\]</span></p>
<p>Now as <span class="math inline">\(U\)</span> is invertible, all the diagonal elements are non-zero i.e., <span class="math inline">\(U_{ii}\neq 0 \ \forall i \in\{1,2,\cdots , n\}\)</span>.</p>
<p>Therefore
<span class="math display">\[\begin{equation}
U_{nn} x_n = b_n \implies x_n = \frac{b_n}{U_{nn}}
\end{equation}\]</span></p>
<p>Similarly, we have:
<span class="math display">\[\begin{equation}
U_{(n-1),(n-1)}x_{n-1}+U_{(n-1),n}x_n = b_{n-1} \implies x_{n-1} = \frac{b_{n-1}-U_{(n-1),n}x_n}{U_{(n-1),(n-1)}}
\end{equation}\]</span></p>
<p>Using induction, we can prove that for <span class="math inline">\(i \in \{1,2,\cdots,n-1\}\)</span>, we have:
<span class="math display">\[\begin{equation}
x_i = \frac{b_i -\sum_{j=i+1}^n U_{i,j}x_j}{U_{ii}}
\end{equation}\]</span></p>
<p>To calculate <span class="math inline">\(x_i\)</span>(for <span class="math inline">\(i \in \{1,2,\cdots,n\}\)</span>), we need to perform 1 division, <span class="math inline">\((n-i)\)</span> multiplications and <span class="math inline">\((n-i)\)</span> additions/subtractions.</p>
<p>Therefore to calculate the output vector <span class="math inline">\(x\)</span>, we need to perform</p>
<ul>
<li><p><span class="math inline">\(\sum_{i=1}^n 1 = n\)</span> divisions</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}\)</span> multiplications</p></li>
<li><p><span class="math inline">\(\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}\)</span> additions/subtractions</p></li>
<li><p>In total <span class="math inline">\(n+ \frac{n^2-n}{2}+\frac{n^2-n}{2} = n^2\)</span> operations are required to get <span class="math inline">\(x\)</span>. In other words “Computational Complexity is <span class="math inline">\(n^2\)</span>”</p></li>
</ul>
<blockquote>
<p><strong>NOTE:- </strong></p>
<ol style="list-style-type: decimal">
<li><p>Loosely speaking computational complexity of an algorithm means the number of operations performed in the algorithm.</p></li>
<li><p>We say that <span class="math inline">\(f(n)\in \mathcal{O}(g(n))\)</span></p></li>
</ol>
</blockquote>
</div>
<div id="general-case" class="section level3 hasAnchor" number="2.4.2">
<h3><span class="header-section-number">2.4.2</span> General Case<a href="numerical-linear-algebra.html#general-case" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We know that to solve <span class="math inline">\(Ax=b\)</span>, we convert the Augmented matrix <span class="math inline">\([A|b]\)</span> to its Row Reduced Echelon Foem(RREF) by doing elementary row operations on <span class="math inline">\(A\)</span> to get an Identity matrix.</p>
<p>Instead of fully converting to an Identity matrix, let us convert to an upper triangular matrix, say <span class="math inline">\(U\)</span>.i.e., we do row operations to get <span class="math display">\[Ux=c\]</span> from <span class="math inline">\(Ax=b\)</span> where <span class="math inline">\(U\)</span> is an upper triangular matrix and <span class="math inline">\(c\)</span> is the vector obtained by performing corresponding operations on <span class="math inline">\(b\)</span>.</p>
<p>Let <span class="math inline">\(Ax=b\)</span> in matrix form be:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
A_{11} &amp; A_{12} &amp; A_{13} &amp; \cdots &amp; A_{1n}\\
A_{21} &amp; A_{22} &amp; A_{23} &amp; \cdots &amp; A_{2n}\\
A_{31} &amp; A_{32} &amp; A_{33} &amp; \cdots &amp; A_{3n}\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
A_{n1} &amp; A_{n2} &amp; A_{n3} &amp; \cdots &amp; A_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}\]</span></p>
<p>To convert <span class="math inline">\(A\)</span> to an upper triangular matrix <span class="math inline">\(U\)</span> (i.e., to have <span class="math inline">\(U_{ij} = 0\)</span> for <span class="math inline">\(i&gt;j\)</span>) using row operations, we need to make all elements below the diagonal to be zero. To achieve this, we firstly make all elements below <span class="math inline">\(A_{11}\)</span> in the 1st column to be zero using the below operations on <span class="math inline">\(j^{th}\)</span> row (<span class="math inline">\(j&gt;1\)</span>) :(We assume that <span class="math inline">\(A_{11}\neq 0\)</span>)</p>
<p><span class="math display">\[\begin{align}
R_{j,1} &amp;=0\\
R_{j,2:n} &amp;\gets R_{j,2:n} - \frac{A_{j1}}{A_{11}}R_{1,2:n} \\
b_j &amp;\gets b_j - \frac{A_{j1}}{A_{11}} b_1
\end{align}\]</span></p>
<p>We proceed like this to make all elements below the diagonal to be zero. Consider <span class="math inline">\(k^{th}\)</span> step in this process. Let the matrix in this step be:</p>
<p><span class="math display">\[\begin{equation}
\begin{bmatrix}
A_{11} &amp; A_{12} &amp; A_{13} &amp; \cdots &amp; A_{1,k} &amp; \cdots &amp; A_{1n}\\
0      &amp; A_{22} &amp; A_{23} &amp; \cdots &amp; A_{2,k} &amp; \cdots &amp; A_{2n}\\
0      &amp; 0      &amp; A_{33} &amp; \cdots &amp; A_{3,k} &amp; \cdots &amp; A_{3n}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots  &amp; \vdots &amp; \vdots\\
0      &amp; 0      &amp; 0      &amp; \cdots &amp; A_{k,k} &amp; \cdots &amp; A_{k,n}\\
0      &amp; 0      &amp; 0      &amp; \cdots &amp; A_{k+1,k} &amp; \cdots &amp; A_{k+1,n}\\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \ddots  &amp; \vdots &amp; \vdots\\
0      &amp; 0      &amp; 0      &amp; \cdots &amp; A_{n,k} &amp; \cdots &amp; A_{n,n}\\
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}\]</span>
The row operations, to be done on <span class="math inline">\(j^{th}\)</span> row, so that all elements below <span class="math inline">\(A_{kk}\)</span> become zero (in <span class="math inline">\(k^{th}\)</span> column) is given as follows: For <span class="math inline">\(j&gt;k\)</span> and <span class="math inline">\(A_{k,k}\neq 0\)</span></p>
<p><span class="math display">\[\begin{align}
R_{j,k} &amp;=0\\
R_{j,k+1:n} &amp;\gets R_{j,k+1:n} - \frac{A_{j,k}}{A_{k,k}}R_{k,k+1:n} \\
b_j &amp;\gets b_j - \frac{A_{j,k}}{A_{k,k}} b_k
\end{align}\]</span></p>
<p>Let us now calculate the number of operations required to be done for converting <span class="math inline">\(A\)</span> to RREF. In 1st step, for the row operation <span class="math display">\[R_{j,2:n} \gets R_{j,2:n} - \frac{A_{j1}}{A_{11}}R_{1,2:n}\]</span> for a particular row-<span class="math inline">\(j\)</span>, we need to do 1 division, <span class="math inline">\(n-1\)</span> multiplications and <span class="math inline">\(n-1\)</span> additions. There are <span class="math inline">\(n-1\)</span> such rows. Therefore, for the row operation on all these <span class="math inline">\(n-1\)</span> rows, <span class="math inline">\(n-1\)</span> divisions, <span class="math inline">\((n-1)^2\)</span> multiplications and <span class="math inline">\((n-1)^2\)</span> additions have to be done.</p>
<p>For <span class="math inline">\(k^{th}\)</span> step(<span class="math inline">\(1\le k\le n-1\)</span>), row operation <span class="math display">\[R_{j,k+1:n} \gets R_{j,k+1:n} - \frac{A_{j,k}}{A_{k,k}}R_{k,k+1:n}\]</span> on row-<span class="math inline">\(j\)</span> for <span class="math inline">\(k+1\le j\le n\)</span>, requires 1 division, <span class="math inline">\((n-k)\)</span> multiplications and <span class="math inline">\((n-k)\)</span> additions. Therefore, we need
to perform <span class="math inline">\(n-k\)</span> divisions, <span class="math inline">\((n-k)^2\)</span> multiplications and <span class="math inline">\((n-k)^2\)</span> additions.</p>
<p>Therefore, the number of operations required to convert <span class="math inline">\(A\)</span> to <span class="math inline">\(U\)</span> are:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}\)</span> Divisions</p></li>
<li><p><span class="math inline">\(\sum_{k=0}^{n-1} (n-k)^2 = \sum_{k=1}^n k^2 = \frac{n(n-1)(2n-1)}{6}\)</span> Multiplications</p></li>
<li><p><span class="math inline">\(\sum_{k=0}^{n-1} (n-k)^2 = \sum_{k=1}^n k^2 = \frac{n(n-1)(2n-1)}{6}\)</span> Additions</p></li>
</ol>
<p>Total number of operations to convert from <span class="math inline">\(A\)</span> to <span class="math inline">\(U\)</span> = <span class="math inline">\(\frac{n(n-1)}{2}+2\frac{n(n-1)(2n-1)}{6}\)</span></p>
<p>Now, let us calculate the number of operations required to get <span class="math inline">\(c\)</span> from <span class="math inline">\(b\)</span>. Consider the operation on the <span class="math inline">\(k^{th}\)</span> step(<span class="math inline">\(1\le k\le n-1\)</span>): <span class="math display">\[b_j \gets b_j - \frac{A_{j,k}}{A_{k,k}} b_k\]</span>. This operation requires <span class="math inline">\((n-k)\)</span> multiplications and <span class="math inline">\((n-k)\)</span> additions. Note that we have already calculated <span class="math inline">\(\frac{A_{j,k}}{A_{k,k}}\)</span> while converting <span class="math inline">\(A\)</span> to <span class="math inline">\(U\)</span>. Hence, no separate division operation is involved.</p>
<p>Therefore, number of operations on RHS are:</p>
<ol style="list-style-type: decimal">
<li><p><span class="math inline">\(\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}\)</span> Multiplications.</p></li>
<li><p><span class="math inline">\(\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}\)</span> Additions</p></li>
</ol>
<p>After converting <span class="math inline">\(Ax=b\)</span> to <span class="math inline">\(Ux=c\)</span>, we have seen that solving <span class="math inline">\(Ux=c\)</span> requires <span class="math inline">\(n\)</span> divisions, <span class="math inline">\(\frac{n(n-1)}{2}\)</span> additions and <span class="math inline">\(\frac{n(n-1)}{2}\)</span> multiplications.</p>
<p>Therefore, for solving <span class="math inline">\(Ax=b\)</span>:</p>
<ol style="list-style-type: decimal">
<li><p>Number of divisions required = <span class="math inline">\(\frac{n(n+1)}{2}+n\)</span></p></li>
<li><p>Number of multiplications required = <span class="math inline">\(\frac{n(n-1)(2n-1)}{6}+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}\)</span></p></li>
<li><p>Number of additions required = <span class="math inline">\(\frac{n(n-1)(2n-1)}{6}+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}\)</span></p></li>
</ol>
<p>We can see that multiplications &amp; addition operations are the most expensive(time consuming)</p>
<p>Therefore, the total number of operations = <span class="math inline">\(2\left[ \frac{n(n-1)(2n-1)}{6}+n(n-1)\right]+\frac{n(n+1)}{2}+n\)</span></p>
<p>In other words Total number of operations <span class="math inline">\(\in \mathcal{O}(n^3)\)</span></p>
<p>Now the question arises. Where will this fail?</p>
<ol style="list-style-type: decimal">
<li><p>We have assumed that <span class="math inline">\(A_{11} \neq 0\)</span> in the first step. This may not always hold true. Also, there is a possibility that <span class="math inline">\(A_{kk}=0\)</span> in the intermediate steps also. If this occurs then we can’t proceed further.</p></li>
<li><p>Precision issues might occur. Consider solving the system of equations.</p></li>
</ol>
<p><span class="math display">\[\begin{align}
2^{-60}x_1+x_2&amp;=1\\
x_1+x_2&amp;=2
\end{align}\]</span>
The equations can be written in matrix form as:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
2^{-60}&amp; 1\\
1&amp; 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
1\\2
\end{bmatrix}
\end{equation}\]</span></p>
<p>The solution is <span class="math inline">\(x_1 \approx 1\)</span> and <span class="math inline">\(x_2 \approx 1\)</span>.</p>
<p>Let us try to solve this with the help of method described in this section.
First, we do the row operations <span class="math inline">\(R_1 \gets R_2 - \frac{1}{2^{-60}}R_1 \implies R_1 \gets R_2 - 2^{60} R_2\)</span> to get:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
2^{-60}&amp; 1\\
0&amp; 1-2^{60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
1\\2-2^{60}
\end{bmatrix}
\end{equation}\]</span></p>
<p><span class="math inline">\(1-2^{60}\)</span> and <span class="math inline">\(2-2^{60}\)</span> is represented on the 64 bit machine as <span class="math inline">\(2^{-60}\)</span>. Therefore, the machine represents the equations as:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
2^{-60}&amp; 1\\
0&amp; -2^{60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
1\\-2^{60}
\end{bmatrix}
\end{equation}\]</span></p>
<p>This implies that <span class="math inline">\(x_2=1\)</span> and <span class="math inline">\(2^{-60}x_1+x_2=1\implies x_1=0\)</span>. But this is <strong>significantly</strong> different from the exact solution.</p>
<p>If we have had an infinite precision machine, then we would have got accurate results. But since, we have only finite precision machines, we have this issue.</p>
<p>Let us calculate condition number of this matrix. The condition number as calculated by MATLAB using <code>cond()</code> function gives condition number as 2.6180 which is a relatively small condition number. This means that small changes in input don’t give very large changes in output. Hence it is a well conditioned problem. Therefore, condition number is not an issue.</p>
<p>Therefore, Algorithm has to be blamed here</p>
<p>We say that an algorithm is stable if it gives almost correct answer to a well conditioned problem. As this gives a completely different answer, we say that this algorithm is unstable.(even if it doesn’t encounter zero in the diagonal)</p>
<p>How to overcome these issues? Let us think this way. For the first issue, if a zero diagonal entry is encountered, let us try swapping row with a row which has a non-zero element in that column. Now the question arises which row? We tend to swap with the row which has largest absolute value. Instead of swapping only when zero is encountered, we do it in every step.</p>
<p>Let us consider the example problem, we were discussing earlier.
<span class="math display">\[\begin{equation}
\begin{bmatrix}
2^{-60}&amp; 1\\
1&amp; 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
1\\2
\end{bmatrix}
\end{equation}\]</span></p>
<p>In first column, the maximum value is 1. Thus, we swap 1st and 2nd rows.
<span class="math display">\[\begin{equation}
\begin{bmatrix}
1&amp; 1\\
2^{-60}&amp; 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
2\\1
\end{bmatrix}
\end{equation}\]</span>
We do a row operation <span class="math inline">\(R_2\gets R_2-2^{-60}R_1\)</span> to get:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
1&amp; 1\\
0&amp; 1-2^{-60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
2\\1-2^{-59}
\end{bmatrix}
\end{equation}\]</span></p>
<p><span class="math inline">\(1-2^{-60}\)</span> and <span class="math inline">\(1-2^{-59}\)</span> is represented on the 64 bit machine as <span class="math inline">\(1\)</span>. Therefore, the machine represents the equations as:
<span class="math display">\[\begin{equation}
\begin{bmatrix}
1&amp; 1\\
0&amp; 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
=
\begin{bmatrix}
2\\1
\end{bmatrix}
\end{equation}\]</span></p>
<p>The solution to these system of equations give <span class="math inline">\(x_2=1\)</span> and <span class="math inline">\(x_1=1\)</span>.</p>
<p>This method in which we swap rows is called <strong>partial pivoting</strong>. Partial pivoting is seen to be stable in practice.</p>
<p>Eg: Solve the system using partial pivoting
<span class="math display">\[\begin{align}
x_1+2x_2+3x_3 &amp;=6\\
x_1+2x_2+4x_3 &amp;=7\\
x_1-2x_2+x_3  &amp;= 0
\end{align}\]</span></p>
<p>A much more better way is including a column swap as well. The process is decribed in the below example. This process is called <strong>Complete Pivoting</strong> which is stable but has more computation cost(due to multiple swaps/ comparisions)</p>
<p>Eg: Solve the system using complete pivoting.
<span class="math display">\[\begin{align}
x_1+2x_2+3x_3 &amp;= 6\\
x_1+2x_2+4x_3 &amp;= 7\\
x_1-2x_2+10x_3&amp;= -11
\end{align}\]</span></p>
<p><strong>ADD SOLUTION</strong></p>
<p>Order of computational cost: Complete Pivoting &gt; Partial Pivoting &gt; No Pivoting</p>
<p>Order of Stability: Complete Pivoting &gt; Partial Pivoting &gt; No Pivoting</p>
</div>
</div>
<div id="solving-overdetermined-systems" class="section level2 hasAnchor" number="2.5">
<h2><span class="header-section-number">2.5</span> Solving Overdetermined Systems<a href="numerical-linear-algebra.html#solving-overdetermined-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="solving-underdetermined-systems" class="section level2 hasAnchor" number="2.6">
<h2><span class="header-section-number">2.6</span> Solving Underdetermined Systems<a href="numerical-linear-algebra.html#solving-underdetermined-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
</div>
<div id="iterative-methods-for-solving-linear-systems" class="section level2 hasAnchor" number="2.7">
<h2><span class="header-section-number">2.7</span> Iterative Methods for solving Linear Systems<a href="numerical-linear-algebra.html#iterative-methods-for-solving-linear-systems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Previously, we have seen “direct methods” to solve linear systems like Gauss-Jordan elimination, Partial and complete pivoting. They can be solved exactly with <span class="math inline">\(\infty\)</span> precision machines. But the computational cost is high.</p>
<p>For lesser computational cost, we use iterative methods. But the issue is that we can not solve exactly using these methods always.</p>
<!-- All chapters start with a first-level heading followed by your chapter title, like the line above. There should be only one first-level heading (`#`) per .Rmd file. -->
<!-- ## A section -->
<!-- All chapter sections start with a second-level (`##`) or higher heading followed by your section title, like the sections above and below here. You can have as many as you want within a chapter. -->
<!-- ### An unnumbered section {-} -->
<!-- Chapters and sections are numbered by default. To un-number a heading, add a `{.unnumbered}` or the shorter `{-}` at the end of the heading, like in this section. -->

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="index.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="interpolation.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/USERNAME/REPO/edit/BRANCH/01-Numerical_LinAl.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
