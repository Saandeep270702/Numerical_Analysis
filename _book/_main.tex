% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Numerical Analysis},
  pdfauthor={Sai Saandeep.S, Dr.~Sivaram},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Numerical Analysis}
\author{Sai Saandeep.S, Dr.~Sivaram}
\date{2023-06-17}

\usepackage{amsthm}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{lemma}{Lemma}[chapter]
\newtheorem{corollary}{Corollary}[chapter]
\newtheorem{proposition}{Proposition}[chapter]
\newtheorem{conjecture}{Conjecture}[chapter]
\theoremstyle{definition}
\newtheorem{definition}{Definition}[chapter]
\theoremstyle{definition}
\newtheorem{example}{Example}[chapter]
\theoremstyle{definition}
\newtheorem{exercise}{Exercise}[chapter]
\theoremstyle{definition}
\newtheorem{hypothesis}{Hypothesis}[chapter]
\theoremstyle{remark}
\newtheorem*{remark}{Remark}
\newtheorem*{solution}{Solution}
\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{why-numerical-analysis}{%
\section{Why Numerical Analysis?}\label{why-numerical-analysis}}

\hypertarget{representing-numbers-on-a-machine}{%
\section{Representing Numbers on a Machine}\label{representing-numbers-on-a-machine}}

\hypertarget{condition-number-of-a-problem}{%
\section{Condition Number of a Problem}\label{condition-number-of-a-problem}}

Consider a function in one variable \(f:\mathbb{R}\to\mathbb{R}\).
Condition number for a function \(f(x)\) tells about the error amplification of a function \(f(x)\) i.e., for a given error in input \(x\), how much is the error in the output \(f(x)\).

Absolute Condition Number \(\kappa_{\text{abs}}\) of the function \(f(x)\) is defined as:
\begin{equation}
\kappa_{\text{abs}} = \frac{\text{Absolute Change in Output}}{\text{Absolute Change in Input}} = \lim_{\delta x \to 0} \left\lvert{\frac{f(x+\delta x)-f(x)}{x+\delta x - x}}\right\rvert = \left\lvert{f'(x)}\right\rvert
\end{equation}

Relative Condition Number \(\kappa_{r}\) of the function \(f(x)\) is defined as:
\begin{equation}
\kappa_{r} = \frac{\text{Relative Change in Output}}{\text{Relative Change in Input}} = \lim_{\delta x \to 0} \frac{\left\lvert{\frac{f(x+\delta x)-f(x)}{f(x)}}\right\rvert}{\left\lvert{\frac{x+\delta x - x}{x}}\right\rvert} = \left\lvert{\frac{x}{f(x)}f'(x)}\right\rvert
\end{equation}

Now what if the function has multiple inputs? Or What if the function has multiple outputs?

Examples:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Input 2 numbers \(a,b\in\mathbb{R}\) and then find \(f(a,b) = a+b\)?. This problem takes 2 inputs- \(a, b\) and one output \(f(a,b)\).
\item
  Find the roots of a polynomial \(a_0+a_1x+a_2x^2+\cdots+ a_nx^n\). We are inputting the vector \(\begin{bmatrix} a_0 & a_1 & a_2 & \cdots & a_n\end{bmatrix}^T\) and the output is \(x\) in this case.
\item
  Given a matrix \(A\in\mathbb{R}^{m\times n}\). Input a vector \(\mathbf{x}\in\mathbb{R}^{n\times 1}\) and then find \(f(\mathbf{x}) = A\mathbf{x} \in \mathbb{R}^{m\times 1}\)?
\item
  Solve the linear system \(A\mathbf{x}=\mathbf{b}\) where \(A\in\mathbb{R}^{m\times n}\), \(\mathbf{x}\in\mathbb{R}^{n\times 1}\) and \(\mathbf{b}\in\mathbb{R}^{m\times 1}\) . Inputs are \(A, \mathbf{b}\) and output is a vector \(\mathbf{x}\).
\end{enumerate}

To accommodate these cases, a generalized definition of a (relative) condition number \(\kappa_r\) for a function \(f:X\to Y\) where \(X \subset \mathbb{R}^{m\times 1}\) and \(Y \subset \mathbb{R}^{n\times 1}\) is shown below:
\begin{equation}
  \kappa_r = \lim_{r\to 0} \sup_{\lVert x \rVert_q \le r} \frac{\frac{\lVert f(x+\delta x)-f(x) \rVert_p}{\lVert f(x) \rVert_p}}{\frac{\lVert \delta x \rVert_q}{\lVert x \rVert_q}}
\end{equation}

where \(p,q \in \mathbb{N}\) and \(\lVert . \rVert_p\) denotes the vector \(p-\) norm.

\hypertarget{vector-normsrecap}{%
\subsection{Vector Norms(Recap)}\label{vector-normsrecap}}

For a vector \(x\) in the vextor space \(X\) over a field \(F\), \(\lVert . \rVert:F\to R\) is defined such that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\lVert x \rVert\ge 0 \ \ \ \forall x \in X\).
\item
  \(\lVert \alpha x \rVert = \left\lvert{\alpha}\right\rvert, \ \ \ \forall x \in X, \ \ \ \alpha \in F\)
\item
  \(\lVert x+y \rVert \le \lVert x \rVert+\lVert y \rVert, \  \ \ \forall x,y \in X\).
\item
  \(\lVert x \rVert = 0 \Longleftrightarrow x = 0\)
\end{enumerate}

Let \(x =\begin{bmatrix} x_1 & x_2&\cdots &x_n \end{bmatrix}^T\). Different possible vector norms which satisfy the above conditions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Euclidean norm (2-norm)
  \begin{equation}
  \lVert x \rVert_2 = \sqrt{x_1^2+x_2^2+\cdots+x_n^2}
  \end{equation}
\item
  Supremum norm(max. norm)
  \begin{equation}
  \lVert x \rVert_{\max} = \lVert x \rVert_{\infty} = \max_{1\le i\le n} |x_i|
  \end{equation}
\item
  1-norm
  \begin{equation}
  \lVert x \rVert_1 = \sum_{i=1}^n |x_i|
  \end{equation}
\item
  \(p\)-norm
  \begin{equation}
  \lVert x \rVert_{p} = \left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}
  \end{equation}
\end{enumerate}

\textbf{NOTE:-} Supremum norm of \(x\) is \(p\)-norm of \(x\) as \(p\to\infty\)

Proof:- From the definition, \[\lim_{p\to\infty} \lVert x \rVert_p = \lim_{p\to\infty}\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}\]

\[\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}} \le \left( n\max_{1\le i\le n} |x_i|^p \right)^{\frac{1}{p}} = n^{\frac{1}{p}} \max_{1\le i\le n} |x_i|\]

\[\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}} \ge \left( \max_{1\le i\le n} |x_i|^p \right)^{\frac{1}{p}} = \max_{1\le i\le n} |x_i|\]

From the above 2 inequalities, we can say that:
\[\max_{1\le i\le n} |x_i| \le \lVert x \rVert_p \le n^{\frac{1}{p}} \max_{1\le i\le n} |x_i|\]
As \(p \to \infty,\ \  \ n^{\frac{1}{p}} \max_{1\le i\le n} |x_i| \to \max_{1\le i\le n} |x_i|\). Therefore, by using sandwich theorem, we can say that \[\lVert x \rVert_p = \max_{1\le i\le n} |x_i|\]

\hypertarget{examples-on-finding-condition-number}{%
\subsection{Examples on finding Condition number}\label{examples-on-finding-condition-number}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Let \(f(a,b) = a+b\). Find the condition number of this problem?
\end{enumerate}

The inputs are \(a,\, b\). Let the inputs have an error \(\delta a,\, \delta b\) respectively.

\[\text{Relative error in input} = \dfrac{\left\Vert \begin{bmatrix} a+\delta a\\ b+\delta b \end{bmatrix}- \begin{bmatrix} a\\  b \end{bmatrix}\right\Vert_p}{\left\Vert \begin{bmatrix} a\\ b \end{bmatrix}\right\Vert_p}\]

For simplicity, let us consider 2-norm. Any norm can be used in fact. Therefore,

\[\text{Relative error in input} =\dfrac{\sqrt{\delta a^2+\delta b^2}}{\sqrt{a^2+b^2}}\]
The output \(f(a+\delta a,b+\delta b) = a+b+\delta a+\delta b\). Therefore,
\[\text{Relative Error in output} = \dfrac{|(a+b+\delta a + \delta b)-(a+b)|}{|a+b|} = \frac{|\delta a+\delta b|}{|a+b|}\]
The relative condition number is:
\[\kappa_r = \lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r} \dfrac{\frac{|\delta a+\delta b|}{|a+b|}}{\dfrac{\sqrt{\delta a^2+\delta b^2}}{\sqrt{a^2+b^2}}}\]
\[\implies \kappa_r = \lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r} \dfrac{|\delta a+\delta b|}{\sqrt{\delta a^2+\delta b^2}} \cdot \dfrac{\sqrt{a^2+b^2}}{|a+b|} \]
To calculate \[\lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r}\dfrac{|\delta a+\delta b|}{\sqrt{\delta a^2+\delta b^2}}\] we assume that \(\delta a = \alpha \cos \theta\) and \(\delta b = \alpha \sin \theta\) where \(\alpha>0\) and \(0\le\theta<2\pi\).

Therefore, we have:
\[\lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r}\dfrac{|\delta a+\delta b|}{\sqrt{\delta a^2+\delta b^2}} = \lim_{r \to 0} \sup_{\alpha < r} \dfrac{|\alpha \cos \theta+\alpha \sin \theta|}{\alpha} = \lim_{r\to 0} \sup_{\alpha<r}|\cos \theta+\sin \theta| = \sqrt{2}\]
Thus, the condition number for adding 2 numbers is:
\[ \kappa_r = \dfrac{\sqrt{2(a^2+b^2)}}{|a+b|}\le \sqrt{2} \text{ (if $a,\, b>0$)}\]
(as \(|a+b| \ge \sqrt{a^2+b^2}\) for \(a,b \in \mathbb{R}^+\))

For \(a,b>0\), we can clearly see that the condition number is bounded above by \(\sqrt{2}\). In other words, \textbf{addition is well-conditioned.}

By performing a similar exercise, we can show that the \textbf{subtraction is ill-conditioned} as for \(\frac{a}{b} \to 1\), \(\kappa_r \to \infty\).

Multiplication and division operations are also ill-conditioned.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Condition number on finding roots of the polynomial \(x^2-2x+1\).
\end{enumerate}

\hypertarget{numerical-linear-algebra}{%
\chapter{Numerical Linear Algebra}\label{numerical-linear-algebra}}

\hypertarget{columnspace-nullspace-and-all}{%
\section{Columnspace, Nullspace and all}\label{columnspace-nullspace-and-all}}

Consider a matrix \(A\in\mathbb{R}^{m\times n}\) defined as:
\[A = \begin{bmatrix} -&r_1^T&-\\ -&r_2^T&-\\ & \vdots& \\ - & r_m^T& - \end{bmatrix} = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}\]
where \(r_i \in \mathbb{R}^{n\times 1}\) for \(1\le i \le m\) are the rows and \(a_i \in \mathbb{R}^{m\times 1}\) for \(1\le i\le n\) are the columns of \(A\).

Columnspace of a matrix \(A\) is the span(linear combination) of columns of \(A\). Also called as Range of \(A\).
\begin{equation}
\text{Range}(A) =\text{Columnspace}(A) =  \{ Ax : x\in \mathbb{R}^{n\times 1} \}
\end{equation}

\[Ax = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}x_1\\x_2\\ \vdots\\ x_n \end{bmatrix} = \sum_{i=1}^n a_i x_i\]

Rowspace of a matrix \(A\) is the span(linear combination) of rows of \(A\).
\begin{equation}
\text{Rowspace}(A) = \{ A^Ty : y\in \mathbb{R}^{m\times 1} \} 
\end{equation}

\[A^Ty = \begin{bmatrix} | & | &  & | \\ r_1 & r_2 &\cdots & r_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots\\ y_n \end{bmatrix} = \sum_{i=1}^n r_i y_i\]
Nullspace of a matrix \(A\) is defined as follows:
\begin{equation}
\text{Nullspace}(A) = \{ z\in \mathbb{R}^{n\times 1}: Az=0\}
\end{equation}

NOTE:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A linear system \(Ax=b\) has a solution ONLY IF \(b\in\text{Range}(A)\).
\item
  Dimension of Range\((A)\) is the number of linearly independent columns of \(A\) or the column rank of \(A\). Similarly, the dimension of Rowspace\((A)\) is the row rank or the number of independent rows of \(A\).
\item
  For a matrix \(A\), row rank = column rank = rank\(\le \min(m,n)\).
\item
  Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector \(z\in \text{Nullspace}(A)\) and \(w \in \text{Rowspace}(A)\) , \(z\) is orthogonal to \(w\).
\end{enumerate}

Proof:- Let \(w \in \text{Rowspace}(A)\), then \(\exists y\in\mathbb{R}^{n\times 1}\) such that \(w = A^Ty\).

Also as \(z\in \text{Nullspace}(A)\), we have \(Az=0\).

Therefore,
\[\langle w,z\rangle = w^Tz = y^T Az = 0\]
Thus, nullspace of a matrix is orthogonal to row space of a matrix.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Rank-Nullity Theorem:} Dimension of Nullspace\((A)\)+Rank\((A)\) = \(n\) = No.~of columns of \(A\)
\end{enumerate}

\hypertarget{matrix-norms}{%
\section{Matrix Norms}\label{matrix-norms}}

Consider a matrix \(A\in\mathbb{R}^{m\times n}\). Just like how we have defined a vector norm, we could have defined an ``element wise matrix norm'' as follows:
\begin{equation}
\lVert A \rVert_p^* = \left(\sum_{i=1}^n |A_{ij}|^p\right)^{\frac{1}{p}} 
\end{equation}

But this definition of norm does not satisfy the \textbf{submultiplicative property}. We are interested in this property as this dictates the convergence of iterative schemes.

A matrix norm is said to be submultiplicative if for any matrices \(A\in \mathbb{R}^{m\times k}\) and \(B \in \mathbb{R}^{k\times n}\), we have
\begin{equation}
\lvert AB \rVert \le \lVert A \rVert  \lVert B \rVert
\end{equation}

Consider the case \[A = \begin{bmatrix} 2&2\\2&2 \end{bmatrix}\] and \(p\to \infty\), Therefore we have \[ \lVert A \rVert_{\infty}^* = \max_{1\le i \le m,1 \le j \le n} |A_{ij}| = 2\].

\[A^2 = \begin{bmatrix} 8 & 8\\ 8& 8 \end{bmatrix}\]
Therefore,
\[\lVert A^2 \rVert_{\infty}^* = 8\]
We can clearly see that:
\[\lVert A^2 \rVert_{\infty}^* = 8 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4\]

which violates the submultiplicative property.

Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows.
\begin{equation}
\lVert A \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p} = \sup_{\lVert y \rVert_p = 1} \lVert Ay \rVert_p
\end{equation}

p-norms are submultiplicative.

PROOF:- From the definition of p-norm,
\begin{align*}
\lVert AB \rVert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}\\
&= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \\
&\le \left[\sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \right] \cdot \left[ \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \right]  \\
&= \lVert A \rVert_p \cdot \lVert B \rVert_p\\
\therefore \lVert AB \rVert_p \le \lVert A \rVert_p \cdot \lVert B \rVert_p
\end{align*}

Using this property, we can say that
\begin{equation}
 \lVert A^n \rVert_p \le \lVert A \rVert^n_p
\end{equation}

\(\lVert A\rVert_1\) = Maximum of column sum of absolute values.

\(\lVert A\rVert_{\infty}\) = Maximum of row sum of absolute values.

\hypertarget{condition-number-of-matrix-vector-products}{%
\section{Condition number of Matrix vector products}\label{condition-number-of-matrix-vector-products}}

Consider a matrix \(A\in\mathbb{R}^{m\times n}\) and a vector \(x\in\mathbb{R}^{n\times 1}\). Assume that there is no error in representing \(A\). We are interested in finding the condition number of the Matrix-Vector product \(f(x;A)= Ax\).

From the definition of condition number, we can write the condition number \(\kappa_r\) of the matrix vector product as:

\[\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_q\le r} \dfrac{\frac{\Vert A(x+\delta x)-Ax \Vert_p}{\Vert Ax \Vert_p}}{\frac{\Vert x+\delta x-x\Vert_q}{\Vert x\Vert_q}}\]
For simplicity let us choose \(p=q\). Therefore,
\[\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} \frac{\Vert x \Vert_p}{\Vert Ax\Vert_p}\]
From the definition of matrix p-norm, we can say that: \[\lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} = \Vert A\Vert_p\]
Therefore the condition number of the matrix vector product is:
\begin{equation}
\kappa_r =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}
\end{equation}

From Sub-multiplicative property, as \(\Vert Ax\Vert_p \ge\Vert A \Vert_p \Vert x \Vert_p\), we can show that \(\kappa_r\ge 1\).

\hypertarget{solving-linear-systems}{%
\section{Solving Linear Systems}\label{solving-linear-systems}}

\hypertarget{interpolation}{%
\chapter{Interpolation}\label{interpolation}}

\hypertarget{motivation---interpolation-vs.-approximation}{%
\section{Motivation - Interpolation vs.~Approximation}\label{motivation---interpolation-vs.-approximation}}

If the exact form of \(f(x)\) is known, then we have full information about \(f(x)\) i.e., Derivatives etc., But what if the exact form is not known?

Given points \(\{x_i\}_{i=1}^n\) and functional values at those points \(f(x_1),f(x_2), \dots, f(x_n)\), we wish to find an Approximation to \(f(x)\). One way to approximate a function is by \emph{interpolating} it.

\begin{quote}
We say that \(p(x)\) is an interpolant to \(f(x)\) at \(\{x_i\}_{i=1}^n\) if \(p(x_i)= f(x_i)\) for \(1\le i\le n\).
\end{quote}

Eg: A step function \(p(x) = f(x_i)\) for \(x\in \left[ \frac{x_i+x_{i-1}}{2}, \frac{x_i+x_{i+1}}{2} \right]\).

The step function, though it is an interpolant, we don't prefer it. The main issue is that it is not continuous. We prefer in some practical applications for the interpolant to be differentiable.

Note that not all approximations are interpolants.

Eg: A polynomial approximation to \(\sin x\) is a truncated Taylor series after a few terms(say till degree \(2n+1\)). This approximation is not an interpolant as it won't intersect \(\sin x\) at \(2n+2\) points.

Assume \(f(x)\) to be continuous. Consider the sequence of interpolants \(\{P_n(x)\}_{n=1}^{\infty}\) converging to \(f(x)\) on \([a,b]\) such that \(P_n(x)=f(x) \ \  \ \forall x\in \{x_1,x_2,\dots,x_n \}\)

\begin{equation}
\label{eq:integralfinterp}
\int_a^b f(x) \, dx \approx \int_a^b P_n(x) \, dx 
\end{equation}

\begin{equation}
\label{eq:dfdxinterp}
\frac{df}{dx}(x) \approx \frac{dP_n}{dx}(x)
\end{equation}

Equation\eqref{eq:integralfinterp} holds true if \(\{P_n(x)\}_{n=1}^{\infty}\) converges to \(f(x)\) \emph{uniformly}.

Equation\eqref{eq:dfdxinterp} holds true if \(P_n'(x)\) exists and \(\{P'_n(x)\}_{n=1}^{\infty}\) converges to \(f'(x)\) \emph{uniformly}.

In this course, we consider the interpolants \(p(x) \in C^{\infty}([a,b])\). A simplest such interpolant would be a polynomial.

\hypertarget{lagrange-interpolation}{%
\section{Lagrange Interpolation}\label{lagrange-interpolation}}

\hypertarget{motivation}{%
\subsection{Motivation}\label{motivation}}

Consider \(p(x) = a_0+a_1x+a_2x^2+\dots+ a_mx^m\) to be a polynomial interpolant for \(f(x)\) with \textbf{node points} as \(\{x_i\}_{i=1}^n\). Therefore,
\[p(x_i)=f(x_i) \implies a_0+a_1x_i+a_2x_i^2+\dots+ a_mx_i^m = f(x_i) \ \ \ \forall i \in \{1,2,\cdots, n\}\].

\begin{equation}
\implies \begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m\\
1 & x_2 & x_2^2 & \cdots & x_2^m\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m\\
\end{bmatrix}_{n \times(m+1)} \begin{bmatrix}a_0\\a_1\\ \vdots \\a_m \end{bmatrix}_{(m+1) \times 1}  = \begin{bmatrix} f(x_1)\\f(x_2)\\ \vdots \\f(x_n) \end{bmatrix}_{n\times 1}
\end{equation}

Let \[X = \begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m\\
1 & x_2 & x_2^2 & \cdots & x_2^m\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m\\
\end{bmatrix}\text{ ,} \ \  \ \bar{a} = \begin{bmatrix}a_0\\a_1\\ \vdots \\a_m \end{bmatrix}  \text{ and } \ \ \ \bar{f}=  \begin{bmatrix} f(x_1)\\f(x_2)\\ \vdots \\f(x_n) \end{bmatrix}\]
\[\implies X\bar{a} = \bar{f}\]

Now the \(\bar{a}\) has the coefficients of the interpolant. To find the coefficients, we need to solve the linear system.

If \(m+1<n\) then \(X\) is a thin matrix. i.e., we have lesser number of variables than equations. Therefore, solution may not exist. i.e., we may not be able to find \(p(x)\).

If \(m+1>n\) then \(X\) is a fat matrix. Infinitely many polynomial interpolants exist.

If \(m+1=n\) then \(X\) is a square matrix. \(X\) is Vandermonde matrix. It can be shown that:
\[\det(X) = \prod_{1\le i<j\le n}(x_i-x_j)\]
If \(x_i's\) are distinct then \(\det(X)\neq 0 \implies X\) is invertible.

\(\implies X\bar{a} = \bar{f}\) has a unique solution for \(m+1=n\).

\(\implies p(x)\) interpolates \(f(x)\) uniquely if \(\deg(p(x)) = n-1\).

Thus, the minimum degree of the interpolant polynomial is \(n-1\).

\(p(x)\) interpolates \(f(x)\) uniquely if \(\deg(p(x))=n-1\). Thus, solving the equation \(X\bar{a} = \bar{f}\).
But there are issues in solving the linear system like this.

\begin{itemize}
\tightlist
\item
  Computational complexity in solving the linear system \(\mathcal{O}(n^3)\).
\item
  Condition number of \(X\) grows exponentially in \(n\). This is not preferred as this might cause large errors with small error in input(say due to roundoff errors etc.,)
\end{itemize}

To overcome these problems, we use Lagrange interpolation.

\hypertarget{lagrange-interpolant}{%
\subsection{Lagrange Interpolant}\label{lagrange-interpolant}}

Consider

\begin{equation}
g(x_i) = \begin{cases}
        1 & \text{if } i\neq j\\
        0 & \text{if } i=j
    \end{cases}
\end{equation}
for \(i,j \in \{1,2,\dots,n\}\). We are interested to find a polynomial interpolant for this. Let us consider the case of \(n=3\) points and \(j=2\). i.e., \(g(x_1)=g(x_3)=0\) and \(g(x_2)=1\) for simplicity. The least degree of the polynomial interpolant \(p(x)\) is \(n-1=2\). By intuition, we can say that \((x-x_1)(x-x_3)\) is a factor of interpolant \(p(x)\). As \(p(x_2)=g(x_2)=1\), we can say that the interpolant \(p(x)\) is:
\begin{equation}
p(x) = \frac{(x-x_1)(x-x_3)}{(x_2-x_1)(x_2-x_3)}
\end{equation}

For \(n\) points, we have the interpolant polynomial to \(g(x)\) as:
\begin{equation}
p(x) = l_j(x) = \prod_{\substack{i=0 \\ i\neq j}} \frac{x-x_i}{x_j-x_i}
\end{equation}

\(l_j(x)\) is a polynomial of degree \(n-1\) such that:
\[l_j(x_i) = \delta_{ij}\]
Now consider \(n\) node points \(\{f(x_i)\}_{i=1}^n\) and consider the polynomial \(p(x)\) defined as:
\begin{equation}
p(x) = \sum_{j=1}^n f(x_j) l_j(x)
\end{equation}

\(p(x)\) is a polynomial of degree atmost \(n-1\). Now,

\[p(x_i) = \sum_{j=1}^n f(x_j) l_j(x_i) = \sum_{j=1}^n f(x_j) \delta_{ij} = f(x_i)\]
This implies that \(p(x)\) is an interpolant. This is known as \emph{Lagrange Interpolant}.

\hypertarget{choice-of-nodes}{%
\section{Choice of Nodes}\label{choice-of-nodes}}

\hypertarget{motivation-1}{%
\subsection{Motivation}\label{motivation-1}}

Now after finding the interpolant, the next question to be asked is how accurate is the interpolant? Immediate obvious answer would be the numer of points chosen. But are there any other factors which determine the accuracy of the interpolant?

Example: Consider the function \(f(x) = \frac{1}{1+25x^2}\) and the interpolation nodes as uniform nodes from {[}-1,1{]}.

\textbf{INSERT CODE HERE}

We can see that the interpolant is not converging to \(f(x)\) uniformly. There are some \textbf{\emph{boundary effects}}

Thus, selection of node points is also important. The question to ask now is what set of nodes guarantees uniform convergence?

\hypertarget{fundamental-theorem-of-polynomial-interpolation}{%
\subsection{Fundamental Theorem of Polynomial Interpolation}\label{fundamental-theorem-of-polynomial-interpolation}}

Let \(f(x)\) be a smooth function on {[}-1,1{]}. Let \(P_n(x)\) be a polynomial interpolant to \(f(x)\) at \(\{x_k\}_{k=0}^n\) with atmost degree \(n\). Then \(\exists \, \zeta\in[-1,1]\) such that:
\begin{equation}
e(x) = f(x)-P_n(x) = \frac{f^{(n+1)}(\zeta)}{(n+1)!}\prod_{k=0}^n(x-x_k)
\end{equation}

\textbf{Proof:-}
Define \[w(x) := \prod_{k=0}^n(x-x_k)\] and
\begin{equation}
g_x(t) : = f(t)-P_n(t)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(t)
\end{equation}
where the subscript \(x\) denotes that \(x\) is fixed.

\[g_x(x) = f(x)-P_n(x)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(x) = 0\]

\[g_x(x_j) = f(x_j)-P_n(x_j)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(x_j)\]
As \(P_n(x)\) is an interpolant to \(f(x)\) at nodes \(\{x_i\}_{i=0}^n\), \(P_n(x_j) = f(x_j)\) and by definition \(w(x_j) =0\). Therefore, for \(j\in\{0,1,2,\dots ,n\}\), we have \(g_x(x_j)=0\).

\(g_x(t)\) is smooth in the interval (-1,1).

Define intervals
\begin{align}
I_1     &= [x_0,x_1]\\
I_2     &= [x_1,x_2]\\
        &\vdots\\
I_k     &= [x_{k-1},x_k]\\
I_{k+1} &= [x_k,x]\\
I_{k+2} &=[x,x_{k+1}]\\
I_{k+3} &= [x_{k+1},x_{k+2}]\\
        &\vdots\\
I_{n+1} &= [x_{n-1},x_n]
\end{align}
According to Rolle's theorem, \(g'_x(t)\) has atleast \(n+1\) zeros on \([-1,1]\).
Again according to Rolle's theorem, we can say that \(g''_x(t)\) has atleast \(n\) zeros in \([-1,1]\). If we keep on using Rolle's theorem for further \(n-1\) times, we can show that \(g_x^{(n+1)}(t)\) has atleast 1 zero on \([-1,1]\) i.e., \(\exists \text{ a } \zeta\in[-1,1]\) such that \(g_x^{(n+1)}(\zeta)=0\)

\[g^{(n+1)}_x(\zeta) = f^{(n+1)}(\zeta)-P^{(n+1)}_n(\zeta)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w^{(n+1)}(\zeta)\]

As \(P_n(t)\) is an \$n\^{}th degree polynomial, \(P^{(n+1)}_n(\zeta)=0\).

\[w(t) = \prod_{k=0}^n(t-x_k)\implies w^{(n+1)}(t) = (n+1)!\]

Therefore \[0 = f^{(n+1)}(\zeta)-0 - \left(\frac{f(x)-P_n(x)}{w(x)}\right)(n+1)!\]
\[\implies e(x) = f(x)-P_n(x) = \frac{f^{(n+1)}(\zeta)}{(n+1)!}w(x)= \frac{f^{(n+1)}(\zeta)}{(n+1)!}\prod_{k=0}^n(x-x_k)\]

\begin{quote}
What if \(x\in[a,b]\) instead of \(x\in[-1,1]\)?
\end{quote}

We can use a linear mapping from \([a,b]\) to \([-1,1]\).

\hypertarget{different-possible-types-of-nodes}{%
\subsection{Different Possible types of nodes}\label{different-possible-types-of-nodes}}

Now the goal is to find the nodes which minimise the maximum absolute interpolation error i.e., \[\min \max_{x\in[-1,1]} |e(x)|\]. In general, we can also try finding nodes which minimise \(p\)-norm of the interpolation error i.e., \(\min \lVert e(x) \rVert_p\) where:
\[\lVert e(x) \rVert_p = \left( \int_{-1}^1 |e(x)|^p \, dx\right)^{\frac{1}{p}}\] and \[\lim_{p\to\infty} \lVert e(x) \rVert_p = \max_{x\in[-1,1]}|e(x)|\]

Now the issue is it is difficult to know about \(f^{(n+1)}(\zeta)\) as \(f(x)\) is not known always. Now the best thing we can do is to find nodes which minimise \[\min \max_{x\in[-1,1]}\left|\prod_{k=0}^n(x-x_k)\right| \text{ or } \min \lVert \prod_{k=0}^n(x-x_k) \rVert_p\].

It turns out that Legendre nodes minimise \(||w(x)||_2\), Chebyshev nodes of first kind minimise \(||w(x)||_{\infty}\) and Chebyshev nodes of second kind minimises \(||w(x)||_1\).

\hypertarget{legendre-nodes}{%
\subsubsection{Legendre Nodes}\label{legendre-nodes}}

Monic Legendre polynomials are defined as:
\[q_0(x) = 1, \, \, q_1(x) = x\]
\(q_n(x)\) is a monic polynomial of degree \(n\) such that:
\begin{equation}
\int_{-1}^1 q_n(x) q_m(x) \, dx = 0 \ \  \ \forall \,  m\neq n
\end{equation}

First few monic Legendre polynomials are:
\begin{align*}
  q_0(x) &= 1\\
  q_1(x) &= x\\
  q_2(x) &= x^2-\frac{1}{3}\\
  q_3(x) &= x^3-\frac{3}{5}x\\
  q_4(x) &= x^4- \frac{6}{7}x^2+\frac{3}{35}
\end{align*}

The zeros of these Legendre polynomials are called Legendre nodes. Legendre nodes minimise \(||w(x)||_2\).

\hypertarget{chebyshev-nodes-of-first-kind}{%
\subsubsection{Chebyshev nodes of first kind}\label{chebyshev-nodes-of-first-kind}}

Chebyshev polynomials of first kind are given by \(T_n(x) = \cos(n\cos^{-1}(x))\).

First few Chebyshev polynomials are:
\begin{align}
T_0(x) &= 1\\
T_1(x) &= x\\
T_2(x) &= 2x^2-1\\
T_3(x) &= 4x^3-3x\\
T_4(x) &= 8x^4-8x^2+1
\end{align}

An interesting property of these polynomials are \(T_m(x)\) and \(T_n(x)\) are orthogonal weighted by \(\frac{1}{\sqrt{1-x^2}}\).

\begin{align}
   \int_{-1}^{1}\frac{ T_n(x)T_m(x)}{\sqrt{1-x^2}}dx &= 0 , m \neq n\\
                                    &= \pi,m = n= 0\\
                                    &= \frac{\pi}{2}, m = n \neq 0               
\end{align}

\textbf{Proof:-}

Let \(x = \cos \theta\) where \(\theta \in [0,\pi]\). This implies that \(dx = -\sin \theta \, d \theta = -\sqrt{1-x^2} \, d \theta\). And \(x=-1 \implies \theta = \pi\) and \(x =1 \implies \theta = 0\). Therefore,
\begin{align*}
\int_{-1}^{1} T_m(x)T_n(x) \frac{1}{\sqrt{1-x^2}} dx & = \int_{0}^{\pi} \cos(m\theta) \cos(n\theta) d\theta \\
&= \frac{1}{2}{\int_{0}^{\pi}} \cos(m+n)\theta 
+ \cos(m-n)\theta d\theta  \\
&=\frac{1}{2} \left[\sin\frac{(m+n)\theta }{m+n}\right]_{0}^{\pi}
+ \left[\sin\frac{(m-n)\theta }{m-n}\right]_{0}^{\pi}\\
&= 0, \text{ if } m \neq n 
\end{align*}

Similarly with appropriate substitution, conditions for \$m = n=0 \$ and \$m = n \neq 0 \$ can be proved.

The zeros of Chebyshev polynomial \(T_{n+1}(x)\) are given by \(x_k = \cos\left( \frac{2k+1}{2n+2} \pi \right)\) where \(k \in \{ 0,1,2,\cdots,n \}\).

\textbf{Proof:-}

\begin{flushleft}
\textbf{Proof:-}
\end{flushleft}

We have \[T_{n+1}(x) = \cos((n+1)\arccos(x))\]
\[T_{n+1}(x) = 0 \implies \cos((n+1)\arccos(x)) = 0\]

\[\implies(n+1) \arccos(x) = (2k+1)\frac{\pi}{2} \ \ \ k \in \mathbb{Z}\]

\[\implies \arccos(x) = (2k+1)\frac{\pi}{2(n+1)} \ \ \ \ k \in \mathbb{Z}\]

But as the principle range of \(\arccos(x)\) is defined as \([0,\pi]\), we must restrict the values \(k\) can take.

\[0 \le (2k+1)\frac{\pi}{2(n+1)} \le \pi \implies 0 \le k \le n+\frac{1}{2} \]

But as \(k \in \mathbb{Z}\), we can say that the possible values \(k\) can take are \(\{ 0,1,2,\cdots,n\}\).

Therefore \(\arccos(x_k) = (2k+1)\frac{\pi}{2(n+1)} \ \ \ \ k \in \{ 0,1,2,\cdots,n\}\)
\[\implies x_k = \cos\left( (2k+1)\frac{\pi}{2(n+1)} \right)\ \ \ \ k \in \{ 0,1,2,\cdots,n\}\]

Therefore the zeros of \(T_{n+1}(x)\) are in the interval \([-1,1]\) are given by \(x_k = \cos\left( \frac{2k+1}{2n+2} \pi \right)\) where \(k \in \{ 0,1,2,\cdots,n \}\). The number of zeros is also consistent with the fact that as \(T_{n+1}(x)\) is an \((n+1)\) th-degree polynomial, it has \((n+1)\) roots. Also, these zeros are distinct.

These zeros are called the Chebyshev nodes of first kind. They minimise \(||w(x)||_{\infty}\)

\textbf{Theorem}

If \(f(x)\) is smooth on \([-1,1]\), then Lagrange interpolation using the roots of Chebyshev nodes of first kind converges uniformly.

\textbf{INSERT RUNGE FUNCTION-CHEBYSHEV NODES CONVERGENCE CODE}

\hypertarget{wierstrass-approximation-theorem}{%
\section{Wierstrass Approximation theorem}\label{wierstrass-approximation-theorem}}

\emph{Suppose \(f\) is a continuous real-valued function defined on the real interval \([a,b]\). For every \(\epsilon > 0\), there exists a polynomial \(p\) such that for all \(x\) in \([a,b]\), we have \(|f(x) - p(x)| < \epsilon\).}

\emph{\textbf{Bernstein polynomial:}}

The Bernstein basis polynomials of degree \(n\) are defined as
\begin{equation}
    b_{k,n}(x) = {}^nC_{k}\, x^k (1-x)^{n-k}
\end{equation}
A linear combination of these basis polynomials can be used to obtain other polynomials. One of the properties of these polynomials is that
\begin{equation}
\begin{aligned}
    \sum_{k=0}^n {b_{k,n}(x)} &= \sum_{k=0}^n {{}^nC_{k}\, x^k (1-x)^{n-k}} \\
    &= (x + (1 - x))^n = 1
    \label{equ:property}
\end{aligned}
\end{equation}
So these polynomials can also be considered to act as some kind of weights. Now, for approximating functions, the Bernstein Polynomial is defined as
\begin{equation}
    B_n(f;x) = \sum_{k=0}^n {f\left(\dfrac{k}{n}\right) {}^nC_{k}\, x^k (1-x)^{n-k}}
    \label{eqn:bern}
\end{equation}
Here, \(f\) is the function being approximated, \(n\) is the order of approximation \& \(x\) is the point at which the approximation is made.

\textbf{Proof:}

To prove the theorem on closed intervals \([a,b]\), without loss of generality, we can take the closed interval as \([0, 1]\). Thus, \(f\) can be considered as a continuous real-valued function on \([0, 1]\). Since \(f\) is a continuous function, we can say that for a given \(\epsilon > 0\), there exists a \(\delta > 0\) such that:
\begin{equation}
    |x-y| < \delta \implies |f(x) - f(y)| <\frac{\epsilon}{2} \ \ \ \  \forall \, x,y \in [0,1]
    \label{equ:convcond}
\end{equation}
To prove that \(B_n(f,x)\) converges to \(f(x)\) uniformly, we show that \(|B_n(f,x) - f(x)|\) has to be made small. By the definition of Bernstein's polynomial, as shown in Eqn. \ref{eqn:bern} and by using the property given in Eqn. \ref{equ:property}, we can write \(|B_n(f;x) - f(x)|\) as:
\begin{align*}
    |B_n(f;x) - f(x)| &= \left| \sum_{k=0}^n {\left(f\left(\dfrac{k}{n}\right) - f(x)\right) {}^nC_{k} \, x^k (1-x)^{n-k}} \right| \\
    &\leq \sum_{k=0}^n {\left|f\left(\dfrac{k}{n}\right) - f(x)\right| {}^nC_{k} \, x^k (1-x)^{n-k}}
\end{align*}
Now, if we consider \(y=\dfrac{k}{n}\) in Eqn. \ref{equ:convcond} and if \(\left|\dfrac{k}{n} - x\right| < \delta\), we can say that \(\left|f\left(\dfrac{k}{n}\right) - f(x)\right| < \dfrac{\epsilon}{2}\). But this is not true in the entire domain. So, we partition the domain in to two sets: \(A\) and \(B\), where \(A\) and \(B\) have the following properties:
\begin{equation}
    \begin{aligned}
        A \cup B &= [0,1]\\
        A \cap B &= \phi\\
        A &= \{x:|k/n - x| \leq \delta, x \in [0,1]\}\\
        B &= \{x:|k/n-x| > \delta, x \in [0,1]\}
    \end{aligned}
    \label{equ:sets}
\end{equation}
By dividing the domain into sets \(A\) and \(B\) as defined in \ref{equ:sets}, we can write:
\begin{equation*}
\begin{split}
    |B_n(f;x) - f(x)| = \sum_{x\in A} \left|f\left(\dfrac{k}{n}\right) - f(x)\right| &{}^nC_{k}\, x^k (1-x)^{n-k} \\
    &+ \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{split}
\end{equation*}
From Eqns. \ref{equ:convcond} and \ref{equ:sets}, we can say that \(\left|f\left(\dfrac{k}{n}\right) - f(x)\right|\) has an upper bound of \(\dfrac{\epsilon}{2}\) on the set \(A\). Therefore, we can write:
\begin{equation*}
\begin{split}
    |B_n(f;x) - f(x)| \leq \sum_{x\in A} \left(\dfrac{\epsilon}{2}\right){}^nC_{k}\, x^k &(1-x)^{n-k} \\
    &+ \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{split}
\end{equation*}
By using the property described in Eqn. \ref{equ:property}, we can simplify the above inequality as:
\begin{equation*}
    |B_n(f,;x) - f(x)| \leq \frac{\epsilon}{2} + \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{equation*}
Since \(f\) is uniformly converging in \([0,1]\), \(f\) is bounded from above. So, let the maximum value of \(f\) in the domain be \(M\). Thus:
\begin{equation*}
    \text{max\{} | f(x) | \} = M \ \ \ \  \forall \, x \in [0,1]
\end{equation*}
Thus, the maximum value \(\left|f\left(\dfrac{k}{n}\right) - f(x)\right|\) can achieve in the domain is \(2M\) (considering the case where, one of them is \(M\) and the other is \(-M\)). Thus, we have:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + (2M) \sum_{x\in B} {{}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
Now, in set \(B\), \(\left| \dfrac{k}{n} - x \right| > \delta\). Thus, we get:
\begin{equation*}
    \frac{\left( k/n - x \right)^2}{\delta^2} > 1
\end{equation*}
Multiplying this to the second term of RHS, we get:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + (2M) \sum_{x\in B} {\frac{\left( k/n - x \right)^2}{\delta^2}{}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
The second term can now be written as:
\begin{equation*}
    \dfrac{2M}{\delta^2 n^2} \sum_{x\in B} {(k-nx)^2 \, {}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
The summation term is equivalent to computing the variance of a binomial distribution with parameters \(n\) \& \(x\). The variance is given by \(nx(1-x)\). Thus, we get:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{2M}{\delta^2 n^2} nx(1-x)
\end{equation*}
We know that using AM \(\geq\) GM:
\begin{equation*}
    x(1-x) \leq \dfrac{1}{4}
\end{equation*}
Thus, we have:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{M}{2\delta^2 n}
\end{equation*}
Now, let us define a quantity \(N\) such that the above condition holds true for all \(n > N\), which gives us \(\dfrac{1}{n} < \dfrac{1}{N}\). Thus:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{M}{2\delta^2 N}
\end{equation*}
If we choose the \(N\) such that:
\begin{equation*}
    \dfrac{M}{2\delta^2N} = \dfrac{\epsilon}{2}
\end{equation*}
giving us:
\begin{align*}
    |B_n(f;x) - f(x)| &\leq \dfrac{\epsilon}{2} + \dfrac{\epsilon}{2} \\
    |B_n(f;x) - f(x)| &\leq \epsilon
\end{align*}
Thus, we have for all \(n > N\), where \(N = \dfrac{M}{\delta^2\epsilon}\), we have
\begin{equation}
    |B_n(f;x) - f(x)| \leq \epsilon
\end{equation}
i.e., the Bernstein Polynomial \(B_n(f;x)\) uniformly converges to \(f(x)\) for all \(x\) in the domain \([0,1]\).

\hypertarget{parts}{%
\chapter{Parts}\label{parts}}

You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file.

Add a numbered part: \texttt{\#\ (PART)\ Act\ one\ \{-\}} (followed by \texttt{\#\ A\ chapter})

Add an unnumbered part: \texttt{\#\ (PART\textbackslash{}*)\ Act\ one\ \{-\}} (followed by \texttt{\#\ A\ chapter})

Add an appendix as a special kind of un-numbered part: \texttt{\#\ (APPENDIX)\ Other\ stuff\ \{-\}} (followed by \texttt{\#\ A\ chapter}). Chapters in an appendix are prepended with letters instead of numbers.

\hypertarget{root-finding-algorithms}{%
\chapter{Root Finding Algorithms}\label{root-finding-algorithms}}

\hypertarget{motivation-2}{%
\section{Motivation}\label{motivation-2}}

Let \(f(x)\) be a continuous function. We are interested in solving \(f(x)=0\) for \(x\),i.e., we are interested in finding \(x^*\) such that \(f(x^*)=0\). But finding \(x^*\) analytically is not always easy.

Examples:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Solving for \(x\) for \(3x=9\), \(x^2+2x-1=0\) are easy. We have closed form zeros for \(x\) till 4th order polynomial. Beyond 4th order closed form solution is NOT possible.
\item
  Solving transcendental equations like \(\tan x = x\), \(x=e^x\) exactly.
\end{enumerate}

Considering the issues present, we need to think of solving the equations numerically.

Consider \(f(x) = xe^x-5\). We are now interested to find \(x^*\) such that \(f(x^*)=0\). Let us guess say \(x^*=1.2\). \(f(1.2) = -1.015859692716143\). We need to refine this further! For this we need to find a sequence of \(x_k\)'s iteratively, such that the sequence \(\{x_k\}\) converges to \(x^*\).

\hypertarget{bisection-method}{%
\section{Bisection Method}\label{bisection-method}}

We know from Intermediate Value theorem that if \(f(x)\) is continuous on \([a,b]\) and if \(f(a) f(b)<0\), \(\exists x^* \, \in(a,b)\) such that \(f(x^*)=0\).

Given a continuous function \(f(x)\) and if we could find \(a,b\in \mathbb{R}\) such that \(a<b\) and \(f(a) f(b)<0\), then we can find \(x^*\) as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose \(x_0 = \frac{a+b}{2}\) and if

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    \(f(x_0) f(a)<0 \implies \ \exists \text{ a } x^*\, \in (a,x_0)\).
  \item
    \(f(x_0) f(a)>0 \implies \ \exists \text{ a } x^*\, \in (x_0,b)\)
  \item
    \(f(x_0) =0 \implies x^* = x_0\)
  \end{enumerate}
\item
  Refine the guess by taking mean of new interval and check for convergence.
\item
  The stopping criteria are \(|f(x)|<\epsilon\) and \(|I_k|<\delta\) where

  \begin{itemize}
  \item
    \(|I_k|\) is the length of the interval \(I_k\) at the \(k^{th}\) step. This is found as:

    Interval at \(0^{th}\) step is \(I_0 = [a,b]\). Hence the length of interval \(I_0\) is \(|I_0|= b-a\).

    Let the interval at \(k^{th}\) step is \(I_k\). Then from step 1,2 we can say that \(|I_k| = \frac{|I_{k-1}|}{2}\). Therefore: \[|I_k| = \frac{b-a}{2^k}\]
  \item
    \(\epsilon\) and \(\delta\) are user specified small tolerences.(eg 1e-10).
  \end{itemize}
\end{enumerate}

\textbf{INSERT PICTURE HERE}

\textbf{INSERT CODE/PSEUDO-CODE HERE}

This method described above is called as Bisection method.

When does Bisection Method fail?

In case of Bisection Method, if 2 initial points \(a,b \in \mathbb{R}\) are chosen such that \(f(a)f(b)<0\), the we can guarantee that Root can be found from the Intermediate value theorem. Thus, the sequence of iterates always converge to the root.

\hypertarget{newton-method}{%
\section{Newton Method}\label{newton-method}}

In Bisection method, we need to find functional value at 2 points initially where we expect the root to lie between them. Is it possible to find the root by taking just 1 initial guess? Newton's method allows us to do this! But we need to know the value of derivative at that point.

\textbf{INSERT PICTURE HERE}

Pick guess value \(x_0\). We improve this guess by finding the unique root of the linear approximation at this point. The linear approximation is the tangent at that point. The equation of tangent to \(f(x)\) at \(x=x_0\) is:
\[y-f(x_0) = f'(x_0) (x-x_0)\]
This intersects \(x\) axis at say \((x_1,0)\). Therefore,
\[0-f(x_0) =  f'(x_0) (x_1-x_0)\]
If \(f'(x_0)\neq 0\), then \(x_1\) exists. Therefore, the improved guess is
\begin{equation}
x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}
\end{equation}

On repeating the process, we get the improved guess iterates as:
\begin{equation}
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
\end{equation}

\textbf{INSERT CODE/PSEUDOCODE HERE}

\hypertarget{rate-of-convergence}{%
\subsection{Rate of Convergence}\label{rate-of-convergence}}

Assume that the iterate sequence \(\{x_k\}\to x^*\) where \(f(x^*)=0\).
Define \(e_k = x_k-x^* \implies x^*=x_k-e_k\).

We assume that \(f(x)\) is a continuous function which is twice differentiable with \(f''(x)\) being continuous. Writing Taylor series of \(f(x^*)=0\) about \(x_k\) gives:
\[f(x^*)=0= f(x_k-e_k) = f(x_k) - e_k f'(x_k)+\frac{e_k^2}{2!} f''(\zeta_k) \]
where \(\zeta_k \in (\min(x_k,x^*),\max(x_k,x^*))\)
\[\implies 0=f(x_k)-(x_k-x^*)f'(x_k)+\frac{e_k^2}{2!} f''(\zeta_k)\]
\[\implies 0 = f(x_k) - (x_k-x_{k+1}+x_{k+1}-x^*)f'(x_k) + \frac{e_k^2}{2!} f''(\zeta_k)\]

\[\implies 0 = f(x_k) - (x_k-x_{k+1}) f'(x_k) - e_{k+1} f'(x_k) +\frac{e_k^2}{2!} f''(\zeta_k)\]
From equation()
\[\implies 0 =f(x_k) - \frac{f(x_k)}{f'(x_k)}f'(x_k) - e_{k+1} f'(x_k) +\frac{e_k^2}{2!} f''(\zeta_k)\]
\[ e_{k+1} = \frac{e_k^2}{2!} \frac{f''(\zeta_k)}{f'(x_k)}\]
As \(\zeta_k \in (\min(x_k,x^*),\max(x_k,x^*))\) and as \(f''(x)\) and \(f'(x)\) are continuous, \(\exists\, M,m\in \mathbb{R}\) such that \(f''(\zeta_k) \le M\) and \(f'(x_k) \ge m\). Therefore,
\[\frac{f''(\zeta_k)}{2 f'(x_k)} \le \frac{M}{2m} = C \ \text{(say)}\]
Therefore,
\begin{equation}
e_{k+1} \le C e_k^2
\end{equation}

This implies that the rate of converge is quadratic for Newton's method. For Bisection, the convergence rate is linear.

\textbf{RATE OF CONVERGENCE plot/code}

\hypertarget{possibility-of-non-convergence}{%
\subsection{Possibility of Non-Convergence?}\label{possibility-of-non-convergence}}

Now the main question comes! When does Newton Method fail?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When \(f'(x_k) = 0\) at any step in iteration. This means that the tangent at \(x=x_k\) is parallel to \(x\) axis and hence, no root can be found from there!
\end{enumerate}

Eg: \(f(x) = x^2-1\) and \(x_0 = 0\) This implies that \(f'(x_0) = 2x_0 = 0\). Thus we have to change initial guess.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(f(x) = x^{\frac{1}{3}}\) and \(x_0 = a>0\). Root is \(x^*=0\)
\end{enumerate}

\(f'(x) = \frac{1}{3x^{\frac{2}{3}}}\)

\[x_{k+1}= x_k - \frac{f(x_k)}{f'(x_k)} = x_k - \frac{x_k^{\frac{1}{3}}}{\frac{1}{3x_k^{\frac{2}{3}}}} = -2 x_k\]

Therefore, \(x_n = (-2)^n x_0 = (-2)^n a\).

\[\lim_{n \to \infty} x_n \text{ doesn't exist.}\]

Thus, convergence is not always guaranteed in Newton's Method.

\hypertarget{blocks}{%
\chapter{Blocks}\label{blocks}}

\hypertarget{equations}{%
\section{Equations}\label{equations}}

Here is an equation.

\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  \label{eq:binom}
\end{equation}

You may refer to using \texttt{\textbackslash{}@ref(eq:binom)}, like see Equation \eqref{eq:binom}.

\hypertarget{theorems-and-proofs}{%
\section{Theorems and proofs}\label{theorems-and-proofs}}

Labeled theorems can be referenced in text using \texttt{\textbackslash{}@ref(thm:tri)}, for example, check out this smart theorem \ref{thm:tri}.

\begin{theorem}
\protect\hypertarget{thm:tri}{}\label{thm:tri}For a right triangle, if \(c\) denotes the \emph{length} of the hypotenuse
and \(a\) and \(b\) denote the lengths of the \textbf{other} two sides, we have
\[a^2 + b^2 = c^2\]
\end{theorem}

Read more here \url{https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html}.

\hypertarget{callout-blocks}{%
\section{Callout blocks}\label{callout-blocks}}

The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: \url{https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html}

\hypertarget{sharing-your-book}{%
\chapter{Sharing your book}\label{sharing-your-book}}

\hypertarget{publishing}{%
\section{Publishing}\label{publishing}}

HTML books can be published online, see: \url{https://bookdown.org/yihui/bookdown/publishing.html}

\hypertarget{pages}{%
\section{404 pages}\label{pages}}

By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you'd like to customize your 404 page instead of using the default, you may add either a \texttt{\_404.Rmd} or \texttt{\_404.md} file to your project root and use code and/or Markdown syntax.

\hypertarget{metadata-for-sharing}{%
\section{Metadata for sharing}\label{metadata-for-sharing}}

Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the \texttt{index.Rmd} YAML. To setup, set the \texttt{url} for your book and the path to your \texttt{cover-image} file. Your book's \texttt{title} and \texttt{description} are also used.

This \texttt{gitbook} uses the same social sharing data across all chapters in your book- all links shared will look the same.

Specify your book's source repository on GitHub using the \texttt{edit} key under the configuration options in the \texttt{\_output.yml} file, which allows users to suggest an edit by linking to a chapter's source file.

Read more about the features of this output format here:

\url{https://pkgs.rstudio.com/bookdown/reference/gitbook.html}

Or use:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{?bookdown}\SpecialCharTok{::}\NormalTok{gitbook}
\end{Highlighting}
\end{Shaded}


  \bibliography{book.bib,packages.bib}

\end{document}
