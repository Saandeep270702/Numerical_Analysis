% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{book}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{5}
\usepackage{booktabs}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{natbib}
\bibliographystyle{plainnat}
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Numerical Analysis},
  pdfauthor={Sai Saandeep.S, Dr.~Sivaram},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Numerical Analysis}
\author{Sai Saandeep.S, Dr.~Sivaram}
\date{2023-08-03}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{1}
\tableofcontents
}
\hypertarget{introduction}{%
\chapter{Introduction}\label{introduction}}

\hypertarget{why-numerical-analysis}{%
\section{Why Numerical Analysis?}\label{why-numerical-analysis}}

\hypertarget{representing-numbers-on-a-machine}{%
\section{Representing Numbers on a Machine}\label{representing-numbers-on-a-machine}}

\hypertarget{condition-number-of-a-problem}{%
\section{Condition Number of a Problem}\label{condition-number-of-a-problem}}

Consider a function in one variable \(f:\mathbb{R}\to\mathbb{R}\).
Condition number for a function \(f(x)\) tells about the error amplification of a function \(f(x)\) i.e., for a given error in input \(x\), how much is the error in the output \(f(x)\).

Absolute Condition Number \(\kappa_{\text{abs}}\) of the function \(f(x)\) is defined as:
\begin{equation}
\kappa_{\text{abs}} = \frac{\text{Absolute Change in Output}}{\text{Absolute Change in Input}} = \lim_{\delta x \to 0} \left\lvert{\frac{f(x+\delta x)-f(x)}{x+\delta x - x}}\right\rvert = \left\lvert{f'(x)}\right\rvert
\end{equation}

Relative Condition Number \(\kappa_{r}\) of the function \(f(x)\) is defined as:
\begin{equation}
\kappa_{r} = \frac{\text{Relative Change in Output}}{\text{Relative Change in Input}} = \lim_{\delta x \to 0} \frac{\left\lvert{\frac{f(x+\delta x)-f(x)}{f(x)}}\right\rvert}{\left\lvert{\frac{x+\delta x - x}{x}}\right\rvert} = \left\lvert{\frac{x}{f(x)}f'(x)}\right\rvert
\end{equation}

Now what if the function has multiple inputs? Or What if the function has multiple outputs?

Examples:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Input 2 numbers \(a,b\in\mathbb{R}\) and then find \(f(a,b) = a+b\)?. This problem takes 2 inputs- \(a, b\) and one output \(f(a,b)\).
\item
  Find the roots of a polynomial \(a_0+a_1x+a_2x^2+\cdots+ a_nx^n\). We are inputting the vector \(\begin{bmatrix} a_0 & a_1 & a_2 & \cdots & a_n\end{bmatrix}^T\) and the output is \(x\) in this case.
\item
  Given a matrix \(A\in\mathbb{R}^{m\times n}\). Input a vector \(\mathbf{x}\in\mathbb{R}^{n\times 1}\) and then find \(f(\mathbf{x}) = A\mathbf{x} \in \mathbb{R}^{m\times 1}\)?
\item
  Solve the linear system \(A\mathbf{x}=\mathbf{b}\) where \(A\in\mathbb{R}^{m\times n}\), \(\mathbf{x}\in\mathbb{R}^{n\times 1}\) and \(\mathbf{b}\in\mathbb{R}^{m\times 1}\) . Inputs are \(A, \mathbf{b}\) and output is a vector \(\mathbf{x}\).
\end{enumerate}

To accommodate these cases, a generalized definition of a (relative) condition number \(\kappa_r\) for a function \(f:X\to Y\) where \(X \subset \mathbb{R}^{m\times 1}\) and \(Y \subset \mathbb{R}^{n\times 1}\) is shown below:
\begin{equation}
  \kappa_r = \lim_{r\to 0} \sup_{\lVert x \rVert_q \le r} \frac{\frac{\lVert f(x+\delta x)-f(x) \rVert_p}{\lVert f(x) \rVert_p}}{\frac{\lVert \delta x \rVert_q}{\lVert x \rVert_q}}
\end{equation}

where \(p,q \in \mathbb{N}\) and \(\lVert . \rVert_p\) denotes the vector \(p-\) norm.

\hypertarget{vector-normsrecap}{%
\subsection{Vector Norms(Recap)}\label{vector-normsrecap}}

For a vector \(x\) in the vextor space \(X\) over a field \(F\), \(\lVert . \rVert:F\to R\) is defined such that:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\lVert x \rVert\ge 0 \ \ \ \forall x \in X\).
\item
  \(\lVert \alpha x \rVert = \left\lvert{\alpha}\right\rvert, \ \ \ \forall x \in X, \ \ \ \alpha \in F\)
\item
  \(\lVert x+y \rVert \le \lVert x \rVert+\lVert y \rVert, \  \ \ \forall x,y \in X\).
\item
  \(\lVert x \rVert = 0 \Longleftrightarrow x = 0\)
\end{enumerate}

Let \(x =\begin{bmatrix} x_1 & x_2&\cdots &x_n \end{bmatrix}^T\). Different possible vector norms which satisfy the above conditions are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Euclidean norm (2-norm)
  \begin{equation}
  \lVert x \rVert_2 = \sqrt{x_1^2+x_2^2+\cdots+x_n^2}
  \end{equation}
\item
  Supremum norm(max. norm)
  \begin{equation}
  \lVert x \rVert_{\max} = \lVert x \rVert_{\infty} = \max_{1\le i\le n} |x_i|
  \end{equation}
\item
  1-norm
  \begin{equation}
  \lVert x \rVert_1 = \sum_{i=1}^n |x_i|
  \end{equation}
\item
  \(p\)-norm
  \begin{equation}
  \lVert x \rVert_{p} = \left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}
  \end{equation}
\end{enumerate}

\textbf{NOTE:-} Supremum norm of \(x\) is \(p\)-norm of \(x\) as \(p\to\infty\)

Proof:- From the definition, \[\lim_{p\to\infty} \lVert x \rVert_p = \lim_{p\to\infty}\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}}\]

\[\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}} \le \left( n\max_{1\le i\le n} |x_i|^p \right)^{\frac{1}{p}} = n^{\frac{1}{p}} \max_{1\le i\le n} |x_i|\]

\[\left(\sum_{i=1}^n |x_i|^p\right)^{\frac{1}{p}} \ge \left( \max_{1\le i\le n} |x_i|^p \right)^{\frac{1}{p}} = \max_{1\le i\le n} |x_i|\]

From the above 2 inequalities, we can say that:
\[\max_{1\le i\le n} |x_i| \le \lVert x \rVert_p \le n^{\frac{1}{p}} \max_{1\le i\le n} |x_i|\]
As \(p \to \infty,\ \  \ n^{\frac{1}{p}} \max_{1\le i\le n} |x_i| \to \max_{1\le i\le n} |x_i|\). Therefore, by using sandwich theorem, we can say that \[\lVert x \rVert_p = \max_{1\le i\le n} |x_i|\]

\hypertarget{examples-on-finding-condition-number}{%
\subsection{Examples on finding Condition number}\label{examples-on-finding-condition-number}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Let \(f(a,b) = a+b\). Find the condition number of this problem?
\end{enumerate}

The inputs are \(a,\, b\). Let the inputs have an error \(\delta a,\, \delta b\) respectively.

\[\text{Relative error in input} = \dfrac{\left\Vert \begin{bmatrix} a+\delta a\\ b+\delta b \end{bmatrix}- \begin{bmatrix} a\\  b \end{bmatrix}\right\Vert_p}{\left\Vert \begin{bmatrix} a\\ b \end{bmatrix}\right\Vert_p}\]

For simplicity, let us consider 2-norm. Any norm can be used in fact. Therefore,

\[\text{Relative error in input} =\dfrac{\sqrt{\delta a^2+\delta b^2}}{\sqrt{a^2+b^2}}\]
The output \(f(a+\delta a,b+\delta b) = a+b+\delta a+\delta b\). Therefore,
\[\text{Relative Error in output} = \dfrac{|(a+b+\delta a + \delta b)-(a+b)|}{|a+b|} = \frac{|\delta a+\delta b|}{|a+b|}\]
The relative condition number is:
\[\kappa_r = \lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r} \dfrac{\frac{|\delta a+\delta b|}{|a+b|}}{\dfrac{\sqrt{\delta a^2+\delta b^2}}{\sqrt{a^2+b^2}}}\]
\[\implies \kappa_r = \lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r} \dfrac{|\delta a+\delta b|}{\sqrt{\delta a^2+\delta b^2}} \cdot \dfrac{\sqrt{a^2+b^2}}{|a+b|} \]
To calculate \[\lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r}\dfrac{|\delta a+\delta b|}{\sqrt{\delta a^2+\delta b^2}}\] we assume that \(\delta a = \alpha \cos \theta\) and \(\delta b = \alpha \sin \theta\) where \(\alpha>0\) and \(0\le\theta<2\pi\).

Therefore, we have:
\[\lim_{r\to 0} \sup_{\left\Vert\begin{bmatrix} \delta a\\ \delta b \end{bmatrix}\right\Vert_2\le r}\dfrac{|\delta a+\delta b|}{\sqrt{\delta a^2+\delta b^2}} = \lim_{r \to 0} \sup_{\alpha < r} \dfrac{|\alpha \cos \theta+\alpha \sin \theta|}{\alpha} = \lim_{r\to 0} \sup_{\alpha<r}|\cos \theta+\sin \theta| = \sqrt{2}\]
Thus, the condition number for adding 2 numbers is:
\[ \kappa_r = \dfrac{\sqrt{2(a^2+b^2)}}{|a+b|}\le \sqrt{2} \text{ (if $a,\, b>0$)}\]
(as \(|a+b| \ge \sqrt{a^2+b^2}\) for \(a,b \in \mathbb{R}^+\))

For \(a,b>0\), we can clearly see that the condition number is bounded above by \(\sqrt{2}\). In other words, \textbf{addition is well-conditioned.}

By performing a similar exercise, we can show that the \textbf{subtraction is ill-conditioned} as for \(\frac{a}{b} \to 1\), \(\kappa_r \to \infty\).

Multiplication and division operations are also ill-conditioned.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Condition number on finding roots of the polynomial \(x^2-2x+1\).
\end{enumerate}

\hypertarget{numerical-linear-algebra}{%
\chapter{Numerical Linear Algebra}\label{numerical-linear-algebra}}

\hypertarget{columnspace-nullspace-and-all}{%
\section{Columnspace, Nullspace and all}\label{columnspace-nullspace-and-all}}

Consider a matrix \(A\in\mathbb{R}^{m\times n}\) defined as:
\[A = \begin{bmatrix} -&r_1^T&-\\ -&r_2^T&-\\ & \vdots& \\ - & r_m^T& - \end{bmatrix} = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}\]
where \(r_i \in \mathbb{R}^{n\times 1}\) for \(1\le i \le m\) are the rows and \(a_i \in \mathbb{R}^{m\times 1}\) for \(1\le i\le n\) are the columns of \(A\).

Columnspace of a matrix \(A\) is the span(linear combination) of columns of \(A\). Also called as Range of \(A\).
\begin{equation}
\text{Range}(A) =\text{Columnspace}(A) =  \{ Ax : x\in \mathbb{R}^{n\times 1} \}
\end{equation}

\[Ax = \begin{bmatrix} | & | &  & | \\ a_1 & a_2 &\cdots & a_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}x_1\\x_2\\ \vdots\\ x_n \end{bmatrix} = \sum_{i=1}^n a_i x_i\]

Rowspace of a matrix \(A\) is the span(linear combination) of rows of \(A\).
\begin{equation}
\text{Rowspace}(A) = \{ A^Ty : y\in \mathbb{R}^{m\times 1} \} 
\end{equation}

\[A^Ty = \begin{bmatrix} | & | &  & | \\ r_1 & r_2 &\cdots & r_n \\ | & | & & |\end{bmatrix}\begin{bmatrix}y_1\\y_2\\ \vdots\\ y_n \end{bmatrix} = \sum_{i=1}^n r_i y_i\]
Nullspace of a matrix \(A\) is defined as follows:
\begin{equation}
\text{Nullspace}(A) = \{ z\in \mathbb{R}^{n\times 1}: Az=0\}
\end{equation}

NOTE:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A linear system \(Ax=b\) has a solution ONLY IF \(b\in\text{Range}(A)\).
\item
  Dimension of Range\((A)\) is the number of linearly independent columns of \(A\) or the column rank of \(A\). Similarly, the dimension of Rowspace\((A)\) is the row rank or the number of independent rows of \(A\).
\item
  For a matrix \(A\), row rank = column rank = rank\(\le \min(m,n)\).
\item
  Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector \(z\in \text{Nullspace}(A)\) and \(w \in \text{Rowspace}(A)\) , \(z\) is orthogonal to \(w\).
\end{enumerate}

Proof:- Let \(w \in \text{Rowspace}(A)\), then \(\exists y\in\mathbb{R}^{n\times 1}\) such that \(w = A^Ty\).

Also as \(z\in \text{Nullspace}(A)\), we have \(Az=0\).

Therefore,
\[\langle w,z\rangle = w^Tz = y^T Az = 0\]
Thus, nullspace of a matrix is orthogonal to row space of a matrix.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  \textbf{Rank-Nullity Theorem:} Dimension of Nullspace\((A)\)+Rank\((A)\) = \(n\) = No.~of columns of \(A\)
\end{enumerate}

\hypertarget{matrix-norms}{%
\section{Matrix Norms}\label{matrix-norms}}

Consider a matrix \(A\in\mathbb{R}^{m\times n}\). Just like how we have defined a vector norm, we could have defined an ``element wise matrix norm'' as follows:
\begin{equation}
\lVert A \rVert_p^* = \left(\sum_{i=1}^n |A_{ij}|^p\right)^{\frac{1}{p}} 
\end{equation}

But this definition of norm does not satisfy the \textbf{submultiplicative property}. We are interested in this property as this dictates the convergence of iterative schemes.

A matrix norm is said to be submultiplicative if for any matrices \(A\in \mathbb{R}^{m\times k}\) and \(B \in \mathbb{R}^{k\times n}\), we have
\begin{equation}
\lvert AB \rVert \le \lVert A \rVert  \lVert B \rVert
\end{equation}

Consider the case \[A = \begin{bmatrix} 2&2\\2&2 \end{bmatrix}\] and \(p\to \infty\), Therefore we have \[ \lVert A \rVert_{\infty}^* = \max_{1\le i \le m,1 \le j \le n} |A_{ij}| = 2\].

\[A^2 = \begin{bmatrix} 8 & 8\\ 8& 8 \end{bmatrix}\]
Therefore,
\[\lVert A^2 \rVert_{\infty}^* = 8\]
We can clearly see that:
\[\lVert A^2 \rVert_{\infty}^* = 8 \ge \lVert A \rVert_{\infty}^* \cdot \lVert A \rVert_{\infty}^* = 2 \times 2 = 4\]

which violates the submultiplicative property.

Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows.
\begin{equation}
\lVert A \rVert_p = \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Ax \rVert_p}{\lVert x \rVert_p} = \sup_{\lVert y \rVert_p = 1} \lVert Ay \rVert_p
\end{equation}

p-norms are submultiplicative.

PROOF:- From the definition of p-norm,
\begin{align*}
\lVert AB \rVert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert x \rVert_p}\\
&= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \\
&\le \left[\sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert ABx \rVert_p}{\lVert Bx \rVert_p} \right] \cdot \left[ \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Bx \rVert_p}{\lVert x \rVert_p} \right]  \\
&= \lVert A \rVert_p \cdot \lVert B \rVert_p\\
\therefore \lVert AB \rVert_p \le \lVert A \rVert_p \cdot \lVert B \rVert_p
\end{align*}

Using this property, we can say that
\begin{equation}
 \lVert A^n \rVert_p \le \lVert A \rVert^n_p
\end{equation}

\(\lVert A\rVert_1\) = Maximum of column sum of absolute values.

\(\lVert A\rVert_{\infty}\) = Maximum of row sum of absolute values.

\hypertarget{condition-number-of-matrix-vector-products}{%
\section{Condition number of Matrix vector products}\label{condition-number-of-matrix-vector-products}}

Consider a matrix \(A\in\mathbb{R}^{m\times n}\) and a vector \(x\in\mathbb{R}^{n\times 1}\). Assume that there is no error in representing \(A\). We are interested in finding the condition number of the Matrix-Vector product \(f(x;A)= Ax\).

From the definition of condition number, we can write the condition number \(\kappa_r\) of the matrix vector product as:

\[\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_q\le r} \dfrac{\frac{\Vert A(x+\delta x)-Ax \Vert_p}{\Vert Ax \Vert_p}}{\frac{\Vert x+\delta x-x\Vert_q}{\Vert x\Vert_q}}\]
For simplicity let us choose \(p=q\). Therefore,
\[\kappa_r = \lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} \frac{\Vert x \Vert_p}{\Vert Ax\Vert_p}\]
From the definition of matrix p-norm, we can say that: \[\lim_{r\to 0} \sup_{\Vert \delta x \Vert_p\le r} \frac{\Vert A\delta x \Vert_p}{\Vert \delta x \Vert_p} = \Vert A\Vert_p\]
Therefore the condition number of the matrix vector product is:
\begin{equation}
\kappa_r =  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}
\end{equation}

From Sub-multiplicative property, as \(\Vert Ax\Vert_p \ge\Vert A \Vert_p \Vert x \Vert_p\), we can show that \(\kappa_r\ge 1\).

\textbf{Case-1:-} \(A\in \mathbb{R}^{m\times n}\) is a fat matrix(\(m<n\))

From Rank-Nullity Theorem, we know that

\[\text{Dimension of Nullspace$(A)$+Rank$(A)$ = $n$}\]

We know that Rank\((A) \le \min(m,n) \implies\) Rank \((A) \le m\) as \(A\) is a fat matrix.

\[\implies \text{Dimension of Nullspace}(A) \ge n-m\]
\(\implies \exists\) a non-zero vector \(z \in\) Nullspace\((A)\) i.e., \(\exists z\in \mathbb{R}^{n\times 1}\) such that \(Az=0\).

From the definition of condition number as in equation(), we have:
\(\kappa_r(A,x) = \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}\)

If \(x=z\) then as \(Az=0\), we have \(\Vert Az\Vert_p = 0\). Therefore \(\kappa_r \to \infty\).

Thus, multiplication by a fat matrix is \textbf{highly ill-conditioned.}

\textbf{Case-2:-} \(A\) is an invertible square matrix

From the definition of condition number as in equation(), we have:
\begin{align}
\kappa_r(A,x) &=  \frac{\Vert A \Vert_p \Vert x \Vert_p}{\Vert Ax\Vert_p}\\
 &= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}

From submultiplicative property of matrix norms, we can write \(\Vert A^{-1}(Ax) \Vert_p \le \Vert A^{-1} \Vert_p \Vert Ax \Vert_p\). Therefore,

\begin{align}
\kappa_r(A,x)&= \frac{\Vert A \Vert_p \Vert A^{-1}(Ax) \Vert_p}{\Vert Ax\Vert_p}\\
\implies \kappa_r(A,x)&\le \frac{\Vert A \Vert_p \Vert A^{-1} \Vert_p \Vert Ax \Vert_p}{\Vert Ax\Vert_p}\\
\end{align}

\begin{equation}
\kappa_r(A,x) \le \Vert A \Vert_p \Vert A^{-1} \Vert_p
\end{equation}
Therefore condition number is bounded above by \(\Vert A \Vert_p \Vert A^{-1} \Vert_p\) which is independent of vector \(x\).

Define Condition number of matrix \(A\) as \[\kappa_p(A) = \Vert A \Vert_p \Vert A^{-1} \Vert_p\].

From submultiplicative property we can show that \(\kappa_p(A)\ge 1\).

Let us find condition number for some special matrices. Let us consider \(A=Q\) to be an orthogonal/ unitary matrix.

As \(Q\) is an orthogonal matrix \(Q^TQ = I\implies Q^T = Q^{-1}\).

Therefore \[\Vert Qx \Vert_2^2 = (Qx)^TQx = x^TQ^TQx = x^Tx = \Vert x\Vert_2^2\]

Thus, \[\Vert Qx \Vert_2= \Vert x\Vert_2 = \Vert Q^Tx \Vert_2\]

From definition of 2-norm of a matrix, we have,
\begin{align}
\Vert Q\Vert_p &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert Qx \rVert_p}{\lVert x \rVert_p}\\
\implies \Vert Q\Vert_2 &= \sup_{x\in\mathbb{R}^{n}\setminus \{ \mathbf{0} \}} \frac{\lVert x \rVert_2}{\lVert x \rVert_2}\\
\implies \Vert Q\Vert_2 &= 1
\end{align}

Therefore,
\begin{equation}
\Vert Q^T\Vert_2 = 1
\end{equation}

The condition number of \(Q\) is therefore,
\begin{equation}
\kappa_2(Q) = \Vert Q \Vert_p \Vert Q^{-1} \Vert_p = \Vert Q \Vert_2 \Vert Q^T \Vert_2 = 1 
\end{equation}

\hypertarget{solving-linear-systems}{%
\section{Solving Linear Systems}\label{solving-linear-systems}}

Consider the linear system \(Ax=b\) where \(A\in\mathbb{R}^{m\times n}\), \(\mathbf{x}\in\mathbb{R}^{n\times 1}\) and \(\mathbf{b}\in\mathbb{R}^{m\times 1}\) . Inputs are \(A, \mathbf{b}\) and output is a vector \(\mathbf{x}\).

In this course, we assume that \(A\) is a square(i.e., \(m=n\)) and invertible matrix (so that we have a unique \(x\)). Our goal is to find that unique \(x\) which satisfies the system of equations.

Let's go step by step. Let us consider special matrices first and then discuss about a general matrix \(A\).

\hypertarget{special-matrices}{%
\subsection{Special Matrices}\label{special-matrices}}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  If \(A\) is a unitary matrix, then \(AA^T = I=A^TA\).
  \[Ax=b \implies A^T Ax = A^Tb \implies x = A^T b\]
  To get \(x\), we need to perform \(n^2\) multiplications and \(n(n-1)\) additions.
\item
  If \(A=U\) is an upper triangular matrix.
\end{enumerate}

Let \(Ux=b\) in matrix form be:
\begin{equation}
\begin{bmatrix}
U_{11} & U_{12} & U_{13} & \cdots & U_{1n}\\
0      & U_{22} & U_{23} & \cdots & U_{2n}\\
0      & 0      & U_{33} & \cdots & U_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
0      & 0      & 0      & \cdots & U_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}

The last row of \(U\) corresponds to the equation
\begin{equation}
U_{nn} x_n = b_n
\end{equation}

Now as \(U\) is invertible, all the diagonal elements are non-zero i.e., \(U_{ii}\neq 0 \ \forall i \in\{1,2,\cdots , n\}\).

Therefore
\begin{equation}
U_{nn} x_n = b_n \implies x_n = \frac{b_n}{U_{nn}}
\end{equation}

Similarly, we have:
\begin{equation}
U_{(n-1),(n-1)}x_{n-1}+U_{(n-1),n}x_n = b_{n-1} \implies x_{n-1} = \frac{b_{n-1}-U_{(n-1),n}x_n}{U_{(n-1),(n-1)}}
\end{equation}

Using induction, we can prove that for \(i \in \{1,2,\cdots,n-1\}\), we have:
\begin{equation}
x_i = \frac{b_i -\sum_{j=i+1}^n U_{i,j}x_j}{U_{ii}}
\end{equation}

To calculate \(x_i\)(for \(i \in \{1,2,\cdots,n\}\)), we need to perform 1 division, \((n-i)\) multiplications and \((n-i)\) additions/subtractions.

Therefore to calculate the output vector \(x\), we need to perform

\begin{itemize}
\item
  \(\sum_{i=1}^n 1 = n\) divisions
\item
  \(\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}\) multiplications
\item
  \(\sum_{i=1}^n (n-i) = \frac{n^2-n}{2}\) additions/subtractions
\item
  In total \(n+ \frac{n^2-n}{2}+\frac{n^2-n}{2} = n^2\) operations are required to get \(x\). In other words ``Computational Complexity is \(n^2\)''
\end{itemize}

\begin{quote}
\textbf{NOTE:- }

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Loosely speaking computational complexity of an algorithm means the number of operations performed in the algorithm.
\item
  We say that \(f(n)\in \mathcal{O}(g(n))\)
\end{enumerate}
\end{quote}

\hypertarget{general-case}{%
\subsection{General Case}\label{general-case}}

We know that to solve \(Ax=b\), we convert the Augmented matrix \([A|b]\) to its Row Reduced Echelon Foem(RREF) by doing elementary row operations on \(A\) to get an Identity matrix.

Instead of fully converting to an Identity matrix, let us convert to an upper triangular matrix, say \(U\).i.e., we do row operations to get \[Ux=c\] from \(Ax=b\) where \(U\) is an upper triangular matrix and \(c\) is the vector obtained by performing corresponding operations on \(b\).

Let \(Ax=b\) in matrix form be:
\begin{equation}
\begin{bmatrix}
A_{11} & A_{12} & A_{13} & \cdots & A_{1n}\\
A_{21} & A_{22} & A_{23} & \cdots & A_{2n}\\
A_{31} & A_{32} & A_{33} & \cdots & A_{3n}\\
\vdots & \vdots & \vdots & \ddots & \vdots\\
A_{n1} & A_{n2} & A_{n3} & \cdots & A_{nn}
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}

To convert \(A\) to an upper triangular matrix \(U\) (i.e., to have \(U_{ij} = 0\) for \(i>j\)) using row operations, we need to make all elements below the diagonal to be zero. To achieve this, we firstly make all elements below \(A_{11}\) in the 1st column to be zero using the below operations on \(j^{th}\) row (\(j>1\)) :(We assume that \(A_{11}\neq 0\))

\begin{align}
R_{j,1} &=0\\
R_{j,2:n} &\gets R_{j,2:n} - \frac{A_{j1}}{A_{11}}R_{1,2:n} \\
b_j &\gets b_j - \frac{A_{j1}}{A_{11}} b_1
\end{align}

We proceed like this to make all elements below the diagonal to be zero. Consider \(k^{th}\) step in this process. Let the matrix in this step be:

\begin{equation}
\begin{bmatrix}
A_{11} & A_{12} & A_{13} & \cdots & A_{1,k} & \cdots & A_{1n}\\
0      & A_{22} & A_{23} & \cdots & A_{2,k} & \cdots & A_{2n}\\
0      & 0      & A_{33} & \cdots & A_{3,k} & \cdots & A_{3n}\\
\vdots & \vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0      & 0      & 0      & \cdots & A_{k,k} & \cdots & A_{k,n}\\
0      & 0      & 0      & \cdots & A_{k+1,k} & \cdots & A_{k+1,n}\\
\vdots & \vdots & \vdots & \vdots & \ddots  & \vdots & \vdots\\
0      & 0      & 0      & \cdots & A_{n,k} & \cdots & A_{n,n}\\
\end{bmatrix}
\begin{bmatrix} x_1 \\ x_2 \\x_3\\ \vdots\\ x_n\end{bmatrix}
=
\begin{bmatrix} b_1 \\ b_2 \\b_3\\ \vdots\\ b_n\end{bmatrix}
\end{equation}
The row operations, to be done on \(j^{th}\) row, so that all elements below \(A_{kk}\) become zero (in \(k^{th}\) column) is given as follows: For \(j>k\) and \(A_{k,k}\neq 0\)

\begin{align}
R_{j,k} &=0\\
R_{j,k+1:n} &\gets R_{j,k+1:n} - \frac{A_{j,k}}{A_{k,k}}R_{k,k+1:n} \\
b_j &\gets b_j - \frac{A_{j,k}}{A_{k,k}} b_k
\end{align}

Let us now calculate the number of operations required to be done for converting \(A\) to RREF. In 1st step, for the row operation \[R_{j,2:n} \gets R_{j,2:n} - \frac{A_{j1}}{A_{11}}R_{1,2:n}\] for a particular row-\(j\), we need to do 1 division, \(n-1\) multiplications and \(n-1\) additions. There are \(n-1\) such rows. Therefore, for the row operation on all these \(n-1\) rows, \(n-1\) divisions, \((n-1)^2\) multiplications and \((n-1)^2\) additions have to be done.

For \(k^{th}\) step(\(1\le k\le n-1\)), row operation \[R_{j,k+1:n} \gets R_{j,k+1:n} - \frac{A_{j,k}}{A_{k,k}}R_{k,k+1:n}\] on row-\(j\) for \(k+1\le j\le n\), requires 1 division, \((n-k)\) multiplications and \((n-k)\) additions. Therefore, we need
to perform \(n-k\) divisions, \((n-k)^2\) multiplications and \((n-k)^2\) additions.

Therefore, the number of operations required to convert \(A\) to \(U\) are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}\) Divisions
\item
  \(\sum_{k=0}^{n-1} (n-k)^2 = \sum_{k=1}^n k^2 = \frac{n(n-1)(2n-1)}{6}\) Multiplications
\item
  \(\sum_{k=0}^{n-1} (n-k)^2 = \sum_{k=1}^n k^2 = \frac{n(n-1)(2n-1)}{6}\) Additions
\end{enumerate}

Total number of operations to convert from \(A\) to \(U\) = \(\frac{n(n-1)}{2}+2\frac{n(n-1)(2n-1)}{6}\)

Now, let us calculate the number of operations required to get \(c\) from \(b\). Consider the operation on the \(k^{th}\) step(\(1\le k\le n-1\)): \[b_j \gets b_j - \frac{A_{j,k}}{A_{k,k}} b_k\]. This operation requires \((n-k)\) multiplications and \((n-k)\) additions. Note that we have already calculated \(\frac{A_{j,k}}{A_{k,k}}\) while converting \(A\) to \(U\). Hence, no separate division operation is involved.

Therefore, number of operations on RHS are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \(\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}\) Multiplications.
\item
  \(\sum_{k=0}^{n-1} (n-k) = \sum_{k=1}^n k = \frac{n(n-1)}{2}\) Additions
\end{enumerate}

After converting \(Ax=b\) to \(Ux=c\), we have seen that solving \(Ux=c\) requires \(n\) divisions, \(\frac{n(n-1)}{2}\) additions and \(\frac{n(n-1)}{2}\) multiplications.

Therefore, for solving \(Ax=b\):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Number of divisions required = \(\frac{n(n+1)}{2}+n\)
\item
  Number of multiplications required = \(\frac{n(n-1)(2n-1)}{6}+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}\)
\item
  Number of additions required = \(\frac{n(n-1)(2n-1)}{6}+\frac{n(n-1)}{2}+\frac{n(n-1)}{2}\)
\end{enumerate}

We can see that multiplications \& addition operations are the most expensive(time consuming)

Therefore, the total number of operations = \(2\left[ \frac{n(n-1)(2n-1)}{6}+n(n-1)\right]+\frac{n(n+1)}{2}+n\)

In other words Total number of operations \(\in \mathcal{O}(n^3)\)

Now the question arises. Where will this fail?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  We have assumed that \(A_{11} \neq 0\) in the first step. This may not always hold true. Also, there is a possibility that \(A_{kk}=0\) in the intermediate steps also. If this occurs then we can't proceed further.
\item
  Precision issues might occur. Consider solving the system of equations.
\end{enumerate}

\begin{align}
2^{-60}x_1+x_2&=1\\
x_1+x_2&=2
\end{align}
The equations can be written in matrix form as:
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
1& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\2
\end{bmatrix}
\end{equation}

The solution is \(x_1 \approx 1\) and \(x_2 \approx 1\).

Let us try to solve this with the help of method described in this section.
First, we do the row operations \(R_1 \gets R_2 - \frac{1}{2^{-60}}R_1 \implies R_1 \gets R_2 - 2^{60} R_2\) to get:
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
0& 1-2^{60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\2-2^{60}
\end{bmatrix}
\end{equation}

\(1-2^{60}\) and \(2-2^{60}\) is represented on the 64 bit machine as \(2^{-60}\). Therefore, the machine represents the equations as:
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
0& -2^{60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\-2^{60}
\end{bmatrix}
\end{equation}

This implies that \(x_2=1\) and \(2^{-60}x_1+x_2=1\implies x_1=0\). But this is \textbf{significantly} different from the exact solution.

If we have had an infinite precision machine, then we would have got accurate results. But since, we have only finite precision machines, we have this issue.

Let us calculate condition number of this matrix. The condition number as calculated by MATLAB using \texttt{cond()} function gives condition number as 2.6180 which is a relatively small condition number. This means that small changes in input don't give very large changes in output. Hence it is a well conditioned problem. Therefore, condition number is not an issue.

Therefore, Algorithm has to be blamed here

We say that an algorithm is stable if it gives almost correct answer to a well conditioned problem. As this gives a completely different answer, we say that this algorithm is unstable.(even if it doesn't encounter zero in the diagonal)

How to overcome these issues? Let us think this way. For the first issue, if a zero diagonal entry is encountered, let us try swapping row with a row which has a non-zero element in that column. Now the question arises which row? We tend to swap with the row which has largest absolute value. Instead of swapping only when zero is encountered, we do it in every step.

Let us consider the example problem, we were discussing earlier.
\begin{equation}
\begin{bmatrix}
2^{-60}& 1\\
1& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
1\\2
\end{bmatrix}
\end{equation}

In first column, the maximum value is 1. Thus, we swap 1st and 2nd rows.
\begin{equation}
\begin{bmatrix}
1& 1\\
2^{-60}& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
2\\1
\end{bmatrix}
\end{equation}
We do a row operation \(R_2\gets R_2-2^{-60}R_1\) to get:
\begin{equation}
\begin{bmatrix}
1& 1\\
0& 1-2^{-60}
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
2\\1-2^{-59}
\end{bmatrix}
\end{equation}

\(1-2^{-60}\) and \(1-2^{-59}\) is represented on the 64 bit machine as \(1\). Therefore, the machine represents the equations as:
\begin{equation}
\begin{bmatrix}
1& 1\\
0& 1
\end{bmatrix}
\begin{bmatrix}
x_1\\x_2
\end{bmatrix}
= 
\begin{bmatrix}
2\\1
\end{bmatrix}
\end{equation}

The solution to these system of equations give \(x_2=1\) and \(x_1=1\).

This method in which we swap rows is called \textbf{partial pivoting}. Partial pivoting is seen to be stable in practice.

Eg: Solve the system using partial pivoting
\begin{align}
x_1+2x_2+3x_3 &=6\\
x_1+2x_2+4x_3 &=7\\
x_1-2x_2+x_3  &= 0
\end{align}

A much more better way is including a column swap as well. The process is decribed in the below example. This process is called \textbf{Complete Pivoting} which is stable but has more computation cost(due to multiple swaps/ comparisions)

Eg: Solve the system using complete pivoting.
\begin{align}
x_1+2x_2+3x_3 &= 6\\
x_1+2x_2+4x_3 &= 7\\
x_1-2x_2+10x_3&= -11
\end{align}

\textbf{ADD SOLUTION}

Order of computational cost: Complete Pivoting \textgreater{} Partial Pivoting \textgreater{} No Pivoting

Order of Stability: Complete Pivoting \textgreater{} Partial Pivoting \textgreater{} No Pivoting

\hypertarget{solving-overdetermined-systems}{%
\section{Solving Overdetermined Systems}\label{solving-overdetermined-systems}}

\hypertarget{solving-underdetermined-systems}{%
\section{Solving Underdetermined Systems}\label{solving-underdetermined-systems}}

\hypertarget{iterative-methods-for-solving-linear-systems}{%
\section{Iterative Methods for solving Linear Systems}\label{iterative-methods-for-solving-linear-systems}}

Previously, we have seen ``direct methods'' to solve linear systems like Gauss-Jordan elimination, Partial and complete pivoting. They can be solved exactly with \(\infty\) precision machines. But the computational cost is high.

For lesser computational cost, we use iterative methods. But the issue is that we can not solve exactly using these methods always.

\hypertarget{interpolation}{%
\chapter{Interpolation}\label{interpolation}}

\hypertarget{motivation---interpolation-vs.-approximation}{%
\section{Motivation - Interpolation vs.~Approximation}\label{motivation---interpolation-vs.-approximation}}

If the exact form of \(f(x)\) is known, then we have full information about \(f(x)\) i.e., Derivatives etc., But what if the exact form is not known?

Given points \(\{x_i\}_{i=1}^n\) and functional values at those points \(f(x_1),f(x_2), \dots, f(x_n)\), we wish to find an Approximation to \(f(x)\). One way to approximate a function is by \emph{interpolating} it.

\begin{quote}
We say that \(p(x)\) is an interpolant to \(f(x)\) at \(\{x_i\}_{i=1}^n\) if \(p(x_i)= f(x_i)\) for \(1\le i\le n\).
\end{quote}

Eg: A step function \(p(x) = f(x_i)\) for \(x\in \left[ \frac{x_i+x_{i-1}}{2}, \frac{x_i+x_{i+1}}{2} \right]\).

The step function, though it is an interpolant, we don't prefer it. The main issue is that it is not continuous. We prefer in some practical applications for the interpolant to be differentiable.

Note that not all approximations are interpolants.

Eg: A polynomial approximation to \(\sin x\) is a truncated Taylor series after a few terms(say till degree \(2n+1\)). This approximation is not an interpolant as it won't intersect \(\sin x\) at \(2n+2\) points.

Assume \(f(x)\) to be continuous. Consider the sequence of interpolants \(\{P_n(x)\}_{n=1}^{\infty}\) converging to \(f(x)\) on \([a,b]\) such that \(P_n(x)=f(x) \ \  \ \forall x\in \{x_1,x_2,\dots,x_n \}\)

\begin{equation}
\label{eq:integralfinterp}
\int_a^b f(x) \, dx \approx \int_a^b P_n(x) \, dx 
\end{equation}

\begin{equation}
\label{eq:dfdxinterp}
\frac{df}{dx}(x) \approx \frac{dP_n}{dx}(x)
\end{equation}

Equation\eqref{eq:integralfinterp} holds true if \(\{P_n(x)\}_{n=1}^{\infty}\) converges to \(f(x)\) \emph{uniformly}.

Equation\eqref{eq:dfdxinterp} holds true if \(P_n'(x)\) exists and \(\{P'_n(x)\}_{n=1}^{\infty}\) converges to \(f'(x)\) \emph{uniformly}.

In this course, we consider the interpolants \(p(x) \in C^{\infty}([a,b])\). A simplest such interpolant would be a polynomial.

\hypertarget{lagrange-interpolation}{%
\section{Lagrange Interpolation}\label{lagrange-interpolation}}

\hypertarget{motivation}{%
\subsection{Motivation}\label{motivation}}

Consider \(p(x) = a_0+a_1x+a_2x^2+\dots+ a_mx^m\) to be a polynomial interpolant for \(f(x)\) with \textbf{node points} as \(\{x_i\}_{i=1}^n\). Therefore,
\[p(x_i)=f(x_i) \implies a_0+a_1x_i+a_2x_i^2+\dots+ a_mx_i^m = f(x_i) \ \ \ \forall i \in \{1,2,\cdots, n\}\].

\begin{equation}
\implies \begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m\\
1 & x_2 & x_2^2 & \cdots & x_2^m\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m\\
\end{bmatrix}_{n \times(m+1)} \begin{bmatrix}a_0\\a_1\\ \vdots \\a_m \end{bmatrix}_{(m+1) \times 1}  = \begin{bmatrix} f(x_1)\\f(x_2)\\ \vdots \\f(x_n) \end{bmatrix}_{n\times 1}
\end{equation}

Let \[X = \begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m\\
1 & x_2 & x_2^2 & \cdots & x_2^m\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m\\
\end{bmatrix}\text{ ,} \ \  \ \bar{a} = \begin{bmatrix}a_0\\a_1\\ \vdots \\a_m \end{bmatrix}  \text{ and } \ \ \ \bar{f}=  \begin{bmatrix} f(x_1)\\f(x_2)\\ \vdots \\f(x_n) \end{bmatrix}\]
\[\implies X\bar{a} = \bar{f}\]

Now the \(\bar{a}\) has the coefficients of the interpolant. To find the coefficients, we need to solve the linear system.

If \(m+1<n\) then \(X\) is a thin matrix. i.e., we have lesser number of variables than equations. Therefore, solution may not exist. i.e., we may not be able to find \(p(x)\).

If \(m+1>n\) then \(X\) is a fat matrix. Infinitely many polynomial interpolants exist.

If \(m+1=n\) then \(X\) is a square matrix. \(X\) is Vandermonde matrix. It can be shown that:
\[\det(X) = \prod_{1\le i<j\le n}(x_i-x_j)\]
If \(x_i's\) are distinct then \(\det(X)\neq 0 \implies X\) is invertible.

\(\implies X\bar{a} = \bar{f}\) has a unique solution for \(m+1=n\).

\(\implies p(x)\) interpolates \(f(x)\) uniquely if \(\deg(p(x)) = n-1\).

Thus, the minimum degree of the interpolant polynomial is \(n-1\).

\(p(x)\) interpolates \(f(x)\) uniquely if \(\deg(p(x))=n-1\). Thus, solving the equation \(X\bar{a} = \bar{f}\).
But there are issues in solving the linear system like this.

\begin{itemize}
\tightlist
\item
  Computational complexity in solving the linear system \(\mathcal{O}(n^3)\).
\item
  Condition number of \(X\) grows exponentially in \(n\). This is not preferred as this might cause large errors with small error in input(say due to roundoff errors etc.,)
\end{itemize}

To overcome these problems, we use Lagrange interpolation.

\hypertarget{lagrange-interpolant}{%
\subsection{Lagrange Interpolant}\label{lagrange-interpolant}}

Consider

\begin{equation}
g(x_i) = \begin{cases}
        1 & \text{if } i\neq j\\
        0 & \text{if } i=j
    \end{cases}
\end{equation}
for \(i,j \in \{1,2,\dots,n\}\). We are interested to find a polynomial interpolant for this. Let us consider the case of \(n=3\) points and \(j=2\). i.e., \(g(x_1)=g(x_3)=0\) and \(g(x_2)=1\) for simplicity. The least degree of the polynomial interpolant \(p(x)\) is \(n-1=2\). By intuition, we can say that \((x-x_1)(x-x_3)\) is a factor of interpolant \(p(x)\). As \(p(x_2)=g(x_2)=1\), we can say that the interpolant \(p(x)\) is:
\begin{equation}
p(x) = \frac{(x-x_1)(x-x_3)}{(x_2-x_1)(x_2-x_3)}
\end{equation}

For \(n\) points, we have the interpolant polynomial to \(g(x)\) as:
\begin{equation}
p(x) = l_j(x) = \prod_{\substack{i=0 \\ i\neq j}} \frac{x-x_i}{x_j-x_i}
\end{equation}

\(l_j(x)\) is a polynomial of degree \(n-1\) such that:
\[l_j(x_i) = \delta_{ij}\]
Now consider \(n\) node points \(\{f(x_i)\}_{i=1}^n\) and consider the polynomial \(p(x)\) defined as:
\begin{equation}
p(x) = \sum_{j=1}^n f(x_j) l_j(x)
\end{equation}

\(p(x)\) is a polynomial of degree atmost \(n-1\). Now,

\[p(x_i) = \sum_{j=1}^n f(x_j) l_j(x_i) = \sum_{j=1}^n f(x_j) \delta_{ij} = f(x_i)\]
This implies that \(p(x)\) is an interpolant. This is known as \emph{Lagrange Interpolant}.

\hypertarget{choice-of-nodes}{%
\section{Choice of Nodes}\label{choice-of-nodes}}

\hypertarget{motivation-1}{%
\subsection{Motivation}\label{motivation-1}}

Now after finding the interpolant, the next question to be asked is how accurate is the interpolant? Immediate obvious answer would be the numer of points chosen. But are there any other factors which determine the accuracy of the interpolant?

Example: Consider the function \(f(x) = \frac{1}{1+25x^2}\) and the interpolation nodes as uniform nodes from {[}-1,1{]}.

\textbf{INSERT CODE HERE}

We can see that the interpolant is not converging to \(f(x)\) uniformly. There are some \textbf{\emph{boundary effects}}

Thus, selection of node points is also important. The question to ask now is what set of nodes guarantees uniform convergence?

\hypertarget{fundamental-theorem-of-polynomial-interpolation}{%
\subsection{Fundamental Theorem of Polynomial Interpolation}\label{fundamental-theorem-of-polynomial-interpolation}}

Let \(f(x)\) be a smooth function on {[}-1,1{]}. Let \(P_n(x)\) be a polynomial interpolant to \(f(x)\) at \(\{x_k\}_{k=0}^n\) with atmost degree \(n\). Then \(\exists \, \zeta\in[-1,1]\) such that:
\begin{equation}
e(x) = f(x)-P_n(x) = \frac{f^{(n+1)}(\zeta)}{(n+1)!}\prod_{k=0}^n(x-x_k)
\end{equation}

\textbf{Proof:-}
Define \[w(x) := \prod_{k=0}^n(x-x_k)\] and
\begin{equation}
g_x(t) : = f(t)-P_n(t)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(t)
\end{equation}
where the subscript \(x\) denotes that \(x\) is fixed.

\[g_x(x) = f(x)-P_n(x)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(x) = 0\]

\[g_x(x_j) = f(x_j)-P_n(x_j)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(x_j)\]
As \(P_n(x)\) is an interpolant to \(f(x)\) at nodes \(\{x_i\}_{i=0}^n\), \(P_n(x_j) = f(x_j)\) and by definition \(w(x_j) =0\). Therefore, for \(j\in\{0,1,2,\dots ,n\}\), we have \(g_x(x_j)=0\).

\(g_x(t)\) is smooth in the interval (-1,1).

Define intervals
\begin{align}
I_1     &= [x_0,x_1]\\
I_2     &= [x_1,x_2]\\
        &\vdots\\
I_k     &= [x_{k-1},x_k]\\
I_{k+1} &= [x_k,x]\\
I_{k+2} &=[x,x_{k+1}]\\
I_{k+3} &= [x_{k+1},x_{k+2}]\\
        &\vdots\\
I_{n+1} &= [x_{n-1},x_n]
\end{align}
According to Rolle's theorem, \(g'_x(t)\) has atleast \(n+1\) zeros on \([-1,1]\).
Again according to Rolle's theorem, we can say that \(g''_x(t)\) has atleast \(n\) zeros in \([-1,1]\). If we keep on using Rolle's theorem for further \(n-1\) times, we can show that \(g_x^{(n+1)}(t)\) has atleast 1 zero on \([-1,1]\) i.e., \(\exists \text{ a } \zeta\in[-1,1]\) such that \(g_x^{(n+1)}(\zeta)=0\)

\[g^{(n+1)}_x(\zeta) = f^{(n+1)}(\zeta)-P^{(n+1)}_n(\zeta)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w^{(n+1)}(\zeta)\]

As \(P_n(t)\) is an \$n\^{}th degree polynomial, \(P^{(n+1)}_n(\zeta)=0\).

\[w(t) = \prod_{k=0}^n(t-x_k)\implies w^{(n+1)}(t) = (n+1)!\]

Therefore \[0 = f^{(n+1)}(\zeta)-0 - \left(\frac{f(x)-P_n(x)}{w(x)}\right)(n+1)!\]
\[\implies e(x) = f(x)-P_n(x) = \frac{f^{(n+1)}(\zeta)}{(n+1)!}w(x)= \frac{f^{(n+1)}(\zeta)}{(n+1)!}\prod_{k=0}^n(x-x_k)\]

\begin{quote}
What if \(x\in[a,b]\) instead of \(x\in[-1,1]\)?
\end{quote}

We can use a linear mapping from \([a,b]\) to \([-1,1]\).

\hypertarget{different-possible-types-of-nodes}{%
\subsection{Different Possible types of nodes}\label{different-possible-types-of-nodes}}

Now the goal is to find the nodes which minimise the maximum absolute interpolation error i.e., \[\min \max_{x\in[-1,1]} |e(x)|\]. In general, we can also try finding nodes which minimise \(p\)-norm of the interpolation error i.e., \(\min \lVert e(x) \rVert_p\) where:
\[\lVert e(x) \rVert_p = \left( \int_{-1}^1 |e(x)|^p \, dx\right)^{\frac{1}{p}}\] and \[\lim_{p\to\infty} \lVert e(x) \rVert_p = \max_{x\in[-1,1]}|e(x)|\]

Now the issue is it is difficult to know about \(f^{(n+1)}(\zeta)\) as \(f(x)\) is not known always. Now the best thing we can do is to find nodes which minimise \[\min \max_{x\in[-1,1]}\left|\prod_{k=0}^n(x-x_k)\right| \text{ or } \min \lVert \prod_{k=0}^n(x-x_k) \rVert_p\].

It turns out that Legendre nodes minimise \(||w(x)||_2\), Chebyshev nodes of first kind minimise \(||w(x)||_{\infty}\) and Chebyshev nodes of second kind minimises \(||w(x)||_1\).

\hypertarget{legendre-nodes}{%
\subsubsection{Legendre Nodes}\label{legendre-nodes}}

Monic Legendre polynomials are defined as:
\[q_0(x) = 1, \, \, q_1(x) = x\]
\(q_n(x)\) is a monic polynomial of degree \(n\) such that:
\begin{equation}
\int_{-1}^1 q_n(x) q_m(x) \, dx = 0 \ \  \ \forall \,  m\neq n
\end{equation}

First few monic Legendre polynomials are:
\begin{align*}
  q_0(x) &= 1\\
  q_1(x) &= x\\
  q_2(x) &= x^2-\frac{1}{3}\\
  q_3(x) &= x^3-\frac{3}{5}x\\
  q_4(x) &= x^4- \frac{6}{7}x^2+\frac{3}{35}
\end{align*}

The zeros of these Legendre polynomials are called Legendre nodes. Legendre nodes minimise \(||w(x)||_2\).

\hypertarget{chebyshev-nodes-of-first-kind}{%
\subsubsection{Chebyshev nodes of first kind}\label{chebyshev-nodes-of-first-kind}}

Chebyshev polynomials of first kind are given by \(T_n(x) = \cos(n\cos^{-1}(x))\).

First few Chebyshev polynomials are:
\begin{align}
T_0(x) &= 1\\
T_1(x) &= x\\
T_2(x) &= 2x^2-1\\
T_3(x) &= 4x^3-3x\\
T_4(x) &= 8x^4-8x^2+1
\end{align}

An interesting property of these polynomials are \(T_m(x)\) and \(T_n(x)\) are orthogonal weighted by \(\frac{1}{\sqrt{1-x^2}}\).

\begin{align}
   \int_{-1}^{1}\frac{ T_n(x)T_m(x)}{\sqrt{1-x^2}}dx &= 0 , m \neq n\\
                                    &= \pi,m = n= 0\\
                                    &= \frac{\pi}{2}, m = n \neq 0               
\end{align}

\textbf{Proof:-}

Let \(x = \cos \theta\) where \(\theta \in [0,\pi]\). This implies that \(dx = -\sin \theta \, d \theta = -\sqrt{1-x^2} \, d \theta\). And \(x=-1 \implies \theta = \pi\) and \(x =1 \implies \theta = 0\). Therefore,
\begin{align*}
\int_{-1}^{1} T_m(x)T_n(x) \frac{1}{\sqrt{1-x^2}} dx & = \int_{0}^{\pi} \cos(m\theta) \cos(n\theta) d\theta \\
&= \frac{1}{2}{\int_{0}^{\pi}} \cos(m+n)\theta 
+ \cos(m-n)\theta d\theta  \\
&=\frac{1}{2} \left[\sin\frac{(m+n)\theta }{m+n}\right]_{0}^{\pi}
+ \left[\sin\frac{(m-n)\theta }{m-n}\right]_{0}^{\pi}\\
&= 0, \text{ if } m \neq n 
\end{align*}

Similarly with appropriate substitution, conditions for \$m = n=0 \$ and \$m = n \neq 0 \$ can be proved.

The zeros of Chebyshev polynomial \(T_{n+1}(x)\) are given by \(x_k = \cos\left( \frac{2k+1}{2n+2} \pi \right)\) where \(k \in \{ 0,1,2,\cdots,n \}\).

\textbf{Proof:-}

\begin{flushleft}
\textbf{Proof:-}
\end{flushleft}

We have \[T_{n+1}(x) = \cos((n+1)\arccos(x))\]
\[T_{n+1}(x) = 0 \implies \cos((n+1)\arccos(x)) = 0\]

\[\implies(n+1) \arccos(x) = (2k+1)\frac{\pi}{2} \ \ \ k \in \mathbb{Z}\]

\[\implies \arccos(x) = (2k+1)\frac{\pi}{2(n+1)} \ \ \ \ k \in \mathbb{Z}\]

But as the principle range of \(\arccos(x)\) is defined as \([0,\pi]\), we must restrict the values \(k\) can take.

\[0 \le (2k+1)\frac{\pi}{2(n+1)} \le \pi \implies 0 \le k \le n+\frac{1}{2} \]

But as \(k \in \mathbb{Z}\), we can say that the possible values \(k\) can take are \(\{ 0,1,2,\cdots,n\}\).

Therefore \(\arccos(x_k) = (2k+1)\frac{\pi}{2(n+1)} \ \ \ \ k \in \{ 0,1,2,\cdots,n\}\)
\[\implies x_k = \cos\left( (2k+1)\frac{\pi}{2(n+1)} \right)\ \ \ \ k \in \{ 0,1,2,\cdots,n\}\]

Therefore the zeros of \(T_{n+1}(x)\) are in the interval \([-1,1]\) are given by \(x_k = \cos\left( \frac{2k+1}{2n+2} \pi \right)\) where \(k \in \{ 0,1,2,\cdots,n \}\). The number of zeros is also consistent with the fact that as \(T_{n+1}(x)\) is an \((n+1)\) th-degree polynomial, it has \((n+1)\) roots. Also, these zeros are distinct.

These zeros are called the Chebyshev nodes of first kind. They minimise \(||w(x)||_{\infty}\)

\textbf{Theorem}

If \(f(x)\) is smooth on \([-1,1]\), then Lagrange interpolation using the roots of Chebyshev nodes of first kind converges uniformly.

\textbf{INSERT RUNGE FUNCTION-CHEBYSHEV NODES CONVERGENCE CODE}

\hypertarget{wierstrass-approximation-theorem}{%
\section{Wierstrass Approximation theorem}\label{wierstrass-approximation-theorem}}

\emph{Suppose \(f\) is a continuous real-valued function defined on the real interval \([a,b]\). For every \(\epsilon > 0\), there exists a polynomial \(p\) such that for all \(x\) in \([a,b]\), we have \(|f(x) - p(x)| < \epsilon\).}

\emph{\textbf{Bernstein polynomial:}}

The Bernstein basis polynomials of degree \(n\) are defined as
\begin{equation}
    b_{k,n}(x) = {}^nC_{k}\, x^k (1-x)^{n-k}
\end{equation}
A linear combination of these basis polynomials can be used to obtain other polynomials. One of the properties of these polynomials is that
\begin{equation}
\begin{aligned}
    \sum_{k=0}^n {b_{k,n}(x)} &= \sum_{k=0}^n {{}^nC_{k}\, x^k (1-x)^{n-k}} \\
    &= (x + (1 - x))^n = 1
    \label{equ:property}
\end{aligned}
\end{equation}
So these polynomials can also be considered to act as some kind of weights. Now, for approximating functions, the Bernstein Polynomial is defined as
\begin{equation}
    B_n(f;x) = \sum_{k=0}^n {f\left(\dfrac{k}{n}\right) {}^nC_{k}\, x^k (1-x)^{n-k}}
    \label{eqn:bern}
\end{equation}
Here, \(f\) is the function being approximated, \(n\) is the order of approximation \& \(x\) is the point at which the approximation is made.

\textbf{Proof:}

To prove the theorem on closed intervals \([a,b]\), without loss of generality, we can take the closed interval as \([0, 1]\). Thus, \(f\) can be considered as a continuous real-valued function on \([0, 1]\). Since \(f\) is a continuous function, we can say that for a given \(\epsilon > 0\), there exists a \(\delta > 0\) such that:
\begin{equation}
    |x-y| < \delta \implies |f(x) - f(y)| <\frac{\epsilon}{2} \ \ \ \  \forall \, x,y \in [0,1]
    \label{equ:convcond}
\end{equation}
To prove that \(B_n(f,x)\) converges to \(f(x)\) uniformly, we show that \(|B_n(f,x) - f(x)|\) has to be made small. By the definition of Bernstein's polynomial, as shown in Eqn. \ref{eqn:bern} and by using the property given in Eqn. \ref{equ:property}, we can write \(|B_n(f;x) - f(x)|\) as:
\begin{align*}
    |B_n(f;x) - f(x)| &= \left| \sum_{k=0}^n {\left(f\left(\dfrac{k}{n}\right) - f(x)\right) {}^nC_{k} \, x^k (1-x)^{n-k}} \right| \\
    &\leq \sum_{k=0}^n {\left|f\left(\dfrac{k}{n}\right) - f(x)\right| {}^nC_{k} \, x^k (1-x)^{n-k}}
\end{align*}
Now, if we consider \(y=\dfrac{k}{n}\) in Eqn. \ref{equ:convcond} and if \(\left|\dfrac{k}{n} - x\right| < \delta\), we can say that \(\left|f\left(\dfrac{k}{n}\right) - f(x)\right| < \dfrac{\epsilon}{2}\). But this is not true in the entire domain. So, we partition the domain in to two sets: \(A\) and \(B\), where \(A\) and \(B\) have the following properties:
\begin{equation}
    \begin{aligned}
        A \cup B &= [0,1]\\
        A \cap B &= \phi\\
        A &= \{x:|k/n - x| \leq \delta, x \in [0,1]\}\\
        B &= \{x:|k/n-x| > \delta, x \in [0,1]\}
    \end{aligned}
    \label{equ:sets}
\end{equation}
By dividing the domain into sets \(A\) and \(B\) as defined in \ref{equ:sets}, we can write:
\begin{equation*}
\begin{split}
    |B_n(f;x) - f(x)| = \sum_{x\in A} \left|f\left(\dfrac{k}{n}\right) - f(x)\right| &{}^nC_{k}\, x^k (1-x)^{n-k} \\
    &+ \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{split}
\end{equation*}
From Eqns. \ref{equ:convcond} and \ref{equ:sets}, we can say that \(\left|f\left(\dfrac{k}{n}\right) - f(x)\right|\) has an upper bound of \(\dfrac{\epsilon}{2}\) on the set \(A\). Therefore, we can write:
\begin{equation*}
\begin{split}
    |B_n(f;x) - f(x)| \leq \sum_{x\in A} \left(\dfrac{\epsilon}{2}\right){}^nC_{k}\, x^k &(1-x)^{n-k} \\
    &+ \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{split}
\end{equation*}
By using the property described in Eqn. \ref{equ:property}, we can simplify the above inequality as:
\begin{equation*}
    |B_n(f,;x) - f(x)| \leq \frac{\epsilon}{2} + \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{equation*}
Since \(f\) is uniformly converging in \([0,1]\), \(f\) is bounded from above. So, let the maximum value of \(f\) in the domain be \(M\). Thus:
\begin{equation*}
    \text{max\{} | f(x) | \} = M \ \ \ \  \forall \, x \in [0,1]
\end{equation*}
Thus, the maximum value \(\left|f\left(\dfrac{k}{n}\right) - f(x)\right|\) can achieve in the domain is \(2M\) (considering the case where, one of them is \(M\) and the other is \(-M\)). Thus, we have:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + (2M) \sum_{x\in B} {{}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
Now, in set \(B\), \(\left| \dfrac{k}{n} - x \right| > \delta\). Thus, we get:
\begin{equation*}
    \frac{\left( k/n - x \right)^2}{\delta^2} > 1
\end{equation*}
Multiplying this to the second term of RHS, we get:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + (2M) \sum_{x\in B} {\frac{\left( k/n - x \right)^2}{\delta^2}{}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
The second term can now be written as:
\begin{equation*}
    \dfrac{2M}{\delta^2 n^2} \sum_{x\in B} {(k-nx)^2 \, {}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
The summation term is equivalent to computing the variance of a binomial distribution with parameters \(n\) \& \(x\). The variance is given by \(nx(1-x)\). Thus, we get:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{2M}{\delta^2 n^2} nx(1-x)
\end{equation*}
We know that using AM \(\geq\) GM:
\begin{equation*}
    x(1-x) \leq \dfrac{1}{4}
\end{equation*}
Thus, we have:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{M}{2\delta^2 n}
\end{equation*}
Now, let us define a quantity \(N\) such that the above condition holds true for all \(n > N\), which gives us \(\dfrac{1}{n} < \dfrac{1}{N}\). Thus:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{M}{2\delta^2 N}
\end{equation*}
If we choose the \(N\) such that:
\begin{equation*}
    \dfrac{M}{2\delta^2N} = \dfrac{\epsilon}{2}
\end{equation*}
giving us:
\begin{align*}
    |B_n(f;x) - f(x)| &\leq \dfrac{\epsilon}{2} + \dfrac{\epsilon}{2} \\
    |B_n(f;x) - f(x)| &\leq \epsilon
\end{align*}
Thus, we have for all \(n > N\), where \(N = \dfrac{M}{\delta^2\epsilon}\), we have
\begin{equation}
    |B_n(f;x) - f(x)| \leq \epsilon
\end{equation}
i.e., the Bernstein Polynomial \(B_n(f;x)\) uniformly converges to \(f(x)\) for all \(x\) in the domain \([0,1]\).

\textbf{NOTE:-}
1. A sequence \(\{a_n\}_{n \ge 0}\) converges to \(a\) \textbf{algebraically} if \(\exists N>0\) such that \(\forall \, n>N\)
\[|a_n-a|<\frac{k}{n^{\alpha}} \text{ for some } k, \,\alpha >0 \]

Examples:- a. \(a_n = 1+\frac{1}{n^2}\) converges to 1 algebaically as \[|a_n-1| = \frac{1}{n^2}<\frac{10}{n^2}\]

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(a_n = e^{-n}\) converges to 0 algebraically as \[|a_n| =e^{-n} <\frac{1}{n} \ \forall n\ge 1\]
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  A sequence \(\{a_n\}_{n \ge 0}\) converges to \(a\) \textbf{geometrically} if \(\exists N>0\) such that \(\forall \, n>N\)
  \[|a_n-a|<kc^n \text{ for some } k>0 , |c|<1 \]
\item
  Given any \(c\) and \(\alpha\) (such that \(|c|<1\) and \(\alpha>0\)), there exists \(N = N(c,\alpha)>0\) such that \(\forall n>N\)
\end{enumerate}

\[c^n<\frac{1}{n^{\alpha}}\]
This implies that all geometrically convergent series are algebraically convergent.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Error in Bernstein polynomial \(B_n(x)\) approximation goes down as \(\frac{1}{n}\). This means that \(B_n(x)\) uniformly converges to \(f(x)\) algebraically at the rate \(\frac{1}{n}\).
\item
  If \(f(x)\) is smooth then the Chebyshev interpolant is Geometrically convergent to \(f\). This convergence is also known as Spectral Convergence.
\end{enumerate}

Consider the function \(f(x) = \frac{1}{1+25x^2}\). Let us consider the analytic continuation of \(f(x)\) on a compact set \(\Omega \in \mathbb{C}^2\) such that \([-1,1] \subset \Omega\) which is \(\frac{1}{1+25z^2}\). This is analytic on any compact set \(\Omega\) which doesn't contain \(\pm i/5\).

\hypertarget{quadratures}{%
\chapter{Quadratures}\label{quadratures}}

In this chapter, we see how to find a definite integral numerically.
\#\# Motivation
Consider a continuous function \(f(x)\) in an interval \([a,b]\). We are interested in finding the integral \(\int_a^b f(x)\, dx\).

If we don't know \(f(x)\) at all points, then how to find integral? Based on what we have studied till now, one way we can think of to approximate the integral is to first find a polynomial interpolant \(p(x)\) using the known points and then find the integral \(\int_a^b p(x) \, dx\). We can say that
\begin{equation}
\int_a^b f(x)\, dx \approx \int_a^b p(x) \, dx
\end{equation}

But we have seen that there are issues like non-uniform convergence based in node distribution in the previous chapter. Therefore, we avoid this method. We resort to other methods to find the definite integral numerically.

Now, even if we know the exact form of the function at all points in the interval \((a,b)\), we can not always find a closed form for the anti-derivative of the function.

\url{Example:-} Consider \(f(x) = \exp(-x^2)\). We can't find the exact form of \(\int f(x)\, dx = \int e^{-x^2}\, dx\). We can find \(\int_{-\infty}^{\infty} e^{-x^2}\, dx\) using multivariable calculus methods say exactly. But for some random finite interval \((a,b)\), it is impossible to find the exact value. To get the approximate value, we use numerical methods.

In some cases, though finding exact value of the integral is possible, it is extremely difficult to find it.

\hypertarget{different-schemes}{%
\section{Different Schemes}\label{different-schemes}}

\hypertarget{basic-schemes}{%
\subsection{Basic Schemes}\label{basic-schemes}}

\textbf{ATTACH FIGURES FOR EACH SCHEME}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Lower/Left Riemann Sum:
\end{enumerate}

\begin{equation}
\int_a^b f(x) \, dx \approx \sum_{i=0}^{n-1} f(x_i) \left( \frac{b-a}{n} \right)
\end{equation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Upper/Right Riemann Sum:
\end{enumerate}

\begin{equation}
\int_a^b f(x) \, dx \approx \sum_{i=1}^{n} f(x_i) \left( \frac{b-a}{n} \right)
\end{equation}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Trapezoidal Rule:
  Instead of rectangles, let us approximate the areas by first dividing into trapeziums and summing up areas.
\end{enumerate}

\begin{equation}
\int_a^b f(x) \, dx \approx \frac{1}{2} h \left[ \sum_{i=1}^n (f(x_{i-1})+f(x_i)) \right]
\end{equation}

We can see that the trapezoidal rule is an average of Left and Right Riemann Sums.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Midpoint/Rectangular rule
\end{enumerate}

\begin{equation}
\int_a^b f(x) \, dx \approx h \sum_{i=0}^{n-1} f\left( \frac{x_i+x_{i+1}}{2} \right) 
\end{equation}

\hypertarget{accuracy-of-these-schemes}{%
\subsection{Accuracy of these schemes}\label{accuracy-of-these-schemes}}

Now the question arises - How accurate are these approximations?

\hypertarget{simpsons-rule}{%
\subsection{Simpson's Rule}\label{simpsons-rule}}

\hypertarget{trapezoidal-rule-with-end-point-corrections}{%
\subsection{Trapezoidal Rule with End point Corrections}\label{trapezoidal-rule-with-end-point-corrections}}

\hypertarget{euler-maclaurian-formula}{%
\section{Euler-Maclaurian Formula}\label{euler-maclaurian-formula}}

\hypertarget{introduction-1}{%
\subsection{Introduction}\label{introduction-1}}

\hypertarget{formula-and-derivation}{%
\subsection{Formula and Derivation}\label{formula-and-derivation}}

\hypertarget{higher-order-quadratures}{%
\subsection{Higher Order Quadratures}\label{higher-order-quadratures}}

\hypertarget{gaussian-quadratures}{%
\section{Gaussian Quadratures}\label{gaussian-quadratures}}

\hypertarget{root-finding-algorithms}{%
\chapter{Root Finding Algorithms}\label{root-finding-algorithms}}

\hypertarget{motivation-2}{%
\section{Motivation}\label{motivation-2}}

Let \(f(x)\) be a continuous function. We are interested in solving \(f(x)=0\) for \(x\),i.e., we are interested in finding \(x^*\) such that \(f(x^*)=0\). But finding \(x^*\) analytically is not always easy.

Examples:-

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Solving for \(x\) for \(3x=9\), \(x^2+2x-1=0\) are easy. We have closed form zeros for \(x\) till 4th order polynomial. Beyond 4th order closed form solution is NOT possible.
\item
  Solving transcendental equations like \(\tan x = x\), \(x=e^x\) exactly.
\end{enumerate}

Considering the issues present, we need to think of solving the equations numerically.

Consider \(f(x) = xe^x-5\). We are now interested to find \(x^*\) such that \(f(x^*)=0\). Let us guess say \(x^*=1.2\). \(f(1.2) = -1.015859692716143\). We need to refine this further! For this we need to find a sequence of \(x_k\)'s iteratively, such that the sequence \(\{x_k\}\) converges to \(x^*\).

\hypertarget{bisection-method}{%
\section{Bisection Method}\label{bisection-method}}

We know from Intermediate Value theorem that if \(f(x)\) is continuous on \([a,b]\) and if \(f(a) f(b)<0\), \(\exists x^* \, \in(a,b)\) such that \(f(x^*)=0\).

Given a continuous function \(f(x)\) and if we could find \(a,b\in \mathbb{R}\) such that \(a<b\) and \(f(a) f(b)<0\), then we can find \(x^*\) as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Choose \(x_0 = \frac{a+b}{2}\) and if

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    \(f(x_0) f(a)<0 \implies \ \exists \text{ a } x^*\, \in (a,x_0)\).
  \item
    \(f(x_0) f(a)>0 \implies \ \exists \text{ a } x^*\, \in (x_0,b)\)
  \item
    \(f(x_0) =0 \implies x^* = x_0\)
  \end{enumerate}
\item
  Refine the guess by taking mean of new interval and check for convergence.
\item
  The stopping criteria are \(|f(x)|<\epsilon\) and \(|I_k|<\delta\) where

  \begin{itemize}
  \item
    \(|I_k|\) is the length of the interval \(I_k\) at the \(k^{th}\) step. This is found as:

    Interval at \(0^{th}\) step is \(I_0 = [a,b]\). Hence the length of interval \(I_0\) is \(|I_0|= b-a\).

    Let the interval at \(k^{th}\) step is \(I_k\). Then from step 1,2 we can say that \(|I_k| = \frac{|I_{k-1}|}{2}\). Therefore: \[|I_k| = \frac{b-a}{2^k}\]
  \item
    \(\epsilon\) and \(\delta\) are user specified small tolerences.(eg 1e-10).
  \end{itemize}
\end{enumerate}

\textbf{INSERT PICTURE HERE}

\textbf{INSERT CODE/PSEUDO-CODE HERE}

This method described above is called as Bisection method.

When does Bisection Method fail?

In case of Bisection Method, if 2 initial points \(a,b \in \mathbb{R}\) are chosen such that \(f(a)f(b)<0\), the we can guarantee that Root can be found from the Intermediate value theorem. Thus, the sequence of iterates always converge to the root.

\hypertarget{newton-method}{%
\section{Newton Method}\label{newton-method}}

In Bisection method, we need to find functional value at 2 points initially where we expect the root to lie between them. Is it possible to find the root by taking just 1 initial guess? Newton's method allows us to do this! But we need to know the value of derivative at that point.

\textbf{INSERT PICTURE HERE}

Pick guess value \(x_0\). We improve this guess by finding the unique root of the linear approximation at this point. The linear approximation is the tangent at that point. The equation of tangent to \(f(x)\) at \(x=x_0\) is:
\[y-f(x_0) = f'(x_0) (x-x_0)\]
This intersects \(x\) axis at say \((x_1,0)\). Therefore,
\[0-f(x_0) =  f'(x_0) (x_1-x_0)\]
If \(f'(x_0)\neq 0\), then \(x_1\) exists. Therefore, the improved guess is
\begin{equation}
x_1 = x_0 - \frac{f(x_0)}{f'(x_0)}
\end{equation}

On repeating the process, we get the improved guess iterates as:
\begin{equation}
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)}
\end{equation}

\textbf{INSERT CODE/PSEUDOCODE HERE}

\hypertarget{rate-of-convergence}{%
\subsection{Rate of Convergence}\label{rate-of-convergence}}

Assume that the iterate sequence \(\{x_k\}\to x^*\) where \(f(x^*)=0\).
Define \(e_k = x_k-x^* \implies x^*=x_k-e_k\).

We assume that \(f(x)\) is a continuous function which is twice differentiable with \(f''(x)\) being continuous. Writing Taylor series of \(f(x^*)=0\) about \(x_k\) gives:
\[f(x^*)=0= f(x_k-e_k) = f(x_k) - e_k f'(x_k)+\frac{e_k^2}{2!} f''(\zeta_k) \]
where \(\zeta_k \in (\min(x_k,x^*),\max(x_k,x^*))\)
\[\implies 0=f(x_k)-(x_k-x^*)f'(x_k)+\frac{e_k^2}{2!} f''(\zeta_k)\]
\[\implies 0 = f(x_k) - (x_k-x_{k+1}+x_{k+1}-x^*)f'(x_k) + \frac{e_k^2}{2!} f''(\zeta_k)\]

\[\implies 0 = f(x_k) - (x_k-x_{k+1}) f'(x_k) - e_{k+1} f'(x_k) +\frac{e_k^2}{2!} f''(\zeta_k)\]
From equation()
\[\implies 0 =f(x_k) - \frac{f(x_k)}{f'(x_k)}f'(x_k) - e_{k+1} f'(x_k) +\frac{e_k^2}{2!} f''(\zeta_k)\]
\[ e_{k+1} = \frac{e_k^2}{2!} \frac{f''(\zeta_k)}{f'(x_k)}\]
As \(\zeta_k \in (\min(x_k,x^*),\max(x_k,x^*))\) and as \(f''(x)\) and \(f'(x)\) are continuous, \(\exists\, M,m\in \mathbb{R}\) such that \(f''(\zeta_k) \le M\) and \(f'(x_k) \ge m\). Therefore,
\[\frac{f''(\zeta_k)}{2 f'(x_k)} \le \frac{M}{2m} = C \ \text{(say)}\]
Therefore,
\begin{equation}
e_{k+1} \le C e_k^2
\end{equation}

This implies that the rate of converge is quadratic for Newton's method. For Bisection, the convergence rate is linear.

\textbf{RATE OF CONVERGENCE plot/code}

\hypertarget{possibility-of-non-convergence}{%
\subsection{Possibility of Non-Convergence?}\label{possibility-of-non-convergence}}

Now the main question comes! When does Newton Method fail?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When \(f'(x_k) = 0\) at any step in iteration. This means that the tangent at \(x=x_k\) is parallel to \(x\) axis and hence, no root can be found from there!
\end{enumerate}

Eg: \(f(x) = x^2-1\) and \(x_0 = 0\) This implies that \(f'(x_0) = 2x_0 = 0\). Thus we have to change initial guess.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  \(f(x) = x^{\frac{1}{3}}\) and \(x_0 = a>0\). Root is \(x^*=0\)
\end{enumerate}

\(f'(x) = \frac{1}{3x^{\frac{2}{3}}}\)

\[x_{k+1}= x_k - \frac{f(x_k)}{f'(x_k)} = x_k - \frac{x_k^{\frac{1}{3}}}{\frac{1}{3x_k^{\frac{2}{3}}}} = -2 x_k\]

Therefore, \(x_n = (-2)^n x_0 = (-2)^n a\).

\[\lim_{n \to \infty} x_n \text{ doesn't exist.}\]

Thus, convergence is not always guaranteed in Newton's Method.

\hypertarget{numerical-differentiation}{%
\chapter{Numerical Differentiation}\label{numerical-differentiation}}

\hypertarget{motivation-3}{%
\section{Motivation}\label{motivation-3}}

Given a continuous function \(f(x)\). We have seen that finding anti-derivative for all functions is not possible and hence, we resort to finding approximate values of definite integrals using Numerical methods(Quadratures). But derivatives can be found for any differentiable function \(f(x)\) using Chain rule etc.,

Eg:- Consider \(f(x) = \cos(x^2)\) which is continuous and differentiable.

\(\int_0^1 f(x) \, dx = \int_0^1 \cos(x^2)\, dx\) has no closed form. But \(f'(x) = -2x\sin(x^2)\) which can be found using chain rule.

Now the question which naturally arises is: Why do we need to think of finding derivatives numerically then?

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Consider the case where \(f(x)\) is known at discrete node points \(\{x_i\}_{i=0}^n\). To find derivatives at these nodes, one way is to interpolate using an interpolant polynomial \(p(x)\) and then find \(p'(x)\). We can find using a less computationally expensive way as will be discussed in this chapter.
\item
  Solving Differential Equations

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \textbf{Ordinary Differential Equations(ODEs):}
  \end{enumerate}

  \textbf{Simple Pendulum:-}

  \textbf{ATTACH SIMPLE PENDULUM PIC WITH ITS FBD}

  For simple pendulum with no damping and using small angle approximation about equilibrium position, we have the equation of motion as:
  \begin{equation}
   \frac{d^2\theta}{dt^2}+\frac{g}{l}\theta = 0
   \end{equation}
  with initial conditions \(\theta(t=0)= \theta_0\) and \(\frac{d\theta}{dt}(t=0) = 0\).

  The exact solution for this ODE is known which is \(\theta = \theta_0 \cos\left( \sqrt{g}{l} t \right)\).

  But if the assumptions of small angle oscillations and no damping are relaxed then the equation of motion becomes:
  \begin{equation}
   \frac{d^2\theta}{dt^2}+C\frac{d\theta}{dt}+\frac{g}{l}\sin \theta = 0
   \end{equation}
  with initial conditions remaining the same. But solving this ODE analytically is impossible. Therefore, we need access to numerical methods to solve ODEs.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \setcounter{enumii}{1}
  \tightlist
  \item
    \textbf{Partial Differential Equations(PDEs):-}
  \end{enumerate}

  Some examples are Wave Equation(Hyperbolic PDE)
  \[\partial{\partial^2 u}{\partial t^2}-c^2 \nabla^2 u= 0\]

  Heat/Diffusion Equation(Parabolic PDE)
  \[\partial{\partial u}{\partial t}-\alpha \nabla^2 u= 0\]

  Laplace Equation(Elliptic PDE)
  \[\nabla^2 u = 0 \text{ on } \Omega\]
  with boundary condition \(u = f(x,y,z)\) on \(\partial \Omega\)

  For specific boundaries like a rectangle, the solution can be found analytically. But for a general boundaries, it is extremely difficult to find analytical solutions. Therefore, numerical methods have to be used for solving these PDEs.
\end{enumerate}

In this course, we consider solving only ODEs i.e, 1 independent variable per dependent variable.

\hypertarget{finite-differences}{%
\section{Finite Differences}\label{finite-differences}}

To construct approximations for derivatives, we use Taylor series. Let \(f(t)\) be a real differentiable function whose values are known at \(\{t_i\}_{i=0}^n=t_0+i h\) where \(h = \Delta t\) which is a constant

Define \(f_i = f(t_i)\) and \(f'_i = \frac{df}{dt}(t_i)\)

We can write \(f_{j+1}\) in terms of \(f_j\) and derivatives of \(f\) at \(t_j\) by writing Taylor series of \(f(t_{j+1})\) about \(t=t_j\).

\[f_{j+1} = f(t_j+h)= f(t_j)+hf'(t_j)+\frac{h^2}{2!}f''(t_j)+\dots\]
\[\dfrac{f_{j+1}-f_j}{h}-\frac{h}{2!}f''(t_j)-\frac{h^2}{3!}f'''(t_j)-\dots = f'(t_j) = f'_j\]

\begin{equation}
f'_j = \dfrac{f_{j+1}-f_j}{h} + \mathcal{O}(h)
\end{equation}
The above approximation is called \textbf{Forward Finite Difference Approximation}

Now consider the Taylor series of \(f(t_{j-1})\) about \(t=t_j\).

\[f_{j-1} = f(t_j-h)= f(t_j)-hf'(t_j)+\frac{h^2}{2!}f''(t_j)+\dots\]
\[\dfrac{f_{j}-f_{j-1}}{h}+\frac{h}{2!}f''(t_j)-\frac{h^2}{3!}f'''(t_j)+\dots = f'(t_j) = f'_j\]

\begin{equation}
f'_j = \dfrac{f_{j}-f_{j-1}}{h} + \mathcal{O}(h)
\end{equation}
The above approximation is called \textbf{Backward Finite Difference Approximation}

We can clearly see that both Forward and Backward Finite differences are \textbf{1st order accurate.} Now the question to ask is ``Can we improve the order of accuracy?''

Consider the Taylor Series Expansions of \(f_{j+1}\) and \(f_{j-1}\) about \(t_j\).
\begin{align*}
f_{j+1} &= f(t_j)+hf'(t_j)+\frac{h^2}{2!}f''(t_j)+\frac{h^3}{3!}f'''(t_j)+\frac{h^4}{4!}f^{(4)}(t_j)+\dots\\
f_{j-1} &= f(t_j)-hf'(t_j)+\frac{h^2}{2!}f''(t_j)-\frac{h^3}{3!}f'''(t_j)+\frac{h^4}{4!}f^{(4)}(t_j)-\dots
\end{align*}
Subtracting () from (), we get
\[\implies f_{j+1}-f_{j-1} = 2hf'_j+\frac{2h^3}{3!}f'''(t_j)+\frac{2h^5}{5!}f^{(5)}(t_j)+\dots\]
\[\implies f_{j+1}-f_{j-1}-\frac{2h^3}{3!}f'''(t_j)-\frac{2h^5}{5!}f^{(5)}(t_j)-\dots = 2hf'_j\]

\begin{equation}
f'_j = \frac{f_{j+1}-f_{j-1}}{2h} + \mathcal{O}(h^2)
\end{equation}
We can see that the order of accuracy has been improved to 2. This approximation is called \textbf{Central Finite Difference Approximation.}

\textbf{ATTACH CODE/ACCURACY PLOT}

To improve the order of accuracy, we need to consider more ``grid points''. To understand this clearly, let us consider the example below.

\textbf{Example:} Construct Finite difference scheme to find \(f'_j\) from \(f_j\), \(f_{j\pm 1}\) and \(f_{j \pm 2}\).

\textbf{Solution:}

Let
\begin{equation}
f'_j = a f_{j+2}+b f_{j+1}+cf_{j}+d f_{j-1}+e f_{j-2}
\end{equation}
where \(a,b,c,d,e\) are to be found.

Write Taylor series of \(f_{j \pm 1}\) and \(f_{j\pm 1}\) about \(t_j\) on RHS of the above equation(), we get:
\begin{align*}
f'_j  &= a\left(f(t_j)+2hf'(t_j)+\frac{(2h)^2}{2!}f''(t_j)+\frac{(2h)^3}{3!}f'''(t_j)+\frac{(2h)^4}{4!}f^{(4)}(t_j)+\frac{(2h)^5}{5!}f^{(5)}(t_j)+\dots \right) \\
&+ b \left( f(t_j)+hf'(t_j)+\frac{h^2}{2!}f''(t_j)+\frac{h^3}{3!}f'''(t_j)+\frac{h^4}{4!}f^{(4)}(t_j)+\frac{h^5}{5!}f^{(5)}(t_j)+\dots \right)\\
&+ cf_j \\
&+ d \left(f(t_j)-hf'(t_j)+\frac{h^2}{2!}f''(t_j)-\frac{h^3}{3!}f'''(t_j)+\frac{h^4}{4!}f^{(4)}(t_j)-\frac{h^5}{5!}f^{(5)}(t_j)+\dots \right)\\
&+ e \left( f(t_j)-2hf'(t_j)+\frac{(2h)^2}{2!}f''(t_j)-\frac{(2h)^3}{3!}f'''(t_j)+\frac{(2h)^4}{4!}f^{(4)}(t_j)-\frac{(2h)^5}{5!}f^{(5)}(t_j)+\dots \right)
\end{align*}

\begin{align*}
\implies f'_j  &= (a+b+c+d+e) f_j + h(2a+b-d-2e)f'_j+ \frac{h^2}{2!}(4a+b+d+4e) f''_j \\
&+ \frac{h^3}{3!}(8a+b-d-8e) f'''_j + \frac{h^4}{4!}(16a+b+d+16e) f^{(4)}_j\\
&+ \frac{h^5}{5!}(32a+b-d-32e) f^{(5)}_j+\mathcal{O}(h^6)
\end{align*}
Now there are 5 variables but infinitely many equations(obtained after comparing coefficients of \(f\) and its derivatives at \(t_j\) on both sides of the above equation).
On comparing coefficients of \(f_j, f'_j, \dots, f^{(4)}_j\), we get 5 equations in 5 variables and order of accuracy in Taylor series is 5. If we take any other set of 5 equations, we get order of accuracy in Taylor series lower than 5. Thus, to get the best order of accuracy, the following equations must be true.

\begin{align}
a+b+c+d+e &=0\\
h(2a+b-d-2e) &=1\\
4a+b+d+4e &=0\\
8a+b-d-8e &=0\\
16a+b+d+16e &=0
\end{align}

On solving these set of equations, we get the values of \((a,b,c,d,e)\) as:

\begin{equation}
(a,b,c,d,e) = \left(-\frac{1}{12h}, \frac{2}{3h}, 0 , -\frac{2}{3h}, \frac{1}{12h} \right)
\end{equation}

Therefore the finite difference approximation with 4th order accuracy is:
\begin{equation}
f'_j = \frac{1}{12h}(-f_{j+2}+8f_{j+1}-8f_{j-1}+f_{j-2}) + \mathcal{O}(h^4)
\end{equation}

\hypertarget{solving-ordinary-differential-equations-numerically}{%
\chapter{Solving Ordinary Differential Equations Numerically}\label{solving-ordinary-differential-equations-numerically}}

\hypertarget{introduction-2}{%
\section{Introduction}\label{introduction-2}}

As seen in the previous chapter, our main focus in this course is to solve ODEs i.e., 1 independent variable per dependent variable numerically.

\textbf{Initial Value Problem(IVP):} ODE along with boundary conditions at a single point. Typically, independent variable is time.

\url{Example:-}
\begin{equation}
    \frac{d^2\theta}{dt^2}+C\frac{d\theta}{dt}+\frac{g}{l}\sin \theta = 0
\end{equation}
with ``initial conditions'' given by \(\theta(t=0)= \theta_0\) and \(\frac{d\theta}{dt}(t=0) = 0\).

\textbf{Boundary Value Problem(BVP):} Conditions known at multiple points
\url{Example:-}
\begin{equation}
    \frac{d^2\theta}{dt^2}+C\frac{d\theta}{dt}+\frac{g}{l}\sin \theta = 0
\end{equation}
with ``initial conditions'' given by \(\theta(t=0)= \theta_0\) and \(\theta(t=1) = 0\).

Note that the above ODE is 2nd order, non-linear and homogeneous ODE. If an external forcing is given, then the equation becomes non-homogeneous.

We can convert the above second order ODE to a set of 2 first order ODEs.

Define \[\omega = \frac{d\theta}{dt}\]

Therefore equation() can be written as:
\[\frac{d\omega}{dt}+C \omega+\frac{g}{l}\sin \theta = 0\]
Therefore, equation() can be written as a system of 1st order ODEs as:
\begin{equation}
\begin{bmatrix} \dot{\theta} \\  \dot{\omega} \end{bmatrix}=\begin{bmatrix} \omega \\  -C \omega-\frac{g}{l}\sin \theta \end{bmatrix}
\end{equation}

with initial conditions as
\begin{equation}
\begin{bmatrix} \theta(0) \\  \omega(0) \end{bmatrix}=\begin{bmatrix} \theta_0 \\  0 \end{bmatrix}
\end{equation}

Similarly, we can express any \(n\)th order ODE as system of \(n\) 1st order ODEs.

Hence, we'll first look into solving a first order ODE numerically and then solve Simple Pendulum equation numerically.

\hypertarget{different-basic-methods}{%
\section{Different Basic Methods}\label{different-basic-methods}}

Consider solving the IVP
\begin{equation}
\frac{dy}{dt} = \sin(\exp(y^2)t)
\end{equation}
with the initial condition \(y(0) = 1\). We can see that the analytical solution is not possible. To solve the IVP numerically, let us first discretise time as \(\{t_k\}_{k=0}^n\) where \(t_0 = 0\) and \(t_n=T\) time at which we are interested in finding \(y\) value and \(y_{i+1}-y_i = \Delta t\) for \(i\in\{0,1,\dots,n-1\}\).

\textbf{ATTACH GRID DISCRETISATION PIC}

We can find derivative \(\frac{dy}{dt}\) at \(t=t_i\)(for all \(i\)), using a finite difference formula and then convert the differential equation to a set of algebraic equations. Solving these would give \(y_i = y(t_i)\).

Forward difference gives:
\begin{equation}
\frac{dy}{dt}|_{t_i}\approx \frac{y_{i+1}-y_i}{\Delta t} = \sin(\exp(y_i^2)t_i)
\end{equation}
for \(i\in\{0,1,\cdots, n-1\}\)

For \(i=0\), we have:
\begin{equation}
y_1 = y_0 +\Delta t \sin(\exp(y_i^2)t_i)
\end{equation}
\(y_1\) is easier to compute as \(y_0 = y(t=0)\) is known. And hence \(y_2\) can be computed and so on.

Backward difference gives:
\begin{equation}
\frac{dy}{dt}|_{t_i}\approx \frac{y_{i}-y_{i-1}}{\Delta t} = \sin(\exp(y_i^2)t_i)
\end{equation}
for \(i\in\{1,2, \cdots,n\}\)
For \(i=1\), we have:
\begin{equation}
y_1 = y_0 +\Delta t \sin(\exp(y_1^2)t_1)
\end{equation}
Given \(y_0\), \(y_1\) can be found by using the above equation. Note that it is not as trivial as that of Forward difference case here. To get \(y_1\), we need to use some root finding algorithm. Similarly to get \(y_{i+1}\), we need to invoke root finding Algorithm. But in the case of Forward difference(also known as \textbf{Forward Euler Scheme}), we need not invoke any Root Finding Algorithm to find \(y_{i+1}\) and hence it is an \textbf{Explicit Scheme}

Backward Difference scheme is also referred to as \textbf{Backward Euler Scheme}. This is an \textbf{Implicit Scheme}

\hypertarget{linear-stability-analysis}{%
\section{Linear Stability Analysis}\label{linear-stability-analysis}}

Forward Euler seems to be more attractive because of the ease to compute the solution. But can we use this to solve all ODEs accurately?

\hypertarget{accuracy}{%
\section{Accuracy}\label{accuracy}}

\hypertarget{simple-pendulum---solving-system-of-odes}{%
\section{Simple Pendulum - Solving System of ODEs}\label{simple-pendulum---solving-system-of-odes}}

\hypertarget{simple-pendulum---finite-difference-for-2nd-order-derivative}{%
\section{Simple Pendulum - Finite difference for 2nd order derivative}\label{simple-pendulum---finite-difference-for-2nd-order-derivative}}

  \bibliography{book.bib,packages.bib}

\end{document}
