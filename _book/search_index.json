[["index.html", "Numerical Analysis Chapter 1 Introduction 1.1 Why Numerical Analysis? 1.2 Representing Numbers on a Machine 1.3 Condition Number of a Problem", " Numerical Analysis Sai Saandeep.S, Dr. Sivaram 2023-06-12 Chapter 1 Introduction 1.1 Why Numerical Analysis? 1.2 Representing Numbers on a Machine 1.3 Condition Number of a Problem Consider a function in one variable \\(f:\\mathbb{R}\\to\\mathbb{R}\\). Condition number for a function \\(f(x)\\) tells about the error amplification of a function \\(f(x)\\) i.e., for a given error in input \\(x\\), how much is the error in the output \\(f(x)\\). Absolute Condition Number \\(\\kappa_{\\text{abs}}\\) of the function \\(f(x)\\) is defined as: \\[\\begin{equation} \\kappa_{\\text{abs}} = \\frac{\\text{Absolute Change in Output}}{\\text{Absolute Change in Input}} = \\lim_{\\delta x \\to 0} \\left\\lvert{\\frac{f(x+\\delta x)-f(x)}{x+\\delta x - x}}\\right\\rvert = \\left\\lvert{f&#39;(x)}\\right\\rvert \\end{equation}\\] Relative Condition Number \\(\\kappa_{r}\\) of the function \\(f(x)\\) is defined as: \\[\\begin{equation} \\kappa_{r} = \\frac{\\text{Relative Change in Output}}{\\text{Relative Change in Input}} = \\lim_{\\delta x \\to 0} \\frac{\\left\\lvert{\\frac{f(x+\\delta x)-f(x)}{f(x)}}\\right\\rvert}{\\left\\lvert{\\frac{x+\\delta x - x}{x}}\\right\\rvert} = \\left\\lvert{\\frac{x}{f(x)}f&#39;(x)}\\right\\rvert \\end{equation}\\] Now what if the function has multiple inputs? Or What if the function has multiple outputs? Examples:- Input 2 numbers \\(a,b\\in\\mathbb{R}\\) and then find \\(f(a,b) = a+b\\)?. This problem takes 2 inputs- \\(a, b\\) and one output \\(f(a,b)\\). Find the roots of a polynomial \\(a_0+a_1x+a_2x^2+\\cdots+ a_nx^n\\). We are inputting the vector \\(\\begin{bmatrix} a_0 &amp; a_1 &amp; a_2 &amp; \\cdots &amp; a_n\\end{bmatrix}^T\\) and the output is \\(x\\) in this case. Given a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\). Input a vector \\(\\mathbf{x}\\in\\mathbb{R}^{n\\times 1}\\) and then find \\(f(\\mathbf{x}) = A\\mathbf{x} \\in \\mathbb{R}^{m\\times 1}\\)? Solve the linear system \\(A\\mathbf{x}=\\mathbf{b}\\) where \\(A\\in\\mathbb{R}^{m\\times n}\\), \\(\\mathbf{x}\\in\\mathbb{R}^{n\\times 1}\\) and \\(\\mathbf{b}\\in\\mathbb{R}^{m\\times 1}\\) . Inputs are \\(A, \\mathbf{b}\\) and output is a vector \\(\\mathbf{x}\\). To accommodate these cases, a generalized definition of a (relative) condition number \\(\\kappa_r\\) for a function \\(f:X\\to Y\\) where \\(X \\subset \\mathbb{R}^{m\\times 1}\\) and \\(Y \\subset \\mathbb{R}^{n\\times 1}\\) is shown below: \\[\\begin{equation} \\kappa_r = \\lim_{r\\to 0} \\sup_{\\lVert x \\rVert_q \\le r} \\frac{\\frac{\\lVert f(x+\\delta x)-f(x) \\rVert_p}{\\lVert f(x) \\rVert_p}}{\\frac{\\lVert \\delta x \\rVert_q}{\\lVert x \\rVert_q}} \\end{equation}\\] where \\(p,q \\in \\mathbb{N}\\) and \\(\\lVert . \\rVert_p\\) denotes the vector \\(p-\\) norm. 1.3.1 Vector Norms(Recap) For a vector \\(x\\) in the vextor space \\(X\\) over a field \\(F\\), \\(\\lVert . \\rVert:F\\to R\\) is defined such that: \\(\\lVert x \\rVert\\ge 0 \\ \\ \\ \\forall x \\in X\\). \\(\\lVert \\alpha x \\rVert = \\left\\lvert{\\alpha}\\right\\rvert, \\ \\ \\ \\forall x \\in X, \\ \\ \\ \\alpha \\in F\\) \\(\\lVert x+y \\rVert \\le \\lVert x \\rVert+\\lVert y \\rVert, \\ \\ \\ \\forall x,y \\in X\\). \\(\\lVert x \\rVert = 0 \\Longleftrightarrow x = 0\\) Let \\(x =\\begin{bmatrix} x_1 &amp; x_2&amp;\\cdots &amp;x_n \\end{bmatrix}^T\\). Different possible vector norms which satisfy the above conditions are: Euclidean norm (2-norm) \\[\\begin{equation} \\lVert x \\rVert_2 = \\sqrt{x_1^2+x_2^2+\\cdots+x_n^2} \\end{equation}\\] Supremum norm(max. norm) \\[\\begin{equation} \\lVert x \\rVert_{\\max} = \\lVert x \\rVert_{\\infty} = \\max_{1\\le i\\le n} |x_i| \\end{equation}\\] 1-norm \\[\\begin{equation} \\lVert x \\rVert_1 = \\sum_{i=1}^n |x_i| \\end{equation}\\] \\(p\\)-norm \\[\\begin{equation} \\lVert x \\rVert_{p} = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}} \\end{equation}\\] NOTE:- Supremum norm of \\(x\\) is \\(p\\)-norm of \\(x\\) as \\(p\\to\\infty\\) Proof:- From the definition, \\[\\lim_{p\\to\\infty} \\lVert x \\rVert_p = \\lim_{p\\to\\infty}\\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}}\\] \\[\\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}} \\le \\left( n\\max_{1\\le i\\le n} |x_i|^p \\right)^{\\frac{1}{p}} = n^{\\frac{1}{p}} \\max_{1\\le i\\le n} |x_i|\\] \\[\\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}} \\ge \\left( \\max_{1\\le i\\le n} |x_i|^p \\right)^{\\frac{1}{p}} = \\max_{1\\le i\\le n} |x_i|\\] From the above 2 inequalities, we can say that: \\[\\max_{1\\le i\\le n} |x_i| \\le \\lVert x \\rVert_p \\le n^{\\frac{1}{p}} \\max_{1\\le i\\le n} |x_i|\\] As \\(p \\to \\infty,\\ \\ \\ n^{\\frac{1}{p}} \\max_{1\\le i\\le n} |x_i| \\to \\max_{1\\le i\\le n} |x_i|\\). Therefore, by using sandwich theorem, we can say that \\[\\lVert x \\rVert_p = \\max_{1\\le i\\le n} |x_i|\\] 1.3.2 Examples on finding Condition number Let \\(f(a,b) = a+b\\). Find the condition number of this problem? The inputs are \\(a,\\, b\\). Let the inputs have an error \\(\\delta a,\\, \\delta b\\) respectively. \\[\\text{Relative error in input} = \\dfrac{\\left\\Vert \\begin{bmatrix} a+\\delta a\\\\ b+\\delta b \\end{bmatrix}- \\begin{bmatrix} a\\\\ b \\end{bmatrix}\\right\\Vert_p}{\\left\\Vert \\begin{bmatrix} a\\\\ b \\end{bmatrix}\\right\\Vert_p}\\] For simplicity, let us consider 2-norm. Any norm can be used in fact. Therefore, \\[\\text{Relative error in input} =\\dfrac{\\sqrt{\\delta a^2+\\delta b^2}}{\\sqrt{a^2+b^2}}\\] The output \\(f(a+\\delta a,b+\\delta b) = a+b+\\delta a+\\delta b\\). Therefore, \\[\\text{Relative Error in output} = \\dfrac{|(a+b+\\delta a + \\delta b)-(a+b)|}{|a+b|} = \\frac{|\\delta a+\\delta b|}{|a+b|}\\] The relative condition number is: \\[\\kappa_r = \\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r} \\dfrac{\\frac{|\\delta a+\\delta b|}{|a+b|}}{\\dfrac{\\sqrt{\\delta a^2+\\delta b^2}}{\\sqrt{a^2+b^2}}}\\] \\[\\implies \\kappa_r = \\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r} \\dfrac{|\\delta a+\\delta b|}{\\sqrt{\\delta a^2+\\delta b^2}} \\cdot \\dfrac{\\sqrt{a^2+b^2}}{|a+b|} \\] To calculate \\[\\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r}\\dfrac{|\\delta a+\\delta b|}{\\sqrt{\\delta a^2+\\delta b^2}}\\] we assume that \\(\\delta a = \\alpha \\cos \\theta\\) and \\(\\delta b = \\alpha \\sin \\theta\\) where \\(\\alpha&gt;0\\) and \\(0\\le\\theta&lt;2\\pi\\). Therefore, we have: \\[\\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r}\\dfrac{|\\delta a+\\delta b|}{\\sqrt{\\delta a^2+\\delta b^2}} = \\lim_{r \\to 0} \\sup_{\\alpha &lt; r} \\dfrac{|\\alpha \\cos \\theta+\\alpha \\sin \\theta|}{\\alpha} = \\lim_{r\\to 0} \\sup_{\\alpha&lt;r}|\\cos \\theta+\\sin \\theta| = \\sqrt{2}\\] Thus, the condition number for adding 2 numbers is: \\[ \\kappa_r = \\dfrac{\\sqrt{2(a^2+b^2)}}{|a+b|}\\le \\sqrt{2} \\text{ (if $a,\\, b&gt;0$)}\\] (as \\(|a+b| \\ge \\sqrt{a^2+b^2}\\) for \\(a,b \\in \\mathbb{R}^+\\)) For \\(a,b&gt;0\\), we can clearly see that the condition number is bounded above by \\(\\sqrt{2}\\). In other words, addition is well-conditioned. By performing a similar exercise, we can show that the subtraction is ill-conditioned as for \\(\\frac{a}{b} \\to 1\\), \\(\\kappa_r \\to \\infty\\). Multiplication and division operations are also ill-conditioned. Condition number on finding roots of the polynomial \\(x^2-2x+1\\). "],["numerical-linear-algebra.html", "Chapter 2 Numerical Linear Algebra 2.1 Columnspace, Nullspace and all 2.2 Matrix Norms 2.3 Condition number of Matrix vector products 2.4 Solving Linear Systems", " Chapter 2 Numerical Linear Algebra 2.1 Columnspace, Nullspace and all Consider a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\) defined as: \\[A = \\begin{bmatrix} -&amp;r_1^T&amp;-\\\\ -&amp;r_2^T&amp;-\\\\ &amp; \\vdots&amp; \\\\ - &amp; r_m^T&amp; - \\end{bmatrix} = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ a_1 &amp; a_2 &amp;\\cdots &amp; a_n \\\\ | &amp; | &amp; &amp; |\\end{bmatrix}\\] where \\(r_i \\in \\mathbb{R}^{n\\times 1}\\) for \\(1\\le i \\le m\\) are the rows and \\(a_i \\in \\mathbb{R}^{m\\times 1}\\) for \\(1\\le i\\le n\\) are the columns of \\(A\\). Columnspace of a matrix \\(A\\) is the span(linear combination) of columns of \\(A\\). Also called as Range of \\(A\\). \\[\\begin{equation} \\text{Range}(A) =\\text{Columnspace}(A) = \\{ Ax : x\\in \\mathbb{R}^{n\\times 1} \\} \\end{equation}\\] \\[Ax = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ a_1 &amp; a_2 &amp;\\cdots &amp; a_n \\\\ | &amp; | &amp; &amp; |\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix} = \\sum_{i=1}^n a_i x_i\\] Rowspace of a matrix \\(A\\) is the span(linear combination) of rows of \\(A\\). \\[\\begin{equation} \\text{Rowspace}(A) = \\{ A^Ty : y\\in \\mathbb{R}^{m\\times 1} \\} \\end{equation}\\] \\[A^Ty = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ r_1 &amp; r_2 &amp;\\cdots &amp; r_n \\\\ | &amp; | &amp; &amp; |\\end{bmatrix}\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n \\end{bmatrix} = \\sum_{i=1}^n r_i y_i\\] Nullspace of a matrix \\(A\\) is defined as follows: \\[\\begin{equation} \\text{Nullspace}(A) = \\{ z\\in \\mathbb{R}^{n\\times 1}: Az=0\\} \\end{equation}\\] NOTE:- A linear system \\(Ax=b\\) has a solution ONLY IF \\(b\\in\\text{Range}(A)\\). Dimension of Range\\((A)\\) is the number of linearly independent columns of \\(A\\) or the column rank of \\(A\\). Similarly, the dimension of Rowspace\\((A)\\) is the row rank or the number of independent rows of \\(A\\). For a matrix \\(A\\), row rank = column rank = rank\\(\\le \\min(m,n)\\). Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector \\(z\\in \\text{Nullspace}(A)\\) and \\(w \\in \\text{Rowspace}(A)\\) , \\(z\\) is orthogonal to \\(w\\). Proof:- Let \\(w \\in \\text{Rowspace}(A)\\), then \\(\\exists y\\in\\mathbb{R}^{n\\times 1}\\) such that \\(w = A^Ty\\). Also as \\(z\\in \\text{Nullspace}(A)\\), we have \\(Az=0\\). Therefore, \\[\\langle w,z\\rangle = w^Tz = y^T Az = 0\\] Thus, nullspace of a matrix is orthogonal to row space of a matrix. Rank-Nullity Theorem: Dimension of Nullspace\\((A)\\)+Rank\\((A)\\) = \\(n\\) = No. of columns of \\(A\\) 2.2 Matrix Norms Consider a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\). Just like how we have defined a vector norm, we could have defined an “element wise matrix norm” as follows: \\[\\begin{equation} \\lVert A \\rVert_p^* = \\left(\\sum_{i=1}^n |A_{ij}|^p\\right)^{\\frac{1}{p}} \\end{equation}\\] But this definition of norm does not satisfy the submultiplicative property. We are interested in this property as this dictates the convergence of iterative schemes. A matrix norm is said to be submultiplicative if for any matrices \\(A\\in \\mathbb{R}^{m\\times k}\\) and \\(B \\in \\mathbb{R}^{k\\times n}\\), we have \\[\\begin{equation} \\lvert AB \\rVert \\le \\lVert A \\rVert \\lVert B \\rVert \\end{equation}\\] Consider the case \\[A = \\begin{bmatrix} 2&amp;2\\\\2&amp;2 \\end{bmatrix}\\] and \\(p\\to \\infty\\), Therefore we have \\[ \\lVert A \\rVert_{\\infty}^* = \\max_{1\\le i \\le m,1 \\le j \\le n} |A_{ij}| = 2\\]. \\[A^2 = \\begin{bmatrix} 8 &amp; 8\\\\ 8&amp; 8 \\end{bmatrix}\\] Therefore, \\[\\lVert A^2 \\rVert_{\\infty}^* = 8\\] We can clearly see that: \\[\\lVert A^2 \\rVert_{\\infty}^* = 8 \\ge \\lVert A \\rVert_{\\infty}^* \\cdot \\lVert A \\rVert_{\\infty}^* = 2 \\times 2 = 4\\] which violates the submultiplicative property. Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows. \\[\\begin{equation} \\lVert A \\rVert_p = \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert Ax \\rVert_p}{\\lVert x \\rVert_p} = \\sup_{\\lVert y \\rVert_p = 1} \\lVert Ay \\rVert_p \\end{equation}\\] p-norms are submultiplicative. PROOF:- From the definition of p-norm, \\[\\begin{align*} \\lVert AB \\rVert_p &amp;= \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert ABx \\rVert_p}{\\lVert x \\rVert_p}\\\\ &amp;= \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert ABx \\rVert_p}{\\lVert Bx \\rVert_p} \\frac{\\lVert Bx \\rVert_p}{\\lVert x \\rVert_p} \\\\ &amp;\\le \\left[\\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert ABx \\rVert_p}{\\lVert Bx \\rVert_p} \\right] \\cdot \\left[ \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert Bx \\rVert_p}{\\lVert x \\rVert_p} \\right] \\\\ &amp;= \\lVert A \\rVert_p \\cdot \\lVert B \\rVert_p\\\\ \\therefore \\lVert AB \\rVert_p \\le \\lVert A \\rVert_p \\cdot \\lVert B \\rVert_p \\end{align*}\\] Using this property, we can say that \\[\\begin{equation} \\lVert A^n \\rVert_p \\le \\lVert A \\rVert^n_p \\end{equation}\\] \\(\\lVert A\\rVert_1\\) = Maximum of column sum of absolute values. \\(\\lVert A\\rVert_{\\infty}\\) = Maximum of row sum of absolute values. 2.3 Condition number of Matrix vector products Consider a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\) and a vector \\(x\\in\\mathbb{R}^{n\\times 1}\\). Assume that there is no error in representing \\(A\\). We are interested in finding the condition number of the Matrix-Vector product \\(f(x;A)= Ax\\). From the definition of condition number, we can write the condition number \\(\\kappa_r\\) of the matrix vector product as: \\[\\kappa_r = \\lim_{r\\to 0} \\sup_{\\Vert \\delta x \\Vert_q\\le r} \\dfrac{\\frac{\\Vert A(x+\\delta x)-Ax \\Vert_p}{\\Vert Ax \\Vert_p}}{\\frac{\\Vert x+\\delta x-x\\Vert_q}{\\Vert x\\Vert_q}}\\] For simplicity let us choose \\(p=q\\). Therefore, \\[\\kappa_r = \\lim_{r\\to 0} \\sup_{\\Vert \\delta x \\Vert_p\\le r} \\frac{\\Vert A\\delta x \\Vert_p}{\\Vert \\delta x \\Vert_p} \\frac{\\Vert x \\Vert_p}{\\Vert Ax\\Vert_p}\\] From the definition of matrix p-norm, we can say that: \\[\\lim_{r\\to 0} \\sup_{\\Vert \\delta x \\Vert_p\\le r} \\frac{\\Vert A\\delta x \\Vert_p}{\\Vert \\delta x \\Vert_p} = \\Vert A\\Vert_p\\] Therefore the condition number of the matrix vector product is: \\[\\begin{equation} \\kappa_r = \\frac{\\Vert A \\Vert_p \\Vert x \\Vert_p}{\\Vert Ax\\Vert_p} \\end{equation}\\] From Sub-multiplicative property, as \\(\\Vert Ax\\Vert_p \\ge\\Vert A \\Vert_p \\Vert x \\Vert_p\\), we can show that \\(\\kappa_r\\ge 1\\). 2.4 Solving Linear Systems "],["interpolation.html", "Chapter 3 Interpolation 3.1 Motivation - Interpolation vs. Approximation 3.2 Lagrange Interpolation 3.3 Choice of Nodes 3.4 Wierstrass Approximation theorem", " Chapter 3 Interpolation 3.1 Motivation - Interpolation vs. Approximation If the exact form of \\(f(x)\\) is known, then we have full information about \\(f(x)\\) i.e., Derivatives etc., But what if the exact form is not known? Given points \\(\\{x_i\\}_{i=1}^n\\) and functional values at those points \\(f(x_1),f(x_2), \\dots, f(x_n)\\), we wish to find an Approximation to \\(f(x)\\). One way to approximate a function is by interpolating it. We say that \\(p(x)\\) is an interpolant to \\(f(x)\\) at \\(\\{x_i\\}_{i=1}^n\\) if \\(p(x_i)= f(x_i)\\) for \\(1\\le i\\le n\\). Eg: A step function \\(p(x) = f(x_i)\\) for \\(x\\in \\left[ \\frac{x_i+x_{i-1}}{2}, \\frac{x_i+x_{i+1}}{2} \\right]\\). The step function, though it is an interpolant, we don’t prefer it. The main issue is that it is not continuous. We prefer in some practical applications for the interpolant to be differentiable. Assume \\(f(x)\\) to be continuous. Consider the sequence of interpolants \\(\\{P_n(x)\\}_{n=1}^{\\infty}\\) converging to \\(f(x)\\) on \\([a,b]\\) such that \\(P_n(x)=f(x) \\ \\ \\ \\forall x\\in \\{x_1,x_2,\\dots,x_n \\}\\) \\[\\begin{equation} \\tag{3.1} \\int_a^b f(x) \\, dx \\approx \\int_a^b P_n(x) \\, dx \\end{equation}\\] \\[\\begin{equation} \\tag{3.2} \\frac{df}{dx}(x) \\approx \\frac{dP_n}{dx}(x) \\end{equation}\\] Equation(3.1) holds true if \\(\\{P_n(x)\\}_{n=1}^{\\infty}\\) converges to \\(f(x)\\) uniformly. Equation(3.2) holds true if \\(P_n&#39;(x)\\) exists and \\(\\{P&#39;_n(x)\\}_{n=1}^{\\infty}\\) converges to \\(f&#39;(x)\\) uniformly. In this course, we consider the interpolants \\(p(x) \\in C^{\\infty}([a,b])\\). A simplest such interpolant would be a polynomial. 3.2 Lagrange Interpolation 3.2.1 Motivation Consider \\(p(x) = a_0+a_1x+a_2x^2+\\dots+ a_mx^m\\) to be a polynomial interpolant for \\(f(x)\\) with node points as \\(\\{x_i\\}_{i=1}^n\\). Therefore, \\[p(x_i)=f(x_i) \\implies a_0+a_1x_i+a_2x_i^2+\\dots+ a_mx_i^m = f(x_i) \\ \\ \\ \\forall i \\in \\{1,2,\\cdots, n\\}\\]. \\[\\begin{equation} \\implies \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^m\\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^m\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp; \\cdots &amp; x_n^m\\\\ \\end{bmatrix}_{n \\times(m+1)} \\begin{bmatrix}a_0\\\\a_1\\\\ \\vdots \\\\a_m \\end{bmatrix}_{(m+1) \\times 1} = \\begin{bmatrix} f(x_1)\\\\f(x_2)\\\\ \\vdots \\\\f(x_n) \\end{bmatrix}_{n\\times 1} \\end{equation}\\] Let \\[X = \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^m\\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^m\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp; \\cdots &amp; x_n^m\\\\ \\end{bmatrix}\\text{ ,} \\ \\ \\ \\bar{a} = \\begin{bmatrix}a_0\\\\a_1\\\\ \\vdots \\\\a_m \\end{bmatrix} \\text{ and } \\ \\ \\ \\bar{f}= \\begin{bmatrix} f(x_1)\\\\f(x_2)\\\\ \\vdots \\\\f(x_n) \\end{bmatrix}\\] \\[\\implies X\\bar{a} = \\bar{f}\\] Now the \\(\\bar{a}\\) has the coefficients of the interpolant. To find the coefficients, we need to solve the linear system. If \\(m+1&lt;n\\) then \\(X\\) is a thin matrix. i.e., we have lesser number of variables than equations. Therefore, solution may not exist. i.e., we may not be able to find \\(p(x)\\). If \\(m+1&gt;n\\) then \\(X\\) is a fat matrix. Infinitely many polynomial interpolants exist. If \\(m+1=n\\) then \\(X\\) is a square matrix. \\(X\\) is Vandermonde matrix. \\[\\det(X) = \\prod_{1\\le i&lt;j\\le n}(x_i-x_j)\\] If \\(x_i&#39;s\\) are distinct then \\(\\det(X)\\neq 0 \\implies X\\) is invertible. \\(\\implies X\\bar{a} = \\bar{f}\\) has a unique solution for \\(m+1=n\\). \\(\\implies p(x)\\) interpolates \\(f(x)\\) uniquely if \\(\\deg(p(x)) = n-1\\). Thus, the minimum degree of the interpolant polynomial is \\(n-1\\). \\(p(x)\\) interpolates \\(f(x)\\) uniquely if \\(\\deg(p(x))=n-1\\). Thus, solving the equation \\(X\\bar{a} = \\bar{f}\\). But there are issues in solving the linear system like this. Computational complexity in solving the linear system \\(\\mathcal{O}(n^3)\\). Condition number of \\(X\\) grows exponentially in \\(n\\). This is not preferred as this might cause large errors with small error in input(say due to roundoff errors etc.,) To overcome these problems, we use Lagrange interpolation. 3.2.2 Lagrange Interpolant 3.3 Choice of Nodes 3.3.1 Motivation 3.3.2 Fundamental Theorem of Polynomial Interpolation 3.3.3 Different Possible types of nodes 3.4 Wierstrass Approximation theorem "],["parts.html", "Chapter 4 Parts", " Chapter 4 Parts You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file. Add a numbered part: # (PART) Act one {-} (followed by # A chapter) Add an unnumbered part: # (PART\\*) Act one {-} (followed by # A chapter) Add an appendix as a special kind of un-numbered part: # (APPENDIX) Other stuff {-} (followed by # A chapter). Chapters in an appendix are prepended with letters instead of numbers. "],["footnotes-and-citations.html", "Chapter 5 Footnotes and citations 5.1 Footnotes 5.2 Citations", " Chapter 5 Footnotes and citations 5.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 1. 5.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2023) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References "],["blocks.html", "Chapter 6 Blocks 6.1 Equations 6.2 Theorems and proofs 6.3 Callout blocks", " Chapter 6 Blocks 6.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{6.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (6.1). 6.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 6.1. Theorem 6.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 6.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["sharing-your-book.html", "Chapter 7 Sharing your book 7.1 Publishing 7.2 404 pages 7.3 Metadata for sharing", " Chapter 7 Sharing your book 7.1 Publishing HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html 7.2 404 pages By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you’d like to customize your 404 page instead of using the default, you may add either a _404.Rmd or _404.md file to your project root and use code and/or Markdown syntax. 7.3 Metadata for sharing Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the index.Rmd YAML. To setup, set the url for your book and the path to your cover-image file. Your book’s title and description are also used. This gitbook uses the same social sharing data across all chapters in your book- all links shared will look the same. Specify your book’s source repository on GitHub using the edit key under the configuration options in the _output.yml file, which allows users to suggest an edit by linking to a chapter’s source file. Read more about the features of this output format here: https://pkgs.rstudio.com/bookdown/reference/gitbook.html Or use: ?bookdown::gitbook "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
