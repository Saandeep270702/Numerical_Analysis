[["index.html", "Numerical Analysis Chapter 1 Introduction 1.1 Why Numerical Analysis? 1.2 Representing Numbers on a Machine 1.3 Condition Number of a Problem", " Numerical Analysis Sai Saandeep.S, Dr. Sivaram 2023-07-03 Chapter 1 Introduction 1.1 Why Numerical Analysis? 1.2 Representing Numbers on a Machine 1.3 Condition Number of a Problem Consider a function in one variable \\(f:\\mathbb{R}\\to\\mathbb{R}\\). Condition number for a function \\(f(x)\\) tells about the error amplification of a function \\(f(x)\\) i.e., for a given error in input \\(x\\), how much is the error in the output \\(f(x)\\). Absolute Condition Number \\(\\kappa_{\\text{abs}}\\) of the function \\(f(x)\\) is defined as: \\[\\begin{equation} \\kappa_{\\text{abs}} = \\frac{\\text{Absolute Change in Output}}{\\text{Absolute Change in Input}} = \\lim_{\\delta x \\to 0} \\left\\lvert{\\frac{f(x+\\delta x)-f(x)}{x+\\delta x - x}}\\right\\rvert = \\left\\lvert{f&#39;(x)}\\right\\rvert \\end{equation}\\] Relative Condition Number \\(\\kappa_{r}\\) of the function \\(f(x)\\) is defined as: \\[\\begin{equation} \\kappa_{r} = \\frac{\\text{Relative Change in Output}}{\\text{Relative Change in Input}} = \\lim_{\\delta x \\to 0} \\frac{\\left\\lvert{\\frac{f(x+\\delta x)-f(x)}{f(x)}}\\right\\rvert}{\\left\\lvert{\\frac{x+\\delta x - x}{x}}\\right\\rvert} = \\left\\lvert{\\frac{x}{f(x)}f&#39;(x)}\\right\\rvert \\end{equation}\\] Now what if the function has multiple inputs? Or What if the function has multiple outputs? Examples:- Input 2 numbers \\(a,b\\in\\mathbb{R}\\) and then find \\(f(a,b) = a+b\\)?. This problem takes 2 inputs- \\(a, b\\) and one output \\(f(a,b)\\). Find the roots of a polynomial \\(a_0+a_1x+a_2x^2+\\cdots+ a_nx^n\\). We are inputting the vector \\(\\begin{bmatrix} a_0 &amp; a_1 &amp; a_2 &amp; \\cdots &amp; a_n\\end{bmatrix}^T\\) and the output is \\(x\\) in this case. Given a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\). Input a vector \\(\\mathbf{x}\\in\\mathbb{R}^{n\\times 1}\\) and then find \\(f(\\mathbf{x}) = A\\mathbf{x} \\in \\mathbb{R}^{m\\times 1}\\)? Solve the linear system \\(A\\mathbf{x}=\\mathbf{b}\\) where \\(A\\in\\mathbb{R}^{m\\times n}\\), \\(\\mathbf{x}\\in\\mathbb{R}^{n\\times 1}\\) and \\(\\mathbf{b}\\in\\mathbb{R}^{m\\times 1}\\) . Inputs are \\(A, \\mathbf{b}\\) and output is a vector \\(\\mathbf{x}\\). To accommodate these cases, a generalized definition of a (relative) condition number \\(\\kappa_r\\) for a function \\(f:X\\to Y\\) where \\(X \\subset \\mathbb{R}^{m\\times 1}\\) and \\(Y \\subset \\mathbb{R}^{n\\times 1}\\) is shown below: \\[\\begin{equation} \\kappa_r = \\lim_{r\\to 0} \\sup_{\\lVert x \\rVert_q \\le r} \\frac{\\frac{\\lVert f(x+\\delta x)-f(x) \\rVert_p}{\\lVert f(x) \\rVert_p}}{\\frac{\\lVert \\delta x \\rVert_q}{\\lVert x \\rVert_q}} \\end{equation}\\] where \\(p,q \\in \\mathbb{N}\\) and \\(\\lVert . \\rVert_p\\) denotes the vector \\(p-\\) norm. 1.3.1 Vector Norms(Recap) For a vector \\(x\\) in the vextor space \\(X\\) over a field \\(F\\), \\(\\lVert . \\rVert:F\\to R\\) is defined such that: \\(\\lVert x \\rVert\\ge 0 \\ \\ \\ \\forall x \\in X\\). \\(\\lVert \\alpha x \\rVert = \\left\\lvert{\\alpha}\\right\\rvert, \\ \\ \\ \\forall x \\in X, \\ \\ \\ \\alpha \\in F\\) \\(\\lVert x+y \\rVert \\le \\lVert x \\rVert+\\lVert y \\rVert, \\ \\ \\ \\forall x,y \\in X\\). \\(\\lVert x \\rVert = 0 \\Longleftrightarrow x = 0\\) Let \\(x =\\begin{bmatrix} x_1 &amp; x_2&amp;\\cdots &amp;x_n \\end{bmatrix}^T\\). Different possible vector norms which satisfy the above conditions are: Euclidean norm (2-norm) \\[\\begin{equation} \\lVert x \\rVert_2 = \\sqrt{x_1^2+x_2^2+\\cdots+x_n^2} \\end{equation}\\] Supremum norm(max. norm) \\[\\begin{equation} \\lVert x \\rVert_{\\max} = \\lVert x \\rVert_{\\infty} = \\max_{1\\le i\\le n} |x_i| \\end{equation}\\] 1-norm \\[\\begin{equation} \\lVert x \\rVert_1 = \\sum_{i=1}^n |x_i| \\end{equation}\\] \\(p\\)-norm \\[\\begin{equation} \\lVert x \\rVert_{p} = \\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}} \\end{equation}\\] NOTE:- Supremum norm of \\(x\\) is \\(p\\)-norm of \\(x\\) as \\(p\\to\\infty\\) Proof:- From the definition, \\[\\lim_{p\\to\\infty} \\lVert x \\rVert_p = \\lim_{p\\to\\infty}\\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}}\\] \\[\\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}} \\le \\left( n\\max_{1\\le i\\le n} |x_i|^p \\right)^{\\frac{1}{p}} = n^{\\frac{1}{p}} \\max_{1\\le i\\le n} |x_i|\\] \\[\\left(\\sum_{i=1}^n |x_i|^p\\right)^{\\frac{1}{p}} \\ge \\left( \\max_{1\\le i\\le n} |x_i|^p \\right)^{\\frac{1}{p}} = \\max_{1\\le i\\le n} |x_i|\\] From the above 2 inequalities, we can say that: \\[\\max_{1\\le i\\le n} |x_i| \\le \\lVert x \\rVert_p \\le n^{\\frac{1}{p}} \\max_{1\\le i\\le n} |x_i|\\] As \\(p \\to \\infty,\\ \\ \\ n^{\\frac{1}{p}} \\max_{1\\le i\\le n} |x_i| \\to \\max_{1\\le i\\le n} |x_i|\\). Therefore, by using sandwich theorem, we can say that \\[\\lVert x \\rVert_p = \\max_{1\\le i\\le n} |x_i|\\] 1.3.2 Examples on finding Condition number Let \\(f(a,b) = a+b\\). Find the condition number of this problem? The inputs are \\(a,\\, b\\). Let the inputs have an error \\(\\delta a,\\, \\delta b\\) respectively. \\[\\text{Relative error in input} = \\dfrac{\\left\\Vert \\begin{bmatrix} a+\\delta a\\\\ b+\\delta b \\end{bmatrix}- \\begin{bmatrix} a\\\\ b \\end{bmatrix}\\right\\Vert_p}{\\left\\Vert \\begin{bmatrix} a\\\\ b \\end{bmatrix}\\right\\Vert_p}\\] For simplicity, let us consider 2-norm. Any norm can be used in fact. Therefore, \\[\\text{Relative error in input} =\\dfrac{\\sqrt{\\delta a^2+\\delta b^2}}{\\sqrt{a^2+b^2}}\\] The output \\(f(a+\\delta a,b+\\delta b) = a+b+\\delta a+\\delta b\\). Therefore, \\[\\text{Relative Error in output} = \\dfrac{|(a+b+\\delta a + \\delta b)-(a+b)|}{|a+b|} = \\frac{|\\delta a+\\delta b|}{|a+b|}\\] The relative condition number is: \\[\\kappa_r = \\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r} \\dfrac{\\frac{|\\delta a+\\delta b|}{|a+b|}}{\\dfrac{\\sqrt{\\delta a^2+\\delta b^2}}{\\sqrt{a^2+b^2}}}\\] \\[\\implies \\kappa_r = \\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r} \\dfrac{|\\delta a+\\delta b|}{\\sqrt{\\delta a^2+\\delta b^2}} \\cdot \\dfrac{\\sqrt{a^2+b^2}}{|a+b|} \\] To calculate \\[\\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r}\\dfrac{|\\delta a+\\delta b|}{\\sqrt{\\delta a^2+\\delta b^2}}\\] we assume that \\(\\delta a = \\alpha \\cos \\theta\\) and \\(\\delta b = \\alpha \\sin \\theta\\) where \\(\\alpha&gt;0\\) and \\(0\\le\\theta&lt;2\\pi\\). Therefore, we have: \\[\\lim_{r\\to 0} \\sup_{\\left\\Vert\\begin{bmatrix} \\delta a\\\\ \\delta b \\end{bmatrix}\\right\\Vert_2\\le r}\\dfrac{|\\delta a+\\delta b|}{\\sqrt{\\delta a^2+\\delta b^2}} = \\lim_{r \\to 0} \\sup_{\\alpha &lt; r} \\dfrac{|\\alpha \\cos \\theta+\\alpha \\sin \\theta|}{\\alpha} = \\lim_{r\\to 0} \\sup_{\\alpha&lt;r}|\\cos \\theta+\\sin \\theta| = \\sqrt{2}\\] Thus, the condition number for adding 2 numbers is: \\[ \\kappa_r = \\dfrac{\\sqrt{2(a^2+b^2)}}{|a+b|}\\le \\sqrt{2} \\text{ (if $a,\\, b&gt;0$)}\\] (as \\(|a+b| \\ge \\sqrt{a^2+b^2}\\) for \\(a,b \\in \\mathbb{R}^+\\)) For \\(a,b&gt;0\\), we can clearly see that the condition number is bounded above by \\(\\sqrt{2}\\). In other words, addition is well-conditioned. By performing a similar exercise, we can show that the subtraction is ill-conditioned as for \\(\\frac{a}{b} \\to 1\\), \\(\\kappa_r \\to \\infty\\). Multiplication and division operations are also ill-conditioned. Condition number on finding roots of the polynomial \\(x^2-2x+1\\). "],["numerical-linear-algebra.html", "Chapter 2 Numerical Linear Algebra 2.1 Columnspace, Nullspace and all 2.2 Matrix Norms 2.3 Condition number of Matrix vector products 2.4 Solving Linear Systems 2.5 Solving Overdetermined Systems 2.6 Solving Underdetermined Systems 2.7 Iterative Methods for solving Linear Systems", " Chapter 2 Numerical Linear Algebra 2.1 Columnspace, Nullspace and all Consider a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\) defined as: \\[A = \\begin{bmatrix} -&amp;r_1^T&amp;-\\\\ -&amp;r_2^T&amp;-\\\\ &amp; \\vdots&amp; \\\\ - &amp; r_m^T&amp; - \\end{bmatrix} = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ a_1 &amp; a_2 &amp;\\cdots &amp; a_n \\\\ | &amp; | &amp; &amp; |\\end{bmatrix}\\] where \\(r_i \\in \\mathbb{R}^{n\\times 1}\\) for \\(1\\le i \\le m\\) are the rows and \\(a_i \\in \\mathbb{R}^{m\\times 1}\\) for \\(1\\le i\\le n\\) are the columns of \\(A\\). Columnspace of a matrix \\(A\\) is the span(linear combination) of columns of \\(A\\). Also called as Range of \\(A\\). \\[\\begin{equation} \\text{Range}(A) =\\text{Columnspace}(A) = \\{ Ax : x\\in \\mathbb{R}^{n\\times 1} \\} \\end{equation}\\] \\[Ax = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ a_1 &amp; a_2 &amp;\\cdots &amp; a_n \\\\ | &amp; | &amp; &amp; |\\end{bmatrix}\\begin{bmatrix}x_1\\\\x_2\\\\ \\vdots\\\\ x_n \\end{bmatrix} = \\sum_{i=1}^n a_i x_i\\] Rowspace of a matrix \\(A\\) is the span(linear combination) of rows of \\(A\\). \\[\\begin{equation} \\text{Rowspace}(A) = \\{ A^Ty : y\\in \\mathbb{R}^{m\\times 1} \\} \\end{equation}\\] \\[A^Ty = \\begin{bmatrix} | &amp; | &amp; &amp; | \\\\ r_1 &amp; r_2 &amp;\\cdots &amp; r_n \\\\ | &amp; | &amp; &amp; |\\end{bmatrix}\\begin{bmatrix}y_1\\\\y_2\\\\ \\vdots\\\\ y_n \\end{bmatrix} = \\sum_{i=1}^n r_i y_i\\] Nullspace of a matrix \\(A\\) is defined as follows: \\[\\begin{equation} \\text{Nullspace}(A) = \\{ z\\in \\mathbb{R}^{n\\times 1}: Az=0\\} \\end{equation}\\] NOTE:- A linear system \\(Ax=b\\) has a solution ONLY IF \\(b\\in\\text{Range}(A)\\). Dimension of Range\\((A)\\) is the number of linearly independent columns of \\(A\\) or the column rank of \\(A\\). Similarly, the dimension of Rowspace\\((A)\\) is the row rank or the number of independent rows of \\(A\\). For a matrix \\(A\\), row rank = column rank = rank\\(\\le \\min(m,n)\\). Nullspace of a matrix is orthogonal to row space of a matrix.i.e, Given any vector \\(z\\in \\text{Nullspace}(A)\\) and \\(w \\in \\text{Rowspace}(A)\\) , \\(z\\) is orthogonal to \\(w\\). Proof:- Let \\(w \\in \\text{Rowspace}(A)\\), then \\(\\exists y\\in\\mathbb{R}^{n\\times 1}\\) such that \\(w = A^Ty\\). Also as \\(z\\in \\text{Nullspace}(A)\\), we have \\(Az=0\\). Therefore, \\[\\langle w,z\\rangle = w^Tz = y^T Az = 0\\] Thus, nullspace of a matrix is orthogonal to row space of a matrix. Rank-Nullity Theorem: Dimension of Nullspace\\((A)\\)+Rank\\((A)\\) = \\(n\\) = No. of columns of \\(A\\) 2.2 Matrix Norms Consider a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\). Just like how we have defined a vector norm, we could have defined an “element wise matrix norm” as follows: \\[\\begin{equation} \\lVert A \\rVert_p^* = \\left(\\sum_{i=1}^n |A_{ij}|^p\\right)^{\\frac{1}{p}} \\end{equation}\\] But this definition of norm does not satisfy the submultiplicative property. We are interested in this property as this dictates the convergence of iterative schemes. A matrix norm is said to be submultiplicative if for any matrices \\(A\\in \\mathbb{R}^{m\\times k}\\) and \\(B \\in \\mathbb{R}^{k\\times n}\\), we have \\[\\begin{equation} \\lvert AB \\rVert \\le \\lVert A \\rVert \\lVert B \\rVert \\end{equation}\\] Consider the case \\[A = \\begin{bmatrix} 2&amp;2\\\\2&amp;2 \\end{bmatrix}\\] and \\(p\\to \\infty\\), Therefore we have \\[ \\lVert A \\rVert_{\\infty}^* = \\max_{1\\le i \\le m,1 \\le j \\le n} |A_{ij}| = 2\\]. \\[A^2 = \\begin{bmatrix} 8 &amp; 8\\\\ 8&amp; 8 \\end{bmatrix}\\] Therefore, \\[\\lVert A^2 \\rVert_{\\infty}^* = 8\\] We can clearly see that: \\[\\lVert A^2 \\rVert_{\\infty}^* = 8 \\ge \\lVert A \\rVert_{\\infty}^* \\cdot \\lVert A \\rVert_{\\infty}^* = 2 \\times 2 = 4\\] which violates the submultiplicative property. Hence, we define a p-norm of matrix which satisfies submultiplicative property as follows. \\[\\begin{equation} \\lVert A \\rVert_p = \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert Ax \\rVert_p}{\\lVert x \\rVert_p} = \\sup_{\\lVert y \\rVert_p = 1} \\lVert Ay \\rVert_p \\end{equation}\\] p-norms are submultiplicative. PROOF:- From the definition of p-norm, \\[\\begin{align*} \\lVert AB \\rVert_p &amp;= \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert ABx \\rVert_p}{\\lVert x \\rVert_p}\\\\ &amp;= \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert ABx \\rVert_p}{\\lVert Bx \\rVert_p} \\frac{\\lVert Bx \\rVert_p}{\\lVert x \\rVert_p} \\\\ &amp;\\le \\left[\\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert ABx \\rVert_p}{\\lVert Bx \\rVert_p} \\right] \\cdot \\left[ \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert Bx \\rVert_p}{\\lVert x \\rVert_p} \\right] \\\\ &amp;= \\lVert A \\rVert_p \\cdot \\lVert B \\rVert_p\\\\ \\therefore \\lVert AB \\rVert_p \\le \\lVert A \\rVert_p \\cdot \\lVert B \\rVert_p \\end{align*}\\] Using this property, we can say that \\[\\begin{equation} \\lVert A^n \\rVert_p \\le \\lVert A \\rVert^n_p \\end{equation}\\] \\(\\lVert A\\rVert_1\\) = Maximum of column sum of absolute values. \\(\\lVert A\\rVert_{\\infty}\\) = Maximum of row sum of absolute values. 2.3 Condition number of Matrix vector products Consider a matrix \\(A\\in\\mathbb{R}^{m\\times n}\\) and a vector \\(x\\in\\mathbb{R}^{n\\times 1}\\). Assume that there is no error in representing \\(A\\). We are interested in finding the condition number of the Matrix-Vector product \\(f(x;A)= Ax\\). From the definition of condition number, we can write the condition number \\(\\kappa_r\\) of the matrix vector product as: \\[\\kappa_r = \\lim_{r\\to 0} \\sup_{\\Vert \\delta x \\Vert_q\\le r} \\dfrac{\\frac{\\Vert A(x+\\delta x)-Ax \\Vert_p}{\\Vert Ax \\Vert_p}}{\\frac{\\Vert x+\\delta x-x\\Vert_q}{\\Vert x\\Vert_q}}\\] For simplicity let us choose \\(p=q\\). Therefore, \\[\\kappa_r = \\lim_{r\\to 0} \\sup_{\\Vert \\delta x \\Vert_p\\le r} \\frac{\\Vert A\\delta x \\Vert_p}{\\Vert \\delta x \\Vert_p} \\frac{\\Vert x \\Vert_p}{\\Vert Ax\\Vert_p}\\] From the definition of matrix p-norm, we can say that: \\[\\lim_{r\\to 0} \\sup_{\\Vert \\delta x \\Vert_p\\le r} \\frac{\\Vert A\\delta x \\Vert_p}{\\Vert \\delta x \\Vert_p} = \\Vert A\\Vert_p\\] Therefore the condition number of the matrix vector product is: \\[\\begin{equation} \\kappa_r = \\frac{\\Vert A \\Vert_p \\Vert x \\Vert_p}{\\Vert Ax\\Vert_p} \\end{equation}\\] From Sub-multiplicative property, as \\(\\Vert Ax\\Vert_p \\ge\\Vert A \\Vert_p \\Vert x \\Vert_p\\), we can show that \\(\\kappa_r\\ge 1\\). Case-1:- \\(A\\in \\mathbb{R}^{m\\times n}\\) is a fat matrix(\\(m&lt;n\\)) From Rank-Nullity Theorem, we know that \\[\\text{Dimension of Nullspace$(A)$+Rank$(A)$ = $n$}\\] We know that Rank\\((A) \\le \\min(m,n) \\implies\\) Rank \\((A) \\le m\\) as \\(A\\) is a fat matrix. \\[\\implies \\text{Dimension of Nullspace}(A) \\ge n-m\\] \\(\\implies \\exists\\) a non-zero vector \\(z \\in\\) Nullspace\\((A)\\) i.e., \\(\\exists z\\in \\mathbb{R}^{n\\times 1}\\) such that \\(Az=0\\). From the definition of condition number as in equation(), we have: \\(\\kappa_r(A,x) = \\frac{\\Vert A \\Vert_p \\Vert x \\Vert_p}{\\Vert Ax\\Vert_p}\\) If \\(x=z\\) then as \\(Az=0\\), we have \\(\\Vert Az\\Vert_p = 0\\). Therefore \\(\\kappa_r \\to \\infty\\). Thus, multiplication by a fat matrix is highly ill-conditioned. Case-2:- \\(A\\) is an invertible square matrix From the definition of condition number as in equation(), we have: \\[\\begin{align} \\kappa_r(A,x) &amp;= \\frac{\\Vert A \\Vert_p \\Vert x \\Vert_p}{\\Vert Ax\\Vert_p}\\\\ &amp;= \\frac{\\Vert A \\Vert_p \\Vert A^{-1}(Ax) \\Vert_p}{\\Vert Ax\\Vert_p}\\\\ \\end{align}\\] From submultiplicative property of matrix norms, we can write \\(\\Vert A^{-1}(Ax) \\Vert_p \\le \\Vert A^{-1} \\Vert_p \\Vert Ax \\Vert_p\\). Therefore, \\[\\begin{align} \\kappa_r(A,x)&amp;= \\frac{\\Vert A \\Vert_p \\Vert A^{-1}(Ax) \\Vert_p}{\\Vert Ax\\Vert_p}\\\\ \\implies \\kappa_r(A,x)&amp;\\le \\frac{\\Vert A \\Vert_p \\Vert A^{-1} \\Vert_p \\Vert Ax \\Vert_p}{\\Vert Ax\\Vert_p}\\\\ \\end{align}\\] \\[\\begin{equation} \\kappa_r(A,x) \\le \\Vert A \\Vert_p \\Vert A^{-1} \\Vert_p \\end{equation}\\] Therefore condition number is bounded above by \\(\\Vert A \\Vert_p \\Vert A^{-1} \\Vert_p\\) which is independent of vector \\(x\\). Define Condition number of matrix \\(A\\) as \\[\\kappa_p(A) = \\Vert A \\Vert_p \\Vert A^{-1} \\Vert_p\\]. From submultiplicative property we can show that \\(\\kappa_p(A)\\ge 1\\). Let us find condition number for some special matrices. Let us consider \\(A=Q\\) to be an orthogonal/ unitary matrix. As \\(Q\\) is an orthogonal matrix \\(Q^TQ = I\\implies Q^T = Q^{-1}\\). Therefore \\[\\Vert Qx \\Vert_2^2 = (Qx)^TQx = x^TQ^TQx = x^Tx = \\Vert x\\Vert_2^2\\] Thus, \\[\\Vert Qx \\Vert_2= \\Vert x\\Vert_2 = \\Vert Q^Tx \\Vert_2\\] From definition of 2-norm of a matrix, we have, \\[\\begin{align} \\Vert Q\\Vert_p &amp;= \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert Qx \\rVert_p}{\\lVert x \\rVert_p}\\\\ \\implies \\Vert Q\\Vert_2 &amp;= \\sup_{x\\in\\mathbb{R}^{n}\\setminus \\{ \\mathbf{0} \\}} \\frac{\\lVert x \\rVert_2}{\\lVert x \\rVert_2}\\\\ \\implies \\Vert Q\\Vert_2 &amp;= 1 \\end{align}\\] Therefore, \\[\\begin{equation} \\Vert Q^T\\Vert_2 = 1 \\end{equation}\\] The condition number of \\(Q\\) is therefore, \\[\\begin{equation} \\kappa_2(Q) = \\Vert Q \\Vert_p \\Vert Q^{-1} \\Vert_p = \\Vert Q \\Vert_2 \\Vert Q^T \\Vert_2 = 1 \\end{equation}\\] 2.4 Solving Linear Systems Consider the linear system \\(Ax=b\\) where \\(A\\in\\mathbb{R}^{m\\times n}\\), \\(\\mathbf{x}\\in\\mathbb{R}^{n\\times 1}\\) and \\(\\mathbf{b}\\in\\mathbb{R}^{m\\times 1}\\) . Inputs are \\(A, \\mathbf{b}\\) and output is a vector \\(\\mathbf{x}\\). In this course, we assume that \\(A\\) is a square(i.e., \\(m=n\\)) and invertible matrix (so that we have a unique \\(x\\)). Our goal is to find that unique \\(x\\) which satisfies the system of equations. Let’s go step by step. Let us consider special matrices first and then discuss about a general matrix \\(A\\). 2.4.1 Special Matrices If \\(A\\) is a unitary matrix, then \\(AA^T = I=A^TA\\). \\[Ax=b \\implies A^T Ax = A^Tb \\implies x = A^T b\\] To get \\(x\\), we need to perform \\(n^2\\) multiplications and \\(n(n-1)\\) additions. If \\(A=U\\) is an upper triangular matrix. Let \\(Ux=b\\) in matrix form be: \\[\\begin{equation} \\begin{bmatrix} U_{11} &amp; U_{12} &amp; U_{13} &amp; \\cdots &amp; U_{1n}\\\\ 0 &amp; U_{22} &amp; U_{23} &amp; \\cdots &amp; U_{2n}\\\\ 0 &amp; 0 &amp; U_{33} &amp; \\cdots &amp; U_{3n}\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots\\\\ 0 &amp; 0 &amp; 0 &amp; \\cdots &amp; U_{nn} \\end{bmatrix} \\begin{bmatrix} x_1 \\\\ x_2 \\\\x_3\\\\ \\vdots\\\\ x_n\\end{bmatrix} = \\begin{bmatrix} b_1 \\\\ b_2 \\\\b_3\\\\ \\vdots\\\\ b_n\\end{bmatrix} \\end{equation}\\] The last row of \\(U\\) corresponds to the equation \\[\\begin{equation} U_{nn} x_n = b_n \\end{equation}\\] Now as \\(U\\) is invertible, all the diagonal elements are non-zero i.e., \\(U_{ii}\\neq 0 \\ \\forall i \\in\\{1,2,\\cdots , n\\}\\). Therefore \\[\\begin{equation} U_{nn} x_n = b_n \\implies x_n = \\frac{b_n}{U_{nn}} \\end{equation}\\] Similarly, we have: \\[\\begin{equation} U_{(n-1),(n-1)}x_{n-1}+U_{(n-1),n}x_n = b_{n-1} \\implies x_{n-1} = \\frac{b_{n-1}-U_{(n-1),n}x_n}{U_{(n-1),(n-1)}} \\end{equation}\\] Using induction, we can prove that for \\(i \\in \\{1,2,\\cdots,n-1\\}\\), we have: \\[\\begin{equation} x_i = \\frac{b_i -\\sum_{j=i+1}^n U_{i,j}x_j}{U_{ii}} \\end{equation}\\] To calculate \\(x_i\\)(for \\(i \\in \\{1,2,\\cdots,n\\}\\)), we need to perform 1 division, \\((n-i)\\) multiplications and \\((n-i)\\) additions/subtractions. Therefore to calculate the output vector \\(x\\), we need to perform \\(\\sum_{i=1}^n 1 = n\\) divisions \\(\\sum_{i=1}^n (n-i) = \\frac{n^2-n}{2}\\) multiplications \\(\\sum_{i=1}^n (n-i) = \\frac{n^2-n}{2}\\) additions/subtractions In total \\(n+ \\frac{n^2-n}{2}+\\frac{n^2-n}{2} = n^2\\) operations are required to get \\(x\\). In other words “Computational Complexity is \\(n^2\\)” NOTE:- Loosely speaking computational complexity of an algorithm means the number of operations performed in the algorithm. We say that \\(f(n)\\in \\mathcal{O}(g(n))\\) 2.4.2 General Case 2.5 Solving Overdetermined Systems 2.6 Solving Underdetermined Systems 2.7 Iterative Methods for solving Linear Systems Previously, we have seen “direct methods” to solve linear systems like Gauss-Jordan elimination, Partial and complete pivoting. They can be solved exactly with \\(\\infty\\) precision machines. But the computational cost is high. For lesser computational cost, we use iterative methods. But the issue is that we can not solve exactly using these methods always. "],["interpolation.html", "Chapter 3 Interpolation 3.1 Motivation - Interpolation vs. Approximation 3.2 Lagrange Interpolation 3.3 Choice of Nodes 3.4 Wierstrass Approximation theorem", " Chapter 3 Interpolation 3.1 Motivation - Interpolation vs. Approximation If the exact form of \\(f(x)\\) is known, then we have full information about \\(f(x)\\) i.e., Derivatives etc., But what if the exact form is not known? Given points \\(\\{x_i\\}_{i=1}^n\\) and functional values at those points \\(f(x_1),f(x_2), \\dots, f(x_n)\\), we wish to find an Approximation to \\(f(x)\\). One way to approximate a function is by interpolating it. We say that \\(p(x)\\) is an interpolant to \\(f(x)\\) at \\(\\{x_i\\}_{i=1}^n\\) if \\(p(x_i)= f(x_i)\\) for \\(1\\le i\\le n\\). Eg: A step function \\(p(x) = f(x_i)\\) for \\(x\\in \\left[ \\frac{x_i+x_{i-1}}{2}, \\frac{x_i+x_{i+1}}{2} \\right]\\). The step function, though it is an interpolant, we don’t prefer it. The main issue is that it is not continuous. We prefer in some practical applications for the interpolant to be differentiable. Note that not all approximations are interpolants. Eg: A polynomial approximation to \\(\\sin x\\) is a truncated Taylor series after a few terms(say till degree \\(2n+1\\)). This approximation is not an interpolant as it won’t intersect \\(\\sin x\\) at \\(2n+2\\) points. Assume \\(f(x)\\) to be continuous. Consider the sequence of interpolants \\(\\{P_n(x)\\}_{n=1}^{\\infty}\\) converging to \\(f(x)\\) on \\([a,b]\\) such that \\(P_n(x)=f(x) \\ \\ \\ \\forall x\\in \\{x_1,x_2,\\dots,x_n \\}\\) \\[\\begin{equation} \\tag{3.1} \\int_a^b f(x) \\, dx \\approx \\int_a^b P_n(x) \\, dx \\end{equation}\\] \\[\\begin{equation} \\tag{3.2} \\frac{df}{dx}(x) \\approx \\frac{dP_n}{dx}(x) \\end{equation}\\] Equation(3.1) holds true if \\(\\{P_n(x)\\}_{n=1}^{\\infty}\\) converges to \\(f(x)\\) uniformly. Equation(3.2) holds true if \\(P_n&#39;(x)\\) exists and \\(\\{P&#39;_n(x)\\}_{n=1}^{\\infty}\\) converges to \\(f&#39;(x)\\) uniformly. In this course, we consider the interpolants \\(p(x) \\in C^{\\infty}([a,b])\\). A simplest such interpolant would be a polynomial. 3.2 Lagrange Interpolation 3.2.1 Motivation Consider \\(p(x) = a_0+a_1x+a_2x^2+\\dots+ a_mx^m\\) to be a polynomial interpolant for \\(f(x)\\) with node points as \\(\\{x_i\\}_{i=1}^n\\). Therefore, \\[p(x_i)=f(x_i) \\implies a_0+a_1x_i+a_2x_i^2+\\dots+ a_mx_i^m = f(x_i) \\ \\ \\ \\forall i \\in \\{1,2,\\cdots, n\\}\\]. \\[\\begin{equation} \\implies \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^m\\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^m\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp; \\cdots &amp; x_n^m\\\\ \\end{bmatrix}_{n \\times(m+1)} \\begin{bmatrix}a_0\\\\a_1\\\\ \\vdots \\\\a_m \\end{bmatrix}_{(m+1) \\times 1} = \\begin{bmatrix} f(x_1)\\\\f(x_2)\\\\ \\vdots \\\\f(x_n) \\end{bmatrix}_{n\\times 1} \\end{equation}\\] Let \\[X = \\begin{bmatrix} 1 &amp; x_1 &amp; x_1^2 &amp; \\cdots &amp; x_1^m\\\\ 1 &amp; x_2 &amp; x_2^2 &amp; \\cdots &amp; x_2^m\\\\ \\vdots &amp; \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; x_n &amp; x_n^2 &amp; \\cdots &amp; x_n^m\\\\ \\end{bmatrix}\\text{ ,} \\ \\ \\ \\bar{a} = \\begin{bmatrix}a_0\\\\a_1\\\\ \\vdots \\\\a_m \\end{bmatrix} \\text{ and } \\ \\ \\ \\bar{f}= \\begin{bmatrix} f(x_1)\\\\f(x_2)\\\\ \\vdots \\\\f(x_n) \\end{bmatrix}\\] \\[\\implies X\\bar{a} = \\bar{f}\\] Now the \\(\\bar{a}\\) has the coefficients of the interpolant. To find the coefficients, we need to solve the linear system. If \\(m+1&lt;n\\) then \\(X\\) is a thin matrix. i.e., we have lesser number of variables than equations. Therefore, solution may not exist. i.e., we may not be able to find \\(p(x)\\). If \\(m+1&gt;n\\) then \\(X\\) is a fat matrix. Infinitely many polynomial interpolants exist. If \\(m+1=n\\) then \\(X\\) is a square matrix. \\(X\\) is Vandermonde matrix. It can be shown that: \\[\\det(X) = \\prod_{1\\le i&lt;j\\le n}(x_i-x_j)\\] If \\(x_i&#39;s\\) are distinct then \\(\\det(X)\\neq 0 \\implies X\\) is invertible. \\(\\implies X\\bar{a} = \\bar{f}\\) has a unique solution for \\(m+1=n\\). \\(\\implies p(x)\\) interpolates \\(f(x)\\) uniquely if \\(\\deg(p(x)) = n-1\\). Thus, the minimum degree of the interpolant polynomial is \\(n-1\\). \\(p(x)\\) interpolates \\(f(x)\\) uniquely if \\(\\deg(p(x))=n-1\\). Thus, solving the equation \\(X\\bar{a} = \\bar{f}\\). But there are issues in solving the linear system like this. Computational complexity in solving the linear system \\(\\mathcal{O}(n^3)\\). Condition number of \\(X\\) grows exponentially in \\(n\\). This is not preferred as this might cause large errors with small error in input(say due to roundoff errors etc.,) To overcome these problems, we use Lagrange interpolation. 3.2.2 Lagrange Interpolant Consider \\[\\begin{equation} g(x_i) = \\begin{cases} 1 &amp; \\text{if } i\\neq j\\\\ 0 &amp; \\text{if } i=j \\end{cases} \\end{equation}\\] for \\(i,j \\in \\{1,2,\\dots,n\\}\\). We are interested to find a polynomial interpolant for this. Let us consider the case of \\(n=3\\) points and \\(j=2\\). i.e., \\(g(x_1)=g(x_3)=0\\) and \\(g(x_2)=1\\) for simplicity. The least degree of the polynomial interpolant \\(p(x)\\) is \\(n-1=2\\). By intuition, we can say that \\((x-x_1)(x-x_3)\\) is a factor of interpolant \\(p(x)\\). As \\(p(x_2)=g(x_2)=1\\), we can say that the interpolant \\(p(x)\\) is: \\[\\begin{equation} p(x) = \\frac{(x-x_1)(x-x_3)}{(x_2-x_1)(x_2-x_3)} \\end{equation}\\] For \\(n\\) points, we have the interpolant polynomial to \\(g(x)\\) as: \\[\\begin{equation} p(x) = l_j(x) = \\prod_{\\substack{i=0 \\\\ i\\neq j}} \\frac{x-x_i}{x_j-x_i} \\end{equation}\\] \\(l_j(x)\\) is a polynomial of degree \\(n-1\\) such that: \\[l_j(x_i) = \\delta_{ij}\\] Now consider \\(n\\) node points \\(\\{f(x_i)\\}_{i=1}^n\\) and consider the polynomial \\(p(x)\\) defined as: \\[\\begin{equation} p(x) = \\sum_{j=1}^n f(x_j) l_j(x) \\end{equation}\\] \\(p(x)\\) is a polynomial of degree atmost \\(n-1\\). Now, \\[p(x_i) = \\sum_{j=1}^n f(x_j) l_j(x_i) = \\sum_{j=1}^n f(x_j) \\delta_{ij} = f(x_i)\\] This implies that \\(p(x)\\) is an interpolant. This is known as Lagrange Interpolant. 3.3 Choice of Nodes 3.3.1 Motivation Now after finding the interpolant, the next question to be asked is how accurate is the interpolant? Immediate obvious answer would be the numer of points chosen. But are there any other factors which determine the accuracy of the interpolant? Example: Consider the function \\(f(x) = \\frac{1}{1+25x^2}\\) and the interpolation nodes as uniform nodes from [-1,1]. INSERT CODE HERE We can see that the interpolant is not converging to \\(f(x)\\) uniformly. There are some boundary effects Thus, selection of node points is also important. The question to ask now is what set of nodes guarantees uniform convergence? 3.3.2 Fundamental Theorem of Polynomial Interpolation Let \\(f(x)\\) be a smooth function on [-1,1]. Let \\(P_n(x)\\) be a polynomial interpolant to \\(f(x)\\) at \\(\\{x_k\\}_{k=0}^n\\) with atmost degree \\(n\\). Then \\(\\exists \\, \\zeta\\in[-1,1]\\) such that: \\[\\begin{equation} e(x) = f(x)-P_n(x) = \\frac{f^{(n+1)}(\\zeta)}{(n+1)!}\\prod_{k=0}^n(x-x_k) \\end{equation}\\] Proof:- Define \\[w(x) := \\prod_{k=0}^n(x-x_k)\\] and \\[\\begin{equation} g_x(t) : = f(t)-P_n(t)-\\left(\\frac{f(x)-P_n(x)}{w(x)}\\right)w(t) \\end{equation}\\] where the subscript \\(x\\) denotes that \\(x\\) is fixed. \\[g_x(x) = f(x)-P_n(x)-\\left(\\frac{f(x)-P_n(x)}{w(x)}\\right)w(x) = 0\\] \\[g_x(x_j) = f(x_j)-P_n(x_j)-\\left(\\frac{f(x)-P_n(x)}{w(x)}\\right)w(x_j)\\] As \\(P_n(x)\\) is an interpolant to \\(f(x)\\) at nodes \\(\\{x_i\\}_{i=0}^n\\), \\(P_n(x_j) = f(x_j)\\) and by definition \\(w(x_j) =0\\). Therefore, for \\(j\\in\\{0,1,2,\\dots ,n\\}\\), we have \\(g_x(x_j)=0\\). \\(g_x(t)\\) is smooth in the interval (-1,1). Define intervals \\[\\begin{align} I_1 &amp;= [x_0,x_1]\\\\ I_2 &amp;= [x_1,x_2]\\\\ &amp;\\vdots\\\\ I_k &amp;= [x_{k-1},x_k]\\\\ I_{k+1} &amp;= [x_k,x]\\\\ I_{k+2} &amp;=[x,x_{k+1}]\\\\ I_{k+3} &amp;= [x_{k+1},x_{k+2}]\\\\ &amp;\\vdots\\\\ I_{n+1} &amp;= [x_{n-1},x_n] \\end{align}\\] According to Rolle’s theorem, \\(g&#39;_x(t)\\) has atleast \\(n+1\\) zeros on \\([-1,1]\\). Again according to Rolle’s theorem, we can say that \\(g&#39;&#39;_x(t)\\) has atleast \\(n\\) zeros in \\([-1,1]\\). If we keep on using Rolle’s theorem for further \\(n-1\\) times, we can show that \\(g_x^{(n+1)}(t)\\) has atleast 1 zero on \\([-1,1]\\) i.e., \\(\\exists \\text{ a } \\zeta\\in[-1,1]\\) such that \\(g_x^{(n+1)}(\\zeta)=0\\) \\[g^{(n+1)}_x(\\zeta) = f^{(n+1)}(\\zeta)-P^{(n+1)}_n(\\zeta)-\\left(\\frac{f(x)-P_n(x)}{w(x)}\\right)w^{(n+1)}(\\zeta)\\] As \\(P_n(t)\\) is an $n^th degree polynomial, \\(P^{(n+1)}_n(\\zeta)=0\\). \\[w(t) = \\prod_{k=0}^n(t-x_k)\\implies w^{(n+1)}(t) = (n+1)!\\] Therefore \\[0 = f^{(n+1)}(\\zeta)-0 - \\left(\\frac{f(x)-P_n(x)}{w(x)}\\right)(n+1)!\\] \\[\\implies e(x) = f(x)-P_n(x) = \\frac{f^{(n+1)}(\\zeta)}{(n+1)!}w(x)= \\frac{f^{(n+1)}(\\zeta)}{(n+1)!}\\prod_{k=0}^n(x-x_k)\\] What if \\(x\\in[a,b]\\) instead of \\(x\\in[-1,1]\\)? We can use a linear mapping from \\([a,b]\\) to \\([-1,1]\\). 3.3.3 Different Possible types of nodes Now the goal is to find the nodes which minimise the maximum absolute interpolation error i.e., \\[\\min \\max_{x\\in[-1,1]} |e(x)|\\]. In general, we can also try finding nodes which minimise \\(p\\)-norm of the interpolation error i.e., \\(\\min \\lVert e(x) \\rVert_p\\) where: \\[\\lVert e(x) \\rVert_p = \\left( \\int_{-1}^1 |e(x)|^p \\, dx\\right)^{\\frac{1}{p}}\\] and \\[\\lim_{p\\to\\infty} \\lVert e(x) \\rVert_p = \\max_{x\\in[-1,1]}|e(x)|\\] Now the issue is it is difficult to know about \\(f^{(n+1)}(\\zeta)\\) as \\(f(x)\\) is not known always. Now the best thing we can do is to find nodes which minimise \\[\\min \\max_{x\\in[-1,1]}\\left|\\prod_{k=0}^n(x-x_k)\\right| \\text{ or } \\min \\lVert \\prod_{k=0}^n(x-x_k) \\rVert_p\\]. It turns out that Legendre nodes minimise \\(||w(x)||_2\\), Chebyshev nodes of first kind minimise \\(||w(x)||_{\\infty}\\) and Chebyshev nodes of second kind minimises \\(||w(x)||_1\\). 3.3.3.1 Legendre Nodes Monic Legendre polynomials are defined as: \\[q_0(x) = 1, \\, \\, q_1(x) = x\\] \\(q_n(x)\\) is a monic polynomial of degree \\(n\\) such that: \\[\\begin{equation} \\int_{-1}^1 q_n(x) q_m(x) \\, dx = 0 \\ \\ \\ \\forall \\, m\\neq n \\end{equation}\\] First few monic Legendre polynomials are: \\[\\begin{align*} q_0(x) &amp;= 1\\\\ q_1(x) &amp;= x\\\\ q_2(x) &amp;= x^2-\\frac{1}{3}\\\\ q_3(x) &amp;= x^3-\\frac{3}{5}x\\\\ q_4(x) &amp;= x^4- \\frac{6}{7}x^2+\\frac{3}{35} \\end{align*}\\] The zeros of these Legendre polynomials are called Legendre nodes. Legendre nodes minimise \\(||w(x)||_2\\). 3.3.3.2 Chebyshev nodes of first kind Chebyshev polynomials of first kind are given by \\(T_n(x) = \\cos(n\\cos^{-1}(x))\\). First few Chebyshev polynomials are: \\[\\begin{align} T_0(x) &amp;= 1\\\\ T_1(x) &amp;= x\\\\ T_2(x) &amp;= 2x^2-1\\\\ T_3(x) &amp;= 4x^3-3x\\\\ T_4(x) &amp;= 8x^4-8x^2+1 \\end{align}\\] An interesting property of these polynomials are \\(T_m(x)\\) and \\(T_n(x)\\) are orthogonal weighted by \\(\\frac{1}{\\sqrt{1-x^2}}\\). \\[\\begin{align} \\int_{-1}^{1}\\frac{ T_n(x)T_m(x)}{\\sqrt{1-x^2}}dx &amp;= 0 , m \\neq n\\\\ &amp;= \\pi,m = n= 0\\\\ &amp;= \\frac{\\pi}{2}, m = n \\neq 0 \\end{align}\\] Proof:- Let \\(x = \\cos \\theta\\) where \\(\\theta \\in [0,\\pi]\\). This implies that \\(dx = -\\sin \\theta \\, d \\theta = -\\sqrt{1-x^2} \\, d \\theta\\). And \\(x=-1 \\implies \\theta = \\pi\\) and \\(x =1 \\implies \\theta = 0\\). Therefore, \\[\\begin{align*} \\int_{-1}^{1} T_m(x)T_n(x) \\frac{1}{\\sqrt{1-x^2}} dx &amp; = \\int_{0}^{\\pi} \\cos(m\\theta) \\cos(n\\theta) d\\theta \\\\ &amp;= \\frac{1}{2}{\\int_{0}^{\\pi}} \\cos(m+n)\\theta + \\cos(m-n)\\theta d\\theta \\\\ &amp;=\\frac{1}{2} \\left[\\sin\\frac{(m+n)\\theta }{m+n}\\right]_{0}^{\\pi} + \\left[\\sin\\frac{(m-n)\\theta }{m-n}\\right]_{0}^{\\pi}\\\\ &amp;= 0, \\text{ if } m \\neq n \\end{align*}\\] Similarly with appropriate substitution, conditions for $m = n=0 $ and $m = n $ can be proved. The zeros of Chebyshev polynomial \\(T_{n+1}(x)\\) are given by \\(x_k = \\cos\\left( \\frac{2k+1}{2n+2} \\pi \\right)\\) where \\(k \\in \\{ 0,1,2,\\cdots,n \\}\\). Proof:- We have \\[T_{n+1}(x) = \\cos((n+1)\\arccos(x))\\] \\[T_{n+1}(x) = 0 \\implies \\cos((n+1)\\arccos(x)) = 0\\] \\[\\implies(n+1) \\arccos(x) = (2k+1)\\frac{\\pi}{2} \\ \\ \\ k \\in \\mathbb{Z}\\] \\[\\implies \\arccos(x) = (2k+1)\\frac{\\pi}{2(n+1)} \\ \\ \\ \\ k \\in \\mathbb{Z}\\] But as the principle range of \\(\\arccos(x)\\) is defined as \\([0,\\pi]\\), we must restrict the values \\(k\\) can take. \\[0 \\le (2k+1)\\frac{\\pi}{2(n+1)} \\le \\pi \\implies 0 \\le k \\le n+\\frac{1}{2} \\] But as \\(k \\in \\mathbb{Z}\\), we can say that the possible values \\(k\\) can take are \\(\\{ 0,1,2,\\cdots,n\\}\\). Therefore \\(\\arccos(x_k) = (2k+1)\\frac{\\pi}{2(n+1)} \\ \\ \\ \\ k \\in \\{ 0,1,2,\\cdots,n\\}\\) \\[\\implies x_k = \\cos\\left( (2k+1)\\frac{\\pi}{2(n+1)} \\right)\\ \\ \\ \\ k \\in \\{ 0,1,2,\\cdots,n\\}\\] Therefore the zeros of \\(T_{n+1}(x)\\) are in the interval \\([-1,1]\\) are given by \\(x_k = \\cos\\left( \\frac{2k+1}{2n+2} \\pi \\right)\\) where \\(k \\in \\{ 0,1,2,\\cdots,n \\}\\). The number of zeros is also consistent with the fact that as \\(T_{n+1}(x)\\) is an \\((n+1)\\) th-degree polynomial, it has \\((n+1)\\) roots. Also, these zeros are distinct. These zeros are called the Chebyshev nodes of first kind. They minimise \\(||w(x)||_{\\infty}\\) Theorem If \\(f(x)\\) is smooth on \\([-1,1]\\), then Lagrange interpolation using the roots of Chebyshev nodes of first kind converges uniformly. INSERT RUNGE FUNCTION-CHEBYSHEV NODES CONVERGENCE CODE 3.4 Wierstrass Approximation theorem Suppose \\(f\\) is a continuous real-valued function defined on the real interval \\([a,b]\\). For every \\(\\epsilon &gt; 0\\), there exists a polynomial \\(p\\) such that for all \\(x\\) in \\([a,b]\\), we have \\(|f(x) - p(x)| &lt; \\epsilon\\). Bernstein polynomial: The Bernstein basis polynomials of degree \\(n\\) are defined as \\[\\begin{equation} b_{k,n}(x) = {}^nC_{k}\\, x^k (1-x)^{n-k} \\end{equation}\\] A linear combination of these basis polynomials can be used to obtain other polynomials. One of the properties of these polynomials is that \\[\\begin{equation} \\begin{aligned} \\sum_{k=0}^n {b_{k,n}(x)} &amp;= \\sum_{k=0}^n {{}^nC_{k}\\, x^k (1-x)^{n-k}} \\\\ &amp;= (x + (1 - x))^n = 1 \\label{equ:property} \\end{aligned} \\end{equation}\\] So these polynomials can also be considered to act as some kind of weights. Now, for approximating functions, the Bernstein Polynomial is defined as \\[\\begin{equation} B_n(f;x) = \\sum_{k=0}^n {f\\left(\\dfrac{k}{n}\\right) {}^nC_{k}\\, x^k (1-x)^{n-k}} \\label{eqn:bern} \\end{equation}\\] Here, \\(f\\) is the function being approximated, \\(n\\) is the order of approximation &amp; \\(x\\) is the point at which the approximation is made. Proof: To prove the theorem on closed intervals \\([a,b]\\), without loss of generality, we can take the closed interval as \\([0, 1]\\). Thus, \\(f\\) can be considered as a continuous real-valued function on \\([0, 1]\\). Since \\(f\\) is a continuous function, we can say that for a given \\(\\epsilon &gt; 0\\), there exists a \\(\\delta &gt; 0\\) such that: \\[\\begin{equation} |x-y| &lt; \\delta \\implies |f(x) - f(y)| &lt;\\frac{\\epsilon}{2} \\ \\ \\ \\ \\forall \\, x,y \\in [0,1] \\label{equ:convcond} \\end{equation}\\] To prove that \\(B_n(f,x)\\) converges to \\(f(x)\\) uniformly, we show that \\(|B_n(f,x) - f(x)|\\) has to be made small. By the definition of Bernstein’s polynomial, as shown in Eqn. \\(\\ref{eqn:bern}\\) and by using the property given in Eqn. \\(\\ref{equ:property}\\), we can write \\(|B_n(f;x) - f(x)|\\) as: \\[\\begin{align*} |B_n(f;x) - f(x)| &amp;= \\left| \\sum_{k=0}^n {\\left(f\\left(\\dfrac{k}{n}\\right) - f(x)\\right) {}^nC_{k} \\, x^k (1-x)^{n-k}} \\right| \\\\ &amp;\\leq \\sum_{k=0}^n {\\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right| {}^nC_{k} \\, x^k (1-x)^{n-k}} \\end{align*}\\] Now, if we consider \\(y=\\dfrac{k}{n}\\) in Eqn. \\(\\ref{equ:convcond}\\) and if \\(\\left|\\dfrac{k}{n} - x\\right| &lt; \\delta\\), we can say that \\(\\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right| &lt; \\dfrac{\\epsilon}{2}\\). But this is not true in the entire domain. So, we partition the domain in to two sets: \\(A\\) and \\(B\\), where \\(A\\) and \\(B\\) have the following properties: \\[\\begin{equation} \\begin{aligned} A \\cup B &amp;= [0,1]\\\\ A \\cap B &amp;= \\phi\\\\ A &amp;= \\{x:|k/n - x| \\leq \\delta, x \\in [0,1]\\}\\\\ B &amp;= \\{x:|k/n-x| &gt; \\delta, x \\in [0,1]\\} \\end{aligned} \\label{equ:sets} \\end{equation}\\] By dividing the domain into sets \\(A\\) and \\(B\\) as defined in \\(\\ref{equ:sets}\\), we can write: \\[\\begin{equation*} \\begin{split} |B_n(f;x) - f(x)| = \\sum_{x\\in A} \\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right| &amp;{}^nC_{k}\\, x^k (1-x)^{n-k} \\\\ &amp;+ \\sum_{x\\in B} \\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right |{}^nC_{k}\\, x^k (1-x)^{n-k} \\end{split} \\end{equation*}\\] From Eqns. \\(\\ref{equ:convcond}\\) and \\(\\ref{equ:sets}\\), we can say that \\(\\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right|\\) has an upper bound of \\(\\dfrac{\\epsilon}{2}\\) on the set \\(A\\). Therefore, we can write: \\[\\begin{equation*} \\begin{split} |B_n(f;x) - f(x)| \\leq \\sum_{x\\in A} \\left(\\dfrac{\\epsilon}{2}\\right){}^nC_{k}\\, x^k &amp;(1-x)^{n-k} \\\\ &amp;+ \\sum_{x\\in B} \\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right |{}^nC_{k}\\, x^k (1-x)^{n-k} \\end{split} \\end{equation*}\\] By using the property described in Eqn. \\(\\ref{equ:property}\\), we can simplify the above inequality as: \\[\\begin{equation*} |B_n(f,;x) - f(x)| \\leq \\frac{\\epsilon}{2} + \\sum_{x\\in B} \\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right |{}^nC_{k}\\, x^k (1-x)^{n-k} \\end{equation*}\\] Since \\(f\\) is uniformly converging in \\([0,1]\\), \\(f\\) is bounded from above. So, let the maximum value of \\(f\\) in the domain be \\(M\\). Thus: \\[\\begin{equation*} \\text{max\\{} | f(x) | \\} = M \\ \\ \\ \\ \\forall \\, x \\in [0,1] \\end{equation*}\\] Thus, the maximum value \\(\\left|f\\left(\\dfrac{k}{n}\\right) - f(x)\\right|\\) can achieve in the domain is \\(2M\\) (considering the case where, one of them is \\(M\\) and the other is \\(-M\\)). Thus, we have: \\[\\begin{equation*} |B_n(f;x) - f(x)| \\leq \\frac{\\epsilon}{2} + (2M) \\sum_{x\\in B} {{}^nC_{k}\\, x^k (1-x)^{n-k}} \\end{equation*}\\] Now, in set \\(B\\), \\(\\left| \\dfrac{k}{n} - x \\right| &gt; \\delta\\). Thus, we get: \\[\\begin{equation*} \\frac{\\left( k/n - x \\right)^2}{\\delta^2} &gt; 1 \\end{equation*}\\] Multiplying this to the second term of RHS, we get: \\[\\begin{equation*} |B_n(f;x) - f(x)| \\leq \\frac{\\epsilon}{2} + (2M) \\sum_{x\\in B} {\\frac{\\left( k/n - x \\right)^2}{\\delta^2}{}^nC_{k}\\, x^k (1-x)^{n-k}} \\end{equation*}\\] The second term can now be written as: \\[\\begin{equation*} \\dfrac{2M}{\\delta^2 n^2} \\sum_{x\\in B} {(k-nx)^2 \\, {}^nC_{k}\\, x^k (1-x)^{n-k}} \\end{equation*}\\] The summation term is equivalent to computing the variance of a binomial distribution with parameters \\(n\\) &amp; \\(x\\). The variance is given by \\(nx(1-x)\\). Thus, we get: \\[\\begin{equation*} |B_n(f;x) - f(x)| \\leq \\frac{\\epsilon}{2} + \\dfrac{2M}{\\delta^2 n^2} nx(1-x) \\end{equation*}\\] We know that using AM \\(\\geq\\) GM: \\[\\begin{equation*} x(1-x) \\leq \\dfrac{1}{4} \\end{equation*}\\] Thus, we have: \\[\\begin{equation*} |B_n(f;x) - f(x)| \\leq \\frac{\\epsilon}{2} + \\dfrac{M}{2\\delta^2 n} \\end{equation*}\\] Now, let us define a quantity \\(N\\) such that the above condition holds true for all \\(n &gt; N\\), which gives us \\(\\dfrac{1}{n} &lt; \\dfrac{1}{N}\\). Thus: \\[\\begin{equation*} |B_n(f;x) - f(x)| \\leq \\frac{\\epsilon}{2} + \\dfrac{M}{2\\delta^2 N} \\end{equation*}\\] If we choose the \\(N\\) such that: \\[\\begin{equation*} \\dfrac{M}{2\\delta^2N} = \\dfrac{\\epsilon}{2} \\end{equation*}\\] giving us: \\[\\begin{align*} |B_n(f;x) - f(x)| &amp;\\leq \\dfrac{\\epsilon}{2} + \\dfrac{\\epsilon}{2} \\\\ |B_n(f;x) - f(x)| &amp;\\leq \\epsilon \\end{align*}\\] Thus, we have for all \\(n &gt; N\\), where \\(N = \\dfrac{M}{\\delta^2\\epsilon}\\), we have \\[\\begin{equation} |B_n(f;x) - f(x)| \\leq \\epsilon \\end{equation}\\] i.e., the Bernstein Polynomial \\(B_n(f;x)\\) uniformly converges to \\(f(x)\\) for all \\(x\\) in the domain \\([0,1]\\). NOTE:- 1. A sequence \\(\\{a_n\\}_{n \\ge 0}\\) converges to \\(a\\) algebraically if \\(\\exists N&gt;0\\) such that \\(\\forall \\, n&gt;N\\) \\[|a_n-a|&lt;\\frac{k}{n^{\\alpha}} \\text{ for some } k, \\,\\alpha &gt;0 \\] Examples:- a. \\(a_n = 1+\\frac{1}{n^2}\\) converges to 1 algebaically as \\[|a_n-1| = \\frac{1}{n^2}&lt;\\frac{10}{n^2}\\] \\(a_n = e^{-n}\\) converges to 0 algebraically as \\[|a_n| =e^{-n} &lt;\\frac{1}{n} \\ \\forall n\\ge 1\\] A sequence \\(\\{a_n\\}_{n \\ge 0}\\) converges to \\(a\\) geometrically if \\(\\exists N&gt;0\\) such that \\(\\forall \\, n&gt;N\\) \\[|a_n-a|&lt;kc^n \\text{ for some } k&gt;0 , |c|&lt;1 \\] Given any \\(c\\) and \\(\\alpha\\) (such that \\(|c|&lt;1\\) and \\(\\alpha&gt;0\\)), there exists \\(N = N(c,\\alpha)&gt;0\\) such that \\(\\forall n&gt;N\\) \\[c^n&lt;\\frac{1}{n^{\\alpha}}\\] This implies that all geometrically convergent series are algebraically convergent. Error in Bernstein polynomial \\(B_n(x)\\) approximation goes down as \\(\\frac{1}{n}\\). This means that \\(B_n(x)\\) uniformly converges to \\(f(x)\\) algebraically at the rate \\(\\frac{1}{n}\\). If \\(f(x)\\) is smooth then the Chebyshev interpolant is Geometrically convergent to \\(f\\). This convergence is also known as Spectral Convergence. Consider the function \\(f(x) = \\frac{1}{1+25x^2}\\). Let us consider the analytic continuation of \\(f(x)\\) on a compact set \\(\\Omega \\in \\mathbb{C}^2\\) such that \\([-1,1] \\subset \\Omega\\) which is \\(\\frac{1}{1+25z^2}\\). This is analytic on any compact set \\(\\Omega\\) which doesn’t contain \\(\\pm i/5\\). "],["quadratures.html", "Chapter 4 Quadratures 4.1 Motivation 4.2 Different Schemes 4.3 Euler-Maclaurian Formula 4.4 Gaussian Quadratures", " Chapter 4 Quadratures 4.1 Motivation 4.2 Different Schemes 4.2.1 Basic Schemes 4.2.2 Accuracy 4.2.3 Simpson’s Rule 4.2.4 Trapezoidal Rule with End point Corrections 4.3 Euler-Maclaurian Formula 4.3.1 Introduction 4.3.2 Formula and Derivation 4.3.3 Higher Order Quadratures 4.4 Gaussian Quadratures "],["root-finding-algorithms.html", "Chapter 5 Root Finding Algorithms 5.1 Motivation 5.2 Bisection Method 5.3 Newton Method", " Chapter 5 Root Finding Algorithms 5.1 Motivation Let \\(f(x)\\) be a continuous function. We are interested in solving \\(f(x)=0\\) for \\(x\\),i.e., we are interested in finding \\(x^*\\) such that \\(f(x^*)=0\\). But finding \\(x^*\\) analytically is not always easy. Examples:- Solving for \\(x\\) for \\(3x=9\\), \\(x^2+2x-1=0\\) are easy. We have closed form zeros for \\(x\\) till 4th order polynomial. Beyond 4th order closed form solution is NOT possible. Solving transcendental equations like \\(\\tan x = x\\), \\(x=e^x\\) exactly. Considering the issues present, we need to think of solving the equations numerically. Consider \\(f(x) = xe^x-5\\). We are now interested to find \\(x^*\\) such that \\(f(x^*)=0\\). Let us guess say \\(x^*=1.2\\). \\(f(1.2) = -1.015859692716143\\). We need to refine this further! For this we need to find a sequence of \\(x_k\\)’s iteratively, such that the sequence \\(\\{x_k\\}\\) converges to \\(x^*\\). 5.2 Bisection Method We know from Intermediate Value theorem that if \\(f(x)\\) is continuous on \\([a,b]\\) and if \\(f(a) f(b)&lt;0\\), \\(\\exists x^* \\, \\in(a,b)\\) such that \\(f(x^*)=0\\). Given a continuous function \\(f(x)\\) and if we could find \\(a,b\\in \\mathbb{R}\\) such that \\(a&lt;b\\) and \\(f(a) f(b)&lt;0\\), then we can find \\(x^*\\) as follows: Choose \\(x_0 = \\frac{a+b}{2}\\) and if \\(f(x_0) f(a)&lt;0 \\implies \\ \\exists \\text{ a } x^*\\, \\in (a,x_0)\\). \\(f(x_0) f(a)&gt;0 \\implies \\ \\exists \\text{ a } x^*\\, \\in (x_0,b)\\) \\(f(x_0) =0 \\implies x^* = x_0\\) Refine the guess by taking mean of new interval and check for convergence. The stopping criteria are \\(|f(x)|&lt;\\epsilon\\) and \\(|I_k|&lt;\\delta\\) where \\(|I_k|\\) is the length of the interval \\(I_k\\) at the \\(k^{th}\\) step. This is found as: Interval at \\(0^{th}\\) step is \\(I_0 = [a,b]\\). Hence the length of interval \\(I_0\\) is \\(|I_0|= b-a\\). Let the interval at \\(k^{th}\\) step is \\(I_k\\). Then from step 1,2 we can say that \\(|I_k| = \\frac{|I_{k-1}|}{2}\\). Therefore: \\[|I_k| = \\frac{b-a}{2^k}\\] \\(\\epsilon\\) and \\(\\delta\\) are user specified small tolerences.(eg 1e-10). INSERT PICTURE HERE INSERT CODE/PSEUDO-CODE HERE This method described above is called as Bisection method. When does Bisection Method fail? In case of Bisection Method, if 2 initial points \\(a,b \\in \\mathbb{R}\\) are chosen such that \\(f(a)f(b)&lt;0\\), the we can guarantee that Root can be found from the Intermediate value theorem. Thus, the sequence of iterates always converge to the root. 5.3 Newton Method In Bisection method, we need to find functional value at 2 points initially where we expect the root to lie between them. Is it possible to find the root by taking just 1 initial guess? Newton’s method allows us to do this! But we need to know the value of derivative at that point. INSERT PICTURE HERE Pick guess value \\(x_0\\). We improve this guess by finding the unique root of the linear approximation at this point. The linear approximation is the tangent at that point. The equation of tangent to \\(f(x)\\) at \\(x=x_0\\) is: \\[y-f(x_0) = f&#39;(x_0) (x-x_0)\\] This intersects \\(x\\) axis at say \\((x_1,0)\\). Therefore, \\[0-f(x_0) = f&#39;(x_0) (x_1-x_0)\\] If \\(f&#39;(x_0)\\neq 0\\), then \\(x_1\\) exists. Therefore, the improved guess is \\[\\begin{equation} x_1 = x_0 - \\frac{f(x_0)}{f&#39;(x_0)} \\end{equation}\\] On repeating the process, we get the improved guess iterates as: \\[\\begin{equation} x_{k+1} = x_k - \\frac{f(x_k)}{f&#39;(x_k)} \\end{equation}\\] INSERT CODE/PSEUDOCODE HERE 5.3.1 Rate of Convergence Assume that the iterate sequence \\(\\{x_k\\}\\to x^*\\) where \\(f(x^*)=0\\). Define \\(e_k = x_k-x^* \\implies x^*=x_k-e_k\\). We assume that \\(f(x)\\) is a continuous function which is twice differentiable with \\(f&#39;&#39;(x)\\) being continuous. Writing Taylor series of \\(f(x^*)=0\\) about \\(x_k\\) gives: \\[f(x^*)=0= f(x_k-e_k) = f(x_k) - e_k f&#39;(x_k)+\\frac{e_k^2}{2!} f&#39;&#39;(\\zeta_k) \\] where \\(\\zeta_k \\in (\\min(x_k,x^*),\\max(x_k,x^*))\\) \\[\\implies 0=f(x_k)-(x_k-x^*)f&#39;(x_k)+\\frac{e_k^2}{2!} f&#39;&#39;(\\zeta_k)\\] \\[\\implies 0 = f(x_k) - (x_k-x_{k+1}+x_{k+1}-x^*)f&#39;(x_k) + \\frac{e_k^2}{2!} f&#39;&#39;(\\zeta_k)\\] \\[\\implies 0 = f(x_k) - (x_k-x_{k+1}) f&#39;(x_k) - e_{k+1} f&#39;(x_k) +\\frac{e_k^2}{2!} f&#39;&#39;(\\zeta_k)\\] From equation() \\[\\implies 0 =f(x_k) - \\frac{f(x_k)}{f&#39;(x_k)}f&#39;(x_k) - e_{k+1} f&#39;(x_k) +\\frac{e_k^2}{2!} f&#39;&#39;(\\zeta_k)\\] \\[ e_{k+1} = \\frac{e_k^2}{2!} \\frac{f&#39;&#39;(\\zeta_k)}{f&#39;(x_k)}\\] As \\(\\zeta_k \\in (\\min(x_k,x^*),\\max(x_k,x^*))\\) and as \\(f&#39;&#39;(x)\\) and \\(f&#39;(x)\\) are continuous, \\(\\exists\\, M,m\\in \\mathbb{R}\\) such that \\(f&#39;&#39;(\\zeta_k) \\le M\\) and \\(f&#39;(x_k) \\ge m\\). Therefore, \\[\\frac{f&#39;&#39;(\\zeta_k)}{2 f&#39;(x_k)} \\le \\frac{M}{2m} = C \\ \\text{(say)}\\] Therefore, \\[\\begin{equation} e_{k+1} \\le C e_k^2 \\end{equation}\\] This implies that the rate of converge is quadratic for Newton’s method. For Bisection, the convergence rate is linear. RATE OF CONVERGENCE plot/code 5.3.2 Possibility of Non-Convergence? Now the main question comes! When does Newton Method fail? When \\(f&#39;(x_k) = 0\\) at any step in iteration. This means that the tangent at \\(x=x_k\\) is parallel to \\(x\\) axis and hence, no root can be found from there! Eg: \\(f(x) = x^2-1\\) and \\(x_0 = 0\\) This implies that \\(f&#39;(x_0) = 2x_0 = 0\\). Thus we have to change initial guess. \\(f(x) = x^{\\frac{1}{3}}\\) and \\(x_0 = a&gt;0\\). Root is \\(x^*=0\\) \\(f&#39;(x) = \\frac{1}{3x^{\\frac{2}{3}}}\\) \\[x_{k+1}= x_k - \\frac{f(x_k)}{f&#39;(x_k)} = x_k - \\frac{x_k^{\\frac{1}{3}}}{\\frac{1}{3x_k^{\\frac{2}{3}}}} = -2 x_k\\] Therefore, \\(x_n = (-2)^n x_0 = (-2)^n a\\). \\[\\lim_{n \\to \\infty} x_n \\text{ doesn&#39;t exist.}\\] Thus, convergence is not always guaranteed in Newton’s Method. "],["numerical-differentiation.html", "Chapter 6 Numerical Differentiation 6.1 Motivation 6.2 Finite Differences", " Chapter 6 Numerical Differentiation 6.1 Motivation Given a continuous function \\(f(x)\\). We have seen that finding anti-derivative for all functions is not possible and hence, we resort to finding approximate values of definite integrals using Numerical methods(Quadratures). But derivatives can be found for any differentiable function \\(f(x)\\) using Chain rule etc., Eg:- Consider \\(f(x) = \\cos(x^2)\\) which is continuous and differentiable. \\(\\int_0^1 f(x) \\, dx = \\int_0^1 \\cos(x^2)\\, dx\\) has no closed form. But \\(f&#39;(x) = -2x\\sin(x^2)\\) which can be found using chain rule. Now the question which naturally arises is: Why do we need to think of finding derivatives numerically then? Consider the case where \\(f(x)\\) is known at discrete node points \\(\\{x_i\\}_{i=0}^n\\). To find derivatives at these nodes, one way is to interpolate using an interpolant polynomial \\(p(x)\\) and then find \\(p&#39;(x)\\). We can find using a less computationally expensive way as will be discussed in this chapter. Solving Differential Equations Ordinary Differential Equations(ODEs): Simple Pendulum:- ATTACH SIMPLE PENDULUM PIC WITH ITS FBD For simple pendulum with no damping and using small angle approximation about equilibrium position, we have the equation of motion as: \\[\\begin{equation} \\frac{d^2\\theta}{dt^2}+\\frac{g}{l}\\theta = 0 \\end{equation}\\] with initial conditions \\(\\theta(t=0)= \\theta_0\\) and \\(\\frac{d\\theta}{dt}(t=0) = 0\\). The exact solution for this ODE is known which is \\(\\theta = \\theta_0 \\cos\\left( \\sqrt{g}{l} t \\right)\\). But if the assumptions of small angle oscillations and no damping are relaxed then the equation of motion becomes: \\[\\begin{equation} \\frac{d^2\\theta}{dt^2}+C\\frac{d\\theta}{dt}+\\frac{g}{l}\\sin \\theta = 0 \\end{equation}\\] with initial conditions remaining the same. But solving this ODE analytically is impossible. Therefore, we need access to numerical methods to solve ODEs. Partial Differential Equations(PDEs):- Some examples are Wave Equation(Hyperbolic PDE) \\[\\partial{\\partial^2 u}{\\partial t^2}-c^2 \\nabla^2 u= 0\\] Heat/Diffusion Equation(Parabolic PDE) \\[\\partial{\\partial u}{\\partial t}-\\alpha \\nabla^2 u= 0\\] Laplace Equation(Elliptic PDE) \\[\\nabla^2 u = 0 \\text{ on } \\Omega\\] with boundary condition \\(u = f(x,y,z)\\) on \\(\\partial \\Omega\\) For specific boundaries like a rectangle, the solution can be found analytically. But for a general boundaries, it is extremely difficult to find analytical solutions. Therefore, numerical methods have to be used for solving these PDEs. In this course, we consider solving only ODEs i.e, 1 independent variable per dependent variable. 6.2 Finite Differences To construct approximations for derivatives, we use Taylor series. Let \\(f(t)\\) be a real differentiable function whose values are known at \\(\\{t_i\\}_{i=0}^n=t_0+i h\\) where \\(h = \\Delta t\\) which is a constant Define \\(f_i = f(t_i)\\) and \\(f&#39;_i = \\frac{df}{dt}(t_i)\\) We can write \\(f_{j+1}\\) in terms of \\(f_j\\) and derivatives of \\(f\\) at \\(t_j\\) by writing Taylor series of \\(f(t_{j+1})\\) about \\(t=t_j\\). \\[f_{j+1} = f(t_j+h)= f(t_j)+hf&#39;(t_j)+\\frac{h^2}{2!}f&#39;&#39;(t_j)+\\dots\\] \\[\\dfrac{f_{j+1}-f_j}{h}-\\frac{h}{2!}f&#39;&#39;(t_j)-\\frac{h^2}{3!}f&#39;&#39;&#39;(t_j)-\\dots = f&#39;(t_j) = f&#39;_j\\] \\[\\begin{equation} f&#39;_j = \\dfrac{f_{j+1}-f_j}{h} + \\mathcal{O}(h) \\end{equation}\\] The above approximation is called Forward Finite Difference Approximation Now consider the Taylor series of \\(f(t_{j-1})\\) about \\(t=t_j\\). \\[f_{j-1} = f(t_j-h)= f(t_j)-hf&#39;(t_j)+\\frac{h^2}{2!}f&#39;&#39;(t_j)+\\dots\\] \\[\\dfrac{f_{j}-f_{j-1}}{h}+\\frac{h}{2!}f&#39;&#39;(t_j)-\\frac{h^2}{3!}f&#39;&#39;&#39;(t_j)+\\dots = f&#39;(t_j) = f&#39;_j\\] \\[\\begin{equation} f&#39;_j = \\dfrac{f_{j}-f_{j-1}}{h} + \\mathcal{O}(h) \\end{equation}\\] The above approximation is called Backward Finite Difference Approximation We can clearly see that both Forward and Backward Finite differences are 1st order accurate. Now the question to ask is “Can we improve the order of accuracy?” Consider the Taylor Series Expansions of \\(f_{j+1}\\) and \\(f_{j-1}\\) about \\(t_j\\). \\[\\begin{align*} f_{j+1} &amp;= f(t_j)+hf&#39;(t_j)+\\frac{h^2}{2!}f&#39;&#39;(t_j)+\\frac{h^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{h^4}{4!}f^{(4)}(t_j)+\\dots\\\\ f_{j-1} &amp;= f(t_j)-hf&#39;(t_j)+\\frac{h^2}{2!}f&#39;&#39;(t_j)-\\frac{h^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{h^4}{4!}f^{(4)}(t_j)-\\dots \\end{align*}\\] Subtracting () from (), we get \\[\\implies f_{j+1}-f_{j-1} = 2hf&#39;_j+\\frac{2h^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{2h^5}{5!}f^{(5)}(t_j)+\\dots\\] \\[\\implies f_{j+1}-f_{j-1}-\\frac{2h^3}{3!}f&#39;&#39;&#39;(t_j)-\\frac{2h^5}{5!}f^{(5)}(t_j)-\\dots = 2hf&#39;_j\\] \\[\\begin{equation} f&#39;_j = \\frac{f_{j+1}-f_{j-1}}{2h} + \\mathcal{O}(h^2) \\end{equation}\\] We can see that the order of accuracy has been improved to 2. This approximation is called Central Finite Difference Approximation. ATTACH CODE/ACCURACY PLOT To improve the order of accuracy, we need to consider more “grid points”. To understand this clearly, let us consider the example below. Example: Construct Finite difference scheme to find \\(f&#39;_j\\) from \\(f_j\\), \\(f_{j\\pm 1}\\) and \\(f_{j \\pm 2}\\). Solution: Let \\[\\begin{equation} f&#39;_j = a f_{j+2}+b f_{j+1}+cf_{j}+d f_{j-1}+e f_{j-2} \\end{equation}\\] where \\(a,b,c,d,e\\) are to be found. Write Taylor series of \\(f_{j \\pm 1}\\) and \\(f_{j\\pm 1}\\) about \\(t_j\\) on RHS of the above equation(), we get: \\[\\begin{align*} f&#39;_j &amp;= a\\left(f(t_j)+2hf&#39;(t_j)+\\frac{(2h)^2}{2!}f&#39;&#39;(t_j)+\\frac{(2h)^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{(2h)^4}{4!}f^{(4)}(t_j)+\\frac{(2h)^5}{5!}f^{(5)}(t_j)+\\dots \\right) \\\\ &amp;+ b \\left( f(t_j)+hf&#39;(t_j)+\\frac{h^2}{2!}f&#39;&#39;(t_j)+\\frac{h^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{h^4}{4!}f^{(4)}(t_j)+\\frac{h^5}{5!}f^{(5)}(t_j)+\\dots \\right)\\\\ &amp;+ cf_j \\\\ &amp;+ d \\left(f(t_j)-hf&#39;(t_j)+\\frac{h^2}{2!}f&#39;&#39;(t_j)-\\frac{h^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{h^4}{4!}f^{(4)}(t_j)-\\frac{h^5}{5!}f^{(5)}(t_j)+\\dots \\right)\\\\ &amp;+ e \\left( f(t_j)-2hf&#39;(t_j)+\\frac{(2h)^2}{2!}f&#39;&#39;(t_j)-\\frac{(2h)^3}{3!}f&#39;&#39;&#39;(t_j)+\\frac{(2h)^4}{4!}f^{(4)}(t_j)-\\frac{(2h)^5}{5!}f^{(5)}(t_j)+\\dots \\right) \\end{align*}\\] \\[\\begin{align*} \\implies f&#39;_j &amp;= (a+b+c+d+e) f_j + h(2a+b-d-2e)f&#39;_j+ \\frac{h^2}{2!}(4a+b+d+4e) f&#39;&#39;_j \\\\ &amp;+ \\frac{h^3}{3!}(8a+b-d-8e) f&#39;&#39;&#39;_j + \\frac{h^4}{4!}(16a+b+d+16e) f^{(4)}_j\\\\ &amp;+ \\frac{h^5}{5!}(32a+b-d-32e) f^{(5)}_j+\\mathcal{O}(h^6) \\end{align*}\\] Now there are 5 variables but infinitely many equations(obtained after comparing coefficients of \\(f\\) and its derivatives at \\(t_j\\) on both sides of the above equation). On comparing coefficients of \\(f_j, f&#39;_j, \\dots, f^{(4)}_j\\), we get 5 equations in 5 variables and order of accuracy in Taylor series is 5. If we take any other set of 5 equations, we get order of accuracy in Taylor series lower than 5. Thus, to get the best order of accuracy, the following equations must be true. \\[\\begin{align} a+b+c+d+e &amp;=0\\\\ h(2a+b-d-2e) &amp;=1\\\\ 4a+b+d+4e &amp;=0\\\\ 8a+b-d-8e &amp;=0\\\\ 16a+b+d+16e &amp;=0 \\end{align}\\] On solving these set of equations, we get the values of \\((a,b,c,d,e)\\) as: \\[\\begin{equation} (a,b,c,d,e) = \\left(-\\frac{1}{12h}, \\frac{2}{3h}, 0 , -\\frac{2}{3h}, \\frac{1}{12h} \\right) \\end{equation}\\] Therefore the finite difference approximation with 4th order accuracy is: \\[\\begin{equation} f&#39;_j = \\frac{1}{12h}(-f_{j+2}+8f_{j+1}-8f_{j-1}+f_{j-2}) + \\mathcal{O}(h^4) \\end{equation}\\] "],["solving-ordinary-differential-equations-numerically.html", "Chapter 7 Solving Ordinary Differential Equations Numerically 7.1 Introduction 7.2 Different Basic Methods 7.3 Linear Stability Analysis 7.4 Accuracy 7.5 Simple Pendulum - Solving System of ODEs 7.6 Simple Pendulum - Finite difference for 2nd order derivative", " Chapter 7 Solving Ordinary Differential Equations Numerically 7.1 Introduction As seen in the previous chapter, our main focus in this course is to solve ODEs i.e., 1 independent variable per dependent variable numerically. Initial Value Problem(IVP): ODE along with boundary conditions at a single point. Typically, independent variable is time. Example:- \\[\\begin{equation} \\frac{d^2\\theta}{dt^2}+C\\frac{d\\theta}{dt}+\\frac{g}{l}\\sin \\theta = 0 \\end{equation}\\] with “initial conditions” given by \\(\\theta(t=0)= \\theta_0\\) and \\(\\frac{d\\theta}{dt}(t=0) = 0\\). Boundary Value Problem(BVP): Conditions known at multiple points Example:- \\[\\begin{equation} \\frac{d^2\\theta}{dt^2}+C\\frac{d\\theta}{dt}+\\frac{g}{l}\\sin \\theta = 0 \\end{equation}\\] with “initial conditions” given by \\(\\theta(t=0)= \\theta_0\\) and \\(\\theta(t=1) = 0\\). Note that the above ODE is 2nd order, non-linear and homogeneous ODE. If an external forcing is given, then the equation becomes non-homogeneous. We can convert the above second order ODE to a set of 2 first order ODEs. Define \\[\\omega = \\frac{d\\theta}{dt}\\] Therefore equation() can be written as: \\[\\frac{d\\omega}{dt}+C \\omega+\\frac{g}{l}\\sin \\theta = 0\\] Therefore, equation() can be written as a system of 1st order ODEs as: \\[\\begin{equation} \\begin{bmatrix} \\dot{\\theta} \\\\ \\dot{\\omega} \\end{bmatrix}=\\begin{bmatrix} \\omega \\\\ -C \\omega-\\frac{g}{l}\\sin \\theta \\end{bmatrix} \\end{equation}\\] with initial conditions as \\[\\begin{equation} \\begin{bmatrix} \\theta(0) \\\\ \\omega(0) \\end{bmatrix}=\\begin{bmatrix} \\theta_0 \\\\ 0 \\end{bmatrix} \\end{equation}\\] Similarly, we can express any \\(n\\)th order ODE as system of \\(n\\) 1st order ODEs. Hence, we’ll first look into solving a first order ODE numerically and then solve Simple Pendulum equation numerically. 7.2 Different Basic Methods Consider solving the IVP \\[\\begin{equation} \\frac{dy}{dt} = \\sin(\\exp(y^2)t) \\end{equation}\\] with the initial condition \\(y(0) = 1\\). We can see that the analytical solution is not possible. To solve the IVP numerically, let us first discretise time as \\(\\{t_k\\}_{k=0}^n\\) where \\(t_0 = 0\\) and \\(t_n=T\\) time at which we are interested in finding \\(y\\) value and \\(y_{i+1}-y_i = \\Delta t\\) for \\(i\\in\\{0,1,\\dots,n-1\\}\\). ATTACH GRID DISCRETISATION PIC We can find derivative \\(\\frac{dy}{dt}\\) at \\(t=t_i\\)(for all \\(i\\)), using a finite difference formula and then convert the differential equation to a set of algebraic equations. Solving these would give \\(y_i = y(t_i)\\). 7.3 Linear Stability Analysis 7.4 Accuracy 7.5 Simple Pendulum - Solving System of ODEs 7.6 Simple Pendulum - Finite difference for 2nd order derivative "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
