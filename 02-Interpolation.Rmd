# Interpolation

## Motivation - Interpolation vs. Approximation

If the exact form of $f(x)$ is known, then we have full information about $f(x)$ i.e., Derivatives etc., But what if the exact form is not known? 

Given points $\{x_i\}_{i=1}^n$ and functional values at those points $f(x_1),f(x_2), \dots, f(x_n)$, we wish to find an Approximation to $f(x)$. One way to approximate a function is by *interpolating* it. 

>We say that $p(x)$ is an interpolant to $f(x)$ at $\{x_i\}_{i=1}^n$ if $p(x_i)= f(x_i)$ for $1\le i\le n$. 

Eg: A step function $p(x) = f(x_i)$ for $x\in \left[ \frac{x_i+x_{i-1}}{2}, \frac{x_i+x_{i+1}}{2} \right]$.

The step function, though it is an interpolant, we don't prefer it. The main issue is that it is not continuous. We prefer in some practical applications for the interpolant to be differentiable. 

Note that not all approximations are interpolants. 

Eg: A polynomial approximation to $\sin x$ is a truncated Taylor series after a few terms(say till degree $2n+1$). This approximation is not an interpolant as it won't intersect $\sin x$ at $2n+2$ points.

Assume $f(x)$ to be continuous. Consider the sequence of interpolants $\{P_n(x)\}_{n=1}^{\infty}$ converging to $f(x)$ on $[a,b]$ such that $P_n(x)=f(x) \ \  \ \forall x\in \{x_1,x_2,\dots,x_n \}$

\begin{equation}
(\#eq:integralfinterp)
\int_a^b f(x) \, dx \approx \int_a^b P_n(x) \, dx 
\end{equation}

\begin{equation}
(\#eq:dfdxinterp)
\frac{df}{dx}(x) \approx \frac{dP_n}{dx}(x)
\end{equation}

Equation\@ref(eq:integralfinterp) holds true if $\{P_n(x)\}_{n=1}^{\infty}$ converges to $f(x)$ *uniformly*.

Equation\@ref(eq:dfdxinterp) holds true if $P_n'(x)$ exists and $\{P'_n(x)\}_{n=1}^{\infty}$ converges to $f'(x)$ *uniformly*.


In this course, we consider the interpolants $p(x) \in C^{\infty}([a,b])$. A simplest such interpolant would be a polynomial. 


## Lagrange Interpolation

### Motivation

Consider $p(x) = a_0+a_1x+a_2x^2+\dots+ a_mx^m$ to be a polynomial interpolant for $f(x)$ with **node points** as $\{x_i\}_{i=1}^n$. Therefore,
$$p(x_i)=f(x_i) \implies a_0+a_1x_i+a_2x_i^2+\dots+ a_mx_i^m = f(x_i) \ \ \ \forall i \in \{1,2,\cdots, n\}$$.


\begin{equation}
\implies \begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m\\
1 & x_2 & x_2^2 & \cdots & x_2^m\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m\\
\end{bmatrix}_{n \times(m+1)} \begin{bmatrix}a_0\\a_1\\ \vdots \\a_m \end{bmatrix}_{(m+1) \times 1}  = \begin{bmatrix} f(x_1)\\f(x_2)\\ \vdots \\f(x_n) \end{bmatrix}_{n\times 1}
\end{equation}

Let $$X = \begin{bmatrix}
1 & x_1 & x_1^2 & \cdots & x_1^m\\
1 & x_2 & x_2^2 & \cdots & x_2^m\\
\vdots & \vdots & \vdots & \ddots & \vdots \\
1 & x_n & x_n^2 & \cdots & x_n^m\\
\end{bmatrix}\text{ ,} \ \  \ \bar{a} = \begin{bmatrix}a_0\\a_1\\ \vdots \\a_m \end{bmatrix}  \text{ and } \ \ \ \bar{f}=  \begin{bmatrix} f(x_1)\\f(x_2)\\ \vdots \\f(x_n) \end{bmatrix}$$
$$\implies X\bar{a} = \bar{f}$$

Now the $\bar{a}$ has the coefficients of the interpolant. To find the coefficients, we need to solve the linear system. 

If $m+1<n$ then $X$ is a thin matrix. i.e., we have lesser number of variables than equations. Therefore, solution may not exist. i.e., we may not be able to find $p(x)$.

If $m+1>n$ then $X$ is a fat matrix. Infinitely many polynomial interpolants exist.

If $m+1=n$ then $X$ is a square matrix. $X$ is Vandermonde matrix. It can be shown that:
$$\det(X) = \prod_{1\le i<j\le n}(x_i-x_j)$$
If $x_i's$ are distinct then $\det(X)\neq 0 \implies X$ is invertible. 

$\implies X\bar{a} = \bar{f}$ has a unique solution for $m+1=n$. 

$\implies p(x)$ interpolates $f(x)$ uniquely if $\deg(p(x)) = n-1$.

Thus, the minimum degree of the interpolant polynomial is $n-1$.

$p(x)$ interpolates $f(x)$ uniquely if $\deg(p(x))=n-1$. Thus, solving the equation $X\bar{a} = \bar{f}$.
But there are issues in solving the linear system like this. 

+ Computational complexity in solving the linear system $\mathcal{O}(n^3)$.
+ Condition number of $X$ grows exponentially in $n$. This is not preferred as this might cause large errors with small error in input(say due to roundoff errors etc.,)

To overcome these problems, we use Lagrange interpolation.

### Lagrange Interpolant

Consider 

\begin{equation}
g(x_i) = \begin{cases}
        1 & \text{if } i\neq j\\
        0 & \text{if } i=j
    \end{cases}
\end{equation}
for $i,j \in \{1,2,\dots,n\}$. We are interested to find a polynomial interpolant for this. Let us consider the case of $n=3$ points and $j=2$. i.e., $g(x_1)=g(x_3)=0$ and $g(x_2)=1$ for simplicity. The least degree of the polynomial interpolant $p(x)$ is $n-1=2$. By intuition, we can say that $(x-x_1)(x-x_3)$ is a factor of interpolant $p(x)$. As $p(x_2)=g(x_2)=1$, we can say that the interpolant $p(x)$ is:
\begin{equation}
p(x) = \frac{(x-x_1)(x-x_3)}{(x_2-x_1)(x_2-x_3)}
\end{equation}

For $n$ points, we have the interpolant polynomial to $g(x)$ as:
\begin{equation}
p(x) = l_j(x) = \prod_{\substack{i=0 \\ i\neq j}} \frac{x-x_i}{x_j-x_i}
\end{equation}

$l_j(x)$ is a polynomial of degree $n-1$ such that:
$$l_j(x_i) = \delta_{ij}$$
Now consider $n$ node points $\{f(x_i)\}_{i=1}^n$ and consider the polynomial $p(x)$ defined as:
\begin{equation}
p(x) = \sum_{j=1}^n f(x_j) l_j(x)
\end{equation}

$p(x)$ is a polynomial of degree atmost $n-1$. Now,

$$p(x_i) = \sum_{j=1}^n f(x_j) l_j(x_i) = \sum_{j=1}^n f(x_j) \delta_{ij} = f(x_i)$$
This implies that $p(x)$ is an interpolant. This is known as *Lagrange Interpolant*. 

## Choice of Nodes

### Motivation

Now after finding the interpolant, the next question to be asked is how accurate is the interpolant? Immediate obvious answer would be the numer of points chosen. But are there any other factors which determine the accuracy of the interpolant?

Example: Consider the function $f(x) = \frac{1}{1+25x^2}$ and the interpolation nodes as  uniform nodes from [-1,1]. 

**INSERT CODE HERE**

We can see that the interpolant is not converging to $f(x)$ uniformly. There are some  ***boundary effects***

Thus, selection of node points is also important.  The question to ask now is what set of nodes guarantees uniform convergence?

### Fundamental Theorem of Polynomial Interpolation
Let $f(x)$ be a smooth function on [-1,1]. Let $P_n(x)$ be a polynomial interpolant to $f(x)$ at $\{x_k\}_{k=0}^n$ with atmost degree $n$. Then $\exists \, \zeta\in[-1,1]$ such that:
\begin{equation}
e(x) = f(x)-P_n(x) = \frac{f^{(n+1)}(\zeta)}{(n+1)!}\prod_{k=0}^n(x-x_k)
\end{equation}

**Proof:-**
Define $$w(x) := \prod_{k=0}^n(x-x_k)$$ and 
\begin{equation}
g_x(t) : = f(t)-P_n(t)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(t)
\end{equation}
where the subscript $x$ denotes that $x$ is fixed.

$$g_x(x) = f(x)-P_n(x)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(x) = 0$$

$$g_x(x_j) = f(x_j)-P_n(x_j)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w(x_j)$$
As $P_n(x)$ is an interpolant to $f(x)$ at nodes $\{x_i\}_{i=0}^n$, $P_n(x_j) = f(x_j)$ and by definition $w(x_j) =0$. Therefore, for $j\in\{0,1,2,\dots ,n\}$, we have $g_x(x_j)=0$.

$g_x(t)$ is smooth in the interval (-1,1).

Define intervals 
\begin{align}
I_1     &= [x_0,x_1]\\
I_2     &= [x_1,x_2]\\
        &\vdots\\
I_k     &= [x_{k-1},x_k]\\
I_{k+1} &= [x_k,x]\\
I_{k+2} &=[x,x_{k+1}]\\
I_{k+3} &= [x_{k+1},x_{k+2}]\\
        &\vdots\\
I_{n+1} &= [x_{n-1},x_n]
\end{align}
According to Rolle's theorem, $g'_x(t)$ has atleast $n+1$ zeros on $[-1,1]$. 
Again according to Rolle's theorem, we can say that $g''_x(t)$ has atleast $n$ zeros in $[-1,1]$. If we keep on using Rolle's theorem for further $n-1$ times, we can show that $g_x^{(n+1)}(t)$ has atleast 1 zero on $[-1,1]$ i.e., $\exists \text{ a } \zeta\in[-1,1]$ such that $g_x^{(n+1)}(\zeta)=0$

$$g^{(n+1)}_x(\zeta) = f^{(n+1)}(\zeta)-P^{(n+1)}_n(\zeta)-\left(\frac{f(x)-P_n(x)}{w(x)}\right)w^{(n+1)}(\zeta)$$

As $P_n(t)$ is an $n^th degree polynomial, $P^{(n+1)}_n(\zeta)=0$.

$$w(t) = \prod_{k=0}^n(t-x_k)\implies w^{(n+1)}(t) = (n+1)!$$

Therefore $$0 = f^{(n+1)}(\zeta)-0 - \left(\frac{f(x)-P_n(x)}{w(x)}\right)(n+1)!$$
$$\implies e(x) = f(x)-P_n(x) = \frac{f^{(n+1)}(\zeta)}{(n+1)!}w(x)= \frac{f^{(n+1)}(\zeta)}{(n+1)!}\prod_{k=0}^n(x-x_k)$$

>What if $x\in[a,b]$ instead of $x\in[-1,1]$?

We can use a linear mapping from $[a,b]$ to $[-1,1]$.

### Different Possible types of nodes

Now the goal is to find the nodes which minimise the maximum absolute interpolation error i.e., $$\min \max_{x\in[-1,1]} |e(x)|$$. In general, we can also try finding nodes which minimise $p$-norm of the interpolation error i.e., $\min \lVert e(x) \rVert_p$ where:
$$\lVert e(x) \rVert_p = \left( \int_{-1}^1 |e(x)|^p \, dx\right)^{\frac{1}{p}}$$ and $$\lim_{p\to\infty} \lVert e(x) \rVert_p = \max_{x\in[-1,1]}|e(x)|$$

Now the issue is it is difficult to know about $f^{(n+1)}(\zeta)$ as $f(x)$ is not known always. Now the best thing we can do is to find nodes which minimise $$\min \max_{x\in[-1,1]}\left|\prod_{k=0}^n(x-x_k)\right| \text{ or } \min \lVert \prod_{k=0}^n(x-x_k) \rVert_p$$.

It turns out that Legendre nodes minimise $||w(x)||_2$, Chebyshev nodes of first kind minimise $||w(x)||_{\infty}$ and Chebyshev nodes of second kind minimises $||w(x)||_1$.

#### Legendre Nodes

Monic Legendre polynomials are defined as:
$$q_0(x) = 1, \, \, q_1(x) = x$$
$q_n(x)$ is a monic polynomial of degree $n$ such that:
\begin{equation}
\int_{-1}^1 q_n(x) q_m(x) \, dx = 0 \ \  \ \forall \,  m\neq n
\end{equation}

First few monic Legendre polynomials are:
\begin{align*}
  q_0(x) &= 1\\
  q_1(x) &= x\\
  q_2(x) &= x^2-\frac{1}{3}\\
  q_3(x) &= x^3-\frac{3}{5}x\\
  q_4(x) &= x^4- \frac{6}{7}x^2+\frac{3}{35}
\end{align*}

The zeros of these Legendre polynomials are called Legendre nodes. Legendre nodes minimise $||w(x)||_2$. 

#### Chebyshev nodes of first kind
Chebyshev polynomials of first kind are given by $T_n(x) = \cos(n\cos^{-1}(x))$.

First few Chebyshev polynomials are:
\begin{align}
T_0(x) &= 1\\
T_1(x) &= x\\
T_2(x) &= 2x^2-1\\
T_3(x) &= 4x^3-3x\\
T_4(x) &= 8x^4-8x^2+1
\end{align}

An interesting property of these polynomials are $T_m(x)$ and $T_n(x)$ are orthogonal weighted by $\frac{1}{\sqrt{1-x^2}}$. 

\begin{align}
   \int_{-1}^{1}\frac{ T_n(x)T_m(x)}{\sqrt{1-x^2}}dx &= 0 , m \neq n\\
                                    &= \pi,m = n= 0\\
                                    &= \frac{\pi}{2}, m = n \neq 0               
\end{align}

**Proof:-**

Let $x = \cos \theta$ where $\theta \in [0,\pi]$. This implies that $dx = -\sin \theta \, d \theta = -\sqrt{1-x^2} \, d \theta$. And $x=-1 \implies \theta = \pi$ and $x =1 \implies \theta = 0$.  Therefore,
\begin{align*}
\int_{-1}^{1} T_m(x)T_n(x) \frac{1}{\sqrt{1-x^2}} dx & = \int_{0}^{\pi} \cos(m\theta) \cos(n\theta) d\theta \\
&= \frac{1}{2}{\int_{0}^{\pi}} \cos(m+n)\theta 
+ \cos(m-n)\theta d\theta  \\
&=\frac{1}{2} \left[\sin\frac{(m+n)\theta }{m+n}\right]_{0}^{\pi}
+ \left[\sin\frac{(m-n)\theta }{m-n}\right]_{0}^{\pi}\\
&= 0, \text{ if } m \neq n 
\end{align*}

Similarly with appropriate substitution, conditions for $m = n=0 $ and $m = n \neq 0 $ can be proved.

The zeros of Chebyshev polynomial $T_{n+1}(x)$ are given by $x_k = \cos\left( \frac{2k+1}{2n+2} \pi \right)$ where $k \in \{ 0,1,2,\cdots,n \}$.

**Proof:-**

\begin{flushleft}
\textbf{Proof:-}
\end{flushleft}

We have $$T_{n+1}(x) = \cos((n+1)\arccos(x))$$
$$T_{n+1}(x) = 0 \implies \cos((n+1)\arccos(x)) = 0$$

$$\implies(n+1) \arccos(x) = (2k+1)\frac{\pi}{2} \ \ \ k \in \mathbb{Z}$$

$$\implies \arccos(x) = (2k+1)\frac{\pi}{2(n+1)} \ \ \ \ k \in \mathbb{Z}$$

But as the principle range of $\arccos(x)$ is defined as $[0,\pi]$, we must restrict the values $k$ can take. 

$$0 \le (2k+1)\frac{\pi}{2(n+1)} \le \pi \implies 0 \le k \le n+\frac{1}{2} $$

But as $k \in \mathbb{Z}$, we can say that the possible values $k$ can take are $\{ 0,1,2,\cdots,n\}$. 

Therefore $\arccos(x_k) = (2k+1)\frac{\pi}{2(n+1)} \ \ \ \ k \in \{ 0,1,2,\cdots,n\}$
$$\implies x_k = \cos\left( (2k+1)\frac{\pi}{2(n+1)} \right)\ \ \ \ k \in \{ 0,1,2,\cdots,n\}$$

Therefore the zeros of $T_{n+1}(x)$ are in the interval $[-1,1]$ are given by $x_k = \cos\left( \frac{2k+1}{2n+2} \pi \right)$ where $k \in \{ 0,1,2,\cdots,n \}$. The number of zeros is also consistent with the fact that as $T_{n+1}(x)$ is an $(n+1)$ th-degree polynomial, it has $(n+1)$ roots. Also, these zeros are distinct.

These zeros are called the Chebyshev nodes of first kind. They minimise $||w(x)||_{\infty}$

**Theorem**

If $f(x)$ is smooth on $[-1,1]$, then Lagrange interpolation using the roots of Chebyshev nodes of first kind converges uniformly. 


**INSERT RUNGE FUNCTION-CHEBYSHEV NODES CONVERGENCE CODE** 

## Wierstrass Approximation theorem

*Suppose $f$ is a continuous real-valued function defined on the real interval $[a,b]$. For every $\epsilon > 0$, there exists a polynomial $p$ such that for all $x$ in $[a,b]$, we have $|f(x) - p(x)| < \epsilon$.*

 _**Bernstein polynomial:**_ 

The Bernstein basis polynomials of degree $n$ are defined as
\begin{equation}
    b_{k,n}(x) = {}^nC_{k}\, x^k (1-x)^{n-k}
\end{equation}
A linear combination of these basis polynomials can be used to obtain other polynomials. One of the properties of these polynomials is that
\begin{equation}
\begin{aligned}
    \sum_{k=0}^n {b_{k,n}(x)} &= \sum_{k=0}^n {{}^nC_{k}\, x^k (1-x)^{n-k}} \\
    &= (x + (1 - x))^n = 1
    \label{equ:property}
\end{aligned}
\end{equation}
So these polynomials can also be considered to act as some kind of weights. Now, for approximating functions, the Bernstein Polynomial is defined as
\begin{equation}
    B_n(f;x) = \sum_{k=0}^n {f\left(\dfrac{k}{n}\right) {}^nC_{k}\, x^k (1-x)^{n-k}}
    \label{eqn:bern}
\end{equation}
Here, $f$ is the function being approximated, $n$ is the order of approximation \& $x$ is the point at which the approximation is made.

**Proof:** 

To prove the theorem on closed intervals $[a,b]$, without loss of generality, we can take the closed interval as $[0, 1]$. Thus, $f$ can be considered as a continuous real-valued function on $[0, 1]$. Since $f$ is a continuous function, we can say that for a given $\epsilon > 0$, there exists a $\delta > 0$ such that:
\begin{equation}
    |x-y| < \delta \implies |f(x) - f(y)| <\frac{\epsilon}{2} \ \ \ \  \forall \, x,y \in [0,1]
    \label{equ:convcond}
\end{equation}
To prove that $B_n(f,x)$ converges to $f(x)$ uniformly, we show that $|B_n(f,x) - f(x)|$ has to be made small. By the definition of Bernstein's polynomial, as shown in Eqn. \ref{eqn:bern} and by using the property given in Eqn. \ref{equ:property}, we can write $|B_n(f;x) - f(x)|$ as: 
\begin{align*}
    |B_n(f;x) - f(x)| &= \left| \sum_{k=0}^n {\left(f\left(\dfrac{k}{n}\right) - f(x)\right) {}^nC_{k} \, x^k (1-x)^{n-k}} \right| \\
    &\leq \sum_{k=0}^n {\left|f\left(\dfrac{k}{n}\right) - f(x)\right| {}^nC_{k} \, x^k (1-x)^{n-k}}
\end{align*}
Now, if we consider $y=\dfrac{k}{n}$ in Eqn. \ref{equ:convcond} and if $\left|\dfrac{k}{n} - x\right| < \delta$, we can say that $\left|f\left(\dfrac{k}{n}\right) - f(x)\right| < \dfrac{\epsilon}{2}$. But this is not true in the entire domain. So, we partition the domain in to two sets: $A$ and $B$, where $A$ and $B$ have the following properties:
\begin{equation}
    \begin{aligned}
        A \cup B &= [0,1]\\
        A \cap B &= \phi\\
        A &= \{x:|k/n - x| \leq \delta, x \in [0,1]\}\\
        B &= \{x:|k/n-x| > \delta, x \in [0,1]\}
    \end{aligned}
    \label{equ:sets}
\end{equation}
By dividing the domain into sets $A$ and $B$ as defined in \ref{equ:sets}, we can write: 
\begin{equation*}
\begin{split}
    |B_n(f;x) - f(x)| = \sum_{x\in A} \left|f\left(\dfrac{k}{n}\right) - f(x)\right| &{}^nC_{k}\, x^k (1-x)^{n-k} \\
    &+ \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{split}
\end{equation*}
From Eqns. \ref{equ:convcond} and \ref{equ:sets}, we can say that $\left|f\left(\dfrac{k}{n}\right) - f(x)\right|$ has an upper bound of $\dfrac{\epsilon}{2}$ on the set $A$. Therefore, we can write: 
\begin{equation*}
\begin{split}
    |B_n(f;x) - f(x)| \leq \sum_{x\in A} \left(\dfrac{\epsilon}{2}\right){}^nC_{k}\, x^k &(1-x)^{n-k} \\
    &+ \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{split}
\end{equation*}
By using the property described in Eqn. \ref{equ:property}, we can simplify the above inequality as: 
\begin{equation*}
    |B_n(f,;x) - f(x)| \leq \frac{\epsilon}{2} + \sum_{x\in B} \left|f\left(\dfrac{k}{n}\right) - f(x)\right |{}^nC_{k}\, x^k (1-x)^{n-k}
\end{equation*}
Since $f$ is uniformly converging in $[0,1]$, $f$ is bounded from above. So, let the maximum value of $f$ in the domain be $M$. Thus:
\begin{equation*}
    \text{max\{} | f(x) | \} = M \ \ \ \  \forall \, x \in [0,1]
\end{equation*}
Thus, the maximum value $\left|f\left(\dfrac{k}{n}\right) - f(x)\right|$ can achieve in the domain is $2M$ (considering the case where, one of them is $M$ and the other is $-M$). Thus, we have:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + (2M) \sum_{x\in B} {{}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
Now, in set $B$, $\left| \dfrac{k}{n} - x \right| > \delta$. Thus, we get:
\begin{equation*}
    \frac{\left( k/n - x \right)^2}{\delta^2} > 1
\end{equation*}
Multiplying this to the second term of RHS, we get:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + (2M) \sum_{x\in B} {\frac{\left( k/n - x \right)^2}{\delta^2}{}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
The second term can now be written as:
\begin{equation*}
    \dfrac{2M}{\delta^2 n^2} \sum_{x\in B} {(k-nx)^2 \, {}^nC_{k}\, x^k (1-x)^{n-k}}
\end{equation*}
The summation term is equivalent to computing the variance of a binomial distribution with parameters $n$ \& $x$. The variance is given by $nx(1-x)$. Thus, we get:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{2M}{\delta^2 n^2} nx(1-x)
\end{equation*}
We know that using AM $\geq$ GM:
\begin{equation*}
    x(1-x) \leq \dfrac{1}{4}
\end{equation*}
Thus, we have:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{M}{2\delta^2 n}
\end{equation*}
Now, let us define a quantity $N$ such that the above condition holds true for all $n > N$, which gives us $\dfrac{1}{n} < \dfrac{1}{N}$. Thus:
\begin{equation*}
    |B_n(f;x) - f(x)| \leq \frac{\epsilon}{2} + \dfrac{M}{2\delta^2 N}
\end{equation*}
If we choose the $N$ such that:
\begin{equation*}
    \dfrac{M}{2\delta^2N} = \dfrac{\epsilon}{2}
\end{equation*}
giving us:
\begin{align*}
    |B_n(f;x) - f(x)| &\leq \dfrac{\epsilon}{2} + \dfrac{\epsilon}{2} \\
    |B_n(f;x) - f(x)| &\leq \epsilon
\end{align*}
Thus, we have for all $n > N$, where $N = \dfrac{M}{\delta^2\epsilon}$, we have
\begin{equation}
    |B_n(f;x) - f(x)| \leq \epsilon
\end{equation}
i.e., the Bernstein Polynomial $B_n(f;x)$ uniformly converges to $f(x)$ for all $x$ in the domain $[0,1]$.

<!-- # Cross-references {#cross} -->

<!-- Cross-references make it easier for your readers to find and link to elements in your book. -->

<!-- ## Chapters and sub-chapters -->

<!-- There are two steps to cross-reference any heading: -->

<!-- 1. Label the heading: `# Hello world {#nice-label}`.  -->
<!--     - Leave the label off if you like the automated heading generated based on your heading title: for example, `# Hello world` = `# Hello world {#hello-world}`. -->
<!--     - To label an un-numbered heading, use: `# Hello world {-#nice-label}` or `{# Hello world .unnumbered}`. -->

<!-- 1. Next, reference the labeled heading anywhere in the text using `\@ref(nice-label)`; for example, please see Chapter \@ref(cross).  -->
<!--     - If you prefer text as the link instead of a numbered reference use: [any text you want can go here](#cross). -->

<!-- ## Captioned figures and tables -->

<!-- Figures and tables *with captions* can also be cross-referenced from elsewhere in your book using `\@ref(fig:chunk-label)` and `\@ref(tab:chunk-label)`, respectively. -->

<!-- See Figure \@ref(fig:nice-fig). -->

<!-- ```{r nice-fig, fig.cap='Here is a nice figure!', out.width='80%', fig.asp=.75, fig.align='center', fig.alt='Plot with connected points showing that vapor pressure of mercury increases exponentially as temperature increases.'} -->
<!-- par(mar = c(4, 4, .1, .1)) -->
<!-- plot(pressure, type = 'b', pch = 19) -->
<!-- ``` -->

<!-- Don't miss Table \@ref(tab:nice-tab). -->

<!-- ```{r nice-tab, tidy=FALSE} -->
<!-- knitr::kable( -->
<!--   head(pressure, 10), caption = 'Here is a nice table!', -->
<!--   booktabs = TRUE -->
<!-- ) -->
<!-- ``` -->
